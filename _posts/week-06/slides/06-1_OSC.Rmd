---
title: "Week 6 - Computing at OSC - I"
output:
  xaringan::moon_reader:
    seal: false
    css: ["default", "default-fonts", "slides.css", "slides_copy.css"]
    lib_dir: libs
    nature:
      highlightStyle: rainbow
      highlightLines: true
      countIncrementalSlides: false
---
class:inverse middle center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(eval = FALSE)
```

## *Week 6: OSC jobs with SLURM*

----

# Part I: <br> An introduction to using OSC

<br> <br> <br> <br> <br>

### Jelmer Poelstra
### 2021/02/16 (updated: `r Sys.Date()`)

---

## Overview of this week

Now that you have a solid basis in working in the shell and in shell scripting,
it's time to learn about OSC (beyond OnDemand) so you are able to run more
demanding data processing analyses on the cluster.

- Today: An introduction to using OSC,
  including software usage and the SLURM resource manager.

- Thursday: SLURM jobs in practice.

---
class: inverse middle center

# Overview of this presentation

----

.left[
- ### [Introduction](#introduction)
- ### [Connecting to OSC](#connect)
- ### [OSC file systems](#filesystems)
- ### [Using software at OSC](#software)
- ### [Compute job options](#jobs)
]

---
class: inverse middle center
name: introduction

# Introduction

-----

<br><br><br><br>

---

## What is the OSC?

- The **Ohio Supercomputer Center (OSC)** provides computing
  resources across the state of Ohio (it is not a part of OSU).

- OSC has two supercomputers and lots of infrastructure for their usage.

--

<br>

- Research usage is charged but via institutions
  at [heavily subsidized rates](https://www.osc.edu/content/academic_fee_model_faq).
  
- Educational usage, like our `PAS1855` project, is free!

---

## Supercomputer?

- A highly interconnected set of many processors and storage units.

- Also known as:
  - A **cluster**
  - A High-Performance Computing cluster (**HPC cluster**)

<br>

--

### When do we benefit from using a supercomputer?

- Our dataset is too large to be handled efficiently, or even at all,
  by our computer.

- Long-running analyses can be sped up by using more computing power.
  
- We need to repeat a computation many times.
  
---

## Learning about OSC

- OSC regularly has online introductory sessions,   
  both overviews and more hands-on sessions &ndash;
  see the [OSC Events page](https://www.osc.edu/events).

--

- OSC provides excellent introductory material at their   
  [Getting Started Page](https://www.osc.edu/resources/getting_started).

<p align="center">
<img src=img/gettingstarted.png width="560">
</p>

---

## Learning about OSC (cont.)

- Also have a look at all the
  ["HOWTO" pages](https://www.osc.edu/resources/getting_started/howto) &ndash;  
  very specific and includes more advanced material.

<br>

<p align="center">
<img src=img/HOWTOs.png width="500">
</p>

---

## Terminology: cluster and node

#### Cluster

- Many processors and storage units tied together.

- OSC currently has two clusters: **_Pitzer_** and **_Owens_**.   
  (Largely separate, but same long-term storage space is accessed by both.)

<br>

--

#### Node

- Essentially a powerful computer &ndash; one of many in a cluster.

  - **Login node**: A node you end up on after logging in, which is not meant
  for computing. *(A few per cluster.)*
  
  - **Compute node**: A node for compute jobs. *(Hundreds per cluster.)*

---

## Terminology: core

#### Core

- One of often many *processors* in a node &ndash;  
  e.g. standard *Pitzer* nodes have 40-48 cores.

--

- Each core can run and be reserved independently for jobs.

- But, using multiple cores for a job is also common, to:
  - Parallelize computations across cores.
  - Access more memory.


---

## Terminology: cluster > node > core

<br>

<p align="center">
<img src=img/cluster-node-core.png width="100%">
</p>


---
class:inverse middle center
name:connect

# Connecting to OSC

-----

<br><br><br><br>

---

## Connecting to OSC with "OnDemand"

- You can make use of OSC not only through `ssh` at the command line,   
but also through the web browser: from https://ondemand.osc.edu.

<p align="center">
<img src=img/ondemand1.png width="900">
</p>

--

<br>

- At OnDemand, you can, for example:

  - Access a terminal directly in your browser.
  
  - Run "Interactive Apps" like RStudio, Jupyter Notebooks, and VS Code.
  
  - Browse and upload files.

  .content-box-diy[
  **Interface Demo**
  ]

---

## Connecting to OSC with SSH (Secure Shell)

- If you want to log in from your local terminal,   
  you need to **connect through SSH.**
  
- Basic `ssh` usage:
  
  ```sh
  $ ssh <username>@pitzer.osc.edu  # e.g. jelmer@pitzer.osc.edu     
  $ ssh <username>@owens.osc.edu   # e.g. jelmer@owens.osc.edu     
  ```
  
  You will be prompted for your OSC password.


<br> <br>

.content-box-info[
See [this optional ungraded assignment](https://mcic-osu.github.io/pracs-sp21/)
for some SSH setup you can do to make this quicker:

- Avoid being prompted for your password.

- Use a *shortcut* for `<username>@<cluster>.osc.edu`.
]

---

## Login nodes: best practices

After logging in to OSC, you're on a **login node**.
  
- Use login nodes for *navigation*, *housekeeping*, and *job submission*.

- Any process that is active for >20 minutes or uses >1GB **will be killed**.

- It is good practice to avoid even getting close to these limits:   
  login nodes are shared by everyone, and can get clogged.   

<br> <br> <br> <br> <br> <br> <br> <br>

.content-box-info[
For more info, see OSC's page [Login environment at OSC](https://www.osc.edu/supercomputing/login-environment-at-osc).
]

---

## Transferring files to and from OSC with GUIs

#### For small transfers (<1 GB): use OnDemand

<p align="center">
<img src=img/ondemand2_circle.png width="85%">
</p>

----

<p align="center">
<img src=img/ondemand3_circle.png width="85%">
</p>

--

<br>

#### For large transfers (>1 GB): use Globus

- Especially useful for very large and/or complex transfers.
- Does need a
  [local installation](https://www.osc.edu/resources/getting_started/howto/howto_transfer_files_using_globus_connect)    and some set-up.

.content-box-info[
See [these bonus slides](#shell-transfer) for small and large transfers in the shell.
]


---
class: inverse center middle
name: filesystems

# File systems at OSC

----

<br><br><br><br>

---

## File systems at OSC

OSC has [several file systems]((https://www.osc.edu/supercomputing/storage-environment-at-osc/available-file-systems) with different typical use cases:
we'll discuss this in terms of short-term versus long-terms storage. 

---

## File systems: long-term storage with daily back-ups

**Home dir**

- Can always be accessed with shortcuts **`$HOME`** and **`~`**.

- 500 GB limit per user.

- Your home dir will include your first project ID, e.g. `/users/PAS0471/jelmer`.
  (However, this is not the project dir!)

--

<br>

**Project dir**
  
- `/fs/project/<project-ID>` (MCIC project: `/fs/project/PAS0471`) **OR**  

- `/fs/ess/<project-ID>` (Course project: `/fs/ess/PAS1855`)
  
- The total storage limit is whatever the project's PI sets this to
  (and pays for...).
  Note that *charges are for the reserved space*, not the actual usage.
  
---

## File systems: short-term storage locations

**scratch**

- `/fs/scratch/<projectID>` or `/fs/ess/scratch/<projectID>`.

- Fast I/O &ndash; good when working with large input and output files.

- **Temporary**: deleted after 120 days, and not backed up.

--

<br>

**`$TMPDIR`**

- Represents temporary storage space *on your compute node*. 1 TB max.

- Available in the job script through the environment variable `$TMPDIR`.

- **Deleted after job ends** &ndash; so copy to/from in job scripts!

- See [this bonus slide](#tmpdir) for some example code for working with `$TMPDIR`.

---
class: inverse middle center
name: software

# Using software at OSC

-----

<br><br><br><br>

---

## Using software at OSC
 
- At OSC, software is managed with the Lmod system.
  While a lot of software is installed, **much is only available to use after
  we explicitly load it.**
  
  This enables the use of different versions of the same software
  and of mutually incompatible software one a single system. 

- For a list of software that has been installed at OSC, see:      
  <https://www.osc.edu/resources/available_software>.
  
---

## Using software at OSC (cont.)

- You can also search for available software ("modules") in the shell:
  
  - `module spider` list all/matching modules that are installed.
    
  - `module avail` lists all/matching modules that *can be directly loaded*,
    given the current environment.
  
  ```sh
  $ module spider
  #> [long list ...]
  $ module spider python
  #>    Versions:
  #>      python/2.7-conda5.2
  #>      python/3.6-conda5.2
  #>      python/3.7-2019.10
  
  $ module avail
  #> [long list ...]
  $ module avail python
  #> python/2.7-conda5.2  python/3.6-conda5.2 (D)  python/3.7-2019.10
  ```

---

## Using software at OSC (cont.)

- *Loading* and unloading installed software is also done with `module` commands:
  
  ```bash
  # Load a module:
  $ module load python              # Load default version
  $ module load python/3.7-2019.10  # Load a specific version
  
  # Unload a module:
  $ module unload python
  ```
  
- To check which modules have been loaded:
  
  ```sh
  $ module list
  ```
  
  *(The list will include some modules that have been loaded automatically.)*
  
---

## What if software isn't installed at OSC?

**Install it yourself or get others to install it:**

- Installing yourself is possible but can be tricky (no admin rights...).

- For commonly used software, send an email to <oschelp@osc.edu>.

- People like me can help with (more niche) bioinformatics software.

--

<br>

**Alternative approaches (recommended):**

- The **Conda** software management system (next slides).

- `Singularity` **containers**.
  
- These alternatives are useful not only at OSC – but more generally, for:
  
  - Reproducible environments.
  
  - Portable environments (especially containers).
  
  - Avoidance of dependency hell.

---

## Conda: a software environment manager

- Creates *environments* with one (best-practice) or more software packages.

- Handles dependencies but does not require admin rights.

- Environments are activated and deactivated like with the `module` system.

- Lots of bioinformatics software is available as a Conda package.

---

## Conda: a software environment manager

**Benefits:**

- Can easily maintain distinct "environments" each with a different
  version of the same software, or with mutually incompatible software.

- Not much of a learning curve.

- Can save and share environment instructions as very simple text files.

<br>

--

**Limits &ndash; Conda does not:**

- Create an isolated environment that can be exported and used anywhere.

- Allow you to run software that requires a different OS / OS version.


.content-box-info[
**Containers** address these limitations &ndash; see
[these bonus slides](#containers).
]

---

## Conda in practice, briefly (more on Thursday)

- Load the Conda module at OSC (comes with the Python module): 

  ```bash
  $ module load python/3.6-conda5.2
  ```

- Example: **create** a new environment with "`cutadapt`" installed:

  ```bash
  $ conda create -y -n cutadapt-env cutadapt
  ```

--

- **Activate** the `cutadapt-env` environment and see if we can run Cutadapt:

  ```bash
  $ source activate cutadapt-env
  
  (cutadapt-env) $ cutadapt --version
  #> 3.2
  ```

  *(Note above that Conda will indicate the active environment in the prompt!)*

---
class: inverse middle center
name: jobs

# Compute jobs

-----

<br><br><br>

---

## Starting a compute job: background

To do analyses at OSC, you need access to a **compute node**.
  
- Automated scheduling software allows 100s of people with different
  requirements to access compute nodes effectively and fairly.

- OSC uses the **SLURM** scheduler  
  (**S**imple **L**inux **U**tility for **R**esource **M**anagement).

<br>

.content-box-info[
Last year, OSC switched from PBS to SLURM &ndash; see
[here](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration)
for migration info and a comparison of commands and options between the two
systems.
]

---

## Starting a compute job: how?

Three main options:

- In OnDemand, start an "**Interactive App**" like Jupyter Notebook or RStudio.   

- Run an **interactive shell job**.

- Provide *SLURM* directives in or along with **scripts**.

<br> <br>

.content-box-info[
Both interactive and non-interactive jobs start in the directory that they were submitted from: i.e., your working directory will remain the same.
]

---

## Passing on requests/instructions to SLURM

You can provide instructions to the *SLURM* scheduler to specify the resources
(cores/memory/time/etc) that you need.

These can be provided at the command-line:
```sh
$ sinteractive -A PAS1855 -t 60
$ sbatch myscript.sh -A PAS1855 -t 60
```

As well as **at the start of** scripts in lines with the `#SBATCH` keyword:
```sh
#!/bin/bash
#SBATCH --account=PAS1855
#SBATCH --time=60
```

--

.content-box-info[
Many `sbatch` options have a long format (`--account=PAS1855`) and a short
format (`-A PAS1855`), which can generally be used interchangeably.
]

---

## Starting compute jobs: Interactive shell jobs

Interactive shell jobs will get you interactive shell access on a compute node.

- For short interactive shell jobs, use OSC's custom
  [`sinteractive`](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration/how_to_monitor_and_manage_jobs)
  wrapper:

  ```sh
  $ sinteractive -A PAS1855 # Default: 30-mins, 1-core, 1-node
  
  $ sinteractive -A PAS1855 -t 60 # 60 mins - max for sinteractive
  ```

  .content-box-info[
  `sinteractive` only accepts a selection of `sbatch` options and they have to be
  specified in short format.
  ]

--

<br>

- You can also use `srun` / `salloc`, which support all options:

  ```sh
  $ srun -A PAS0471 -t 60 --pty /bin/bash
  
  # Equivalent -- syntax that OSC uses in their docs:
  $ salloc -A PAS0471 -t 60 srun --pty /bin/bash
  ```

---

## Starting compute jobs: Shell scripts

- To submit a job, we simply preface the call to our script with `sbatch`:

  ```sh
  # Run a script on whatever node we are on:
  $ ./fastqc.sh
  
  # Same, but now submit it to the queue:
  $ sbatch fastqc.sh
  # Submitted batch job 2526085
  ```

--

<br>

- We can also provide `sbatch` options and/or script arguments:
  
  ```sh
  $ sbatch [sbatch-options] <script> [script-arguments]
  
  $ sbatch fastqc.sh sampleA.fastq.gz
  # Submitted batch job 2526086
  
  $ sbatch -t 60 -A PAS1855 --mem=20G fastqc.sh
  # Submitted batch job 2526085 
  ```

---

## Starting compute jobs: Shell scripts (cont.)

.content-box-info[
Any `sbatch` options provided on the command-line will override the equivalent
options provided in the script itself.

Our script, `fastqc.sh`:

```sh
#!/bin/bash
#SBATCH --account=PAS0471
#SBATCH --time=00:45:00
#SBATCH --mem=8G
```

Submitting the script:

```sh
$ sbatch --mem=20G fastqc.sh
```

In this case, the memory request will be `20G` and not `8G`.
]

---

## Compute job options: Project

- Your jobs need to be billed to a project, and you *always* need to specify one.  

<br> <br> <br> <br> <br> <br> <br> <br> <br> <br>

| Resource/use                 | short    | long      | default
|------------------------------|----------|-----------|:--------:|
| Project to be billed         | -A PAS1855 | --account=PAS1855 | N/A

```sh
#!/bin/bash
#SBATCH --account=PAS0471
```

---

## Compute job options: Nodes and cores

| Resource/use                  | short    | long                    | default
|-------------------------------|----------|-------------------------|:--------:| 
| Number of nodes               | -N 1     | --nodes=1               | 1
| Number of cores               | -c 1     | --cpus-per-task=1       | 1
| Number of "tasks" (processes) | -n 1     | --ntasks=1              | 1
| Number of tasks per node      | -        | --ntasks-per-node=1     | 1

--

```sh
#!/bin/bash
#SBATCH --cpus-per-task=2       # Preferred for multi-threading
```

```sh
#!/bin/bash
#SBATCH --ntasks=2              # Preferred for multi-process
#SBATCH --ntasks-per-node=2     # (Equivalent for single-node jobs)
```

.content-box-info[
- Only ask for **>1 node** when you have explicit parallelization.

- Multi-threading is much more common than having multiple processes.
]

---

## Compute job options: Time

You need to specify a **time limit** for your job (= "wall time"),
and your job gets killed as soon as it hits the time limit!

- You will only be charged for the time your job *actually used*.

- Shorter jobs are likely to start sooner.

**If you are uncertain about the time your job will take,
ask for (much) more time than you think you will need.**

--

<br>

.content-box-info[
For single-node jobs, up to 168 hours (7 days) can be requested.
If that's not enough, you can request access to the `longserial` queue for jobs
of up to 336 hours (14 days).
]

---

## Compute job options: Time (cont.)

Acceptable time formats include:
  - `minutes`
  - `minutes:seconds`
  - `hours:minutes:seconds`
  - `days-hours`
  - `days-hours:minutes`
  - `days-hours:minutes:seconds`

<br> <br>

| Resource/use                 | short    | long               | default
|------------------------------|----------|--------------------|:-----:
| Time limit (4 h)             | -t 4:00:00 | --time=4:00:00   | 1:00:00

```sh
$ sbatch -t 30 myscript.sh   # 30 minutes
```

```sh
#!/bin/bash
#SBATCH --time=60            # 1 hour
```

---

## Compute job options: Memory

Specify a maximum amount of RAM (Random Access Memory;
not to be confused with disk storage space).

Like with time, your job **gets killed** when it hits the memory limit.  
But this is not that common &ndash; as the [OSC documentation](https://www.osc.edu/supercomputing/batch-processing-at-osc/job-scripts)
says:

> There is no need to specify a memory limit unless your memory requirements
> are disproportionate to the number of cores you are requesting or you need a large-memory node.

---

## Compute job options: Memory (cont.)

- The default unit is MB (MegaBytes) &ndash; use "G" for GB.

<br> <br> <br> <br> <br> <br> <br> <br>

| Resource/use                 | short    | long      | default |
|------------------------------|----------|-----------|:-----|
| Memory limit per node (8 GB) | -        | --mem=8G  | single-core value

```sh
$ sbatch --mem=20G myscript.sh   # 20 GB
```

```sh
#!/bin/bash
#SBATCH --mem=20G
```

---
class: center middle inverse

# Questions?

-----

<br> <br> <br> <br>

---
class: inverse center middle
name:bonus

# Bonus material

-----

<br>

.left[
- ### [Transferring files in the shell](#shell-transfer)
- ### [Working with $TMPDIR in SLURM scripts](#tmpdir)
- ### [Working with Singularity containers](#containers)
]

<br><br>

---
name: shell-transfer
background-color: #f2f5eb

## Transferring files in the shell: small transfers (<1 GB)

With **`scp`** (secure copy), which works much like the regular `cp`:

- Local to remote:
  
  ```bash
  $ scp <local-path> <user>@pitzer.osc.edu:<remote-path>
  ```
  
- Remote to local is simply the reverse:
  
  ```sh
  $ scp <user>@pitzer.osc.edu:<remote-path> <local-path>
  ```
  
- For instance:
  
  ```sh
  $ scp -r scripts/ jelmer@pitzer.osc.edu:~/scripts
  $ scp -r scripts/ jelmer@pitzer.osc.edu:/fs/ess/PAS1855/users/jelmer/scripts
  
  $ scp -r jelmer@pitzer.osc.edu:~/scripts scripts/
  ```

.content-box-info[
Transfers in either direction should be done from your *local* shell.
]

---
background-color: #f2f5eb

## Transferring files in the shell: small transfers (<1 GB)

Another option is the **`rsync`** command, which I recommend,
especially when you want to keep certain folders synced: `rsync` won't copy
any files that are present & identical in source and destination.

The basic from-to syntax is the same as with `scp`.

- A useful combination of options is `-avz`:
  - `-a` enables archival mode (also makes `-r` unnecessary);
  - `-v` increases verbosity &ndash; tells you what is being copied.
  - `-z` enables compressed file transfer (=> faster).

  ```bash
  $ rsync -avrz ~/scripts jelmer@pitzer.osc.edu:~/scripts
  
  # If the files are large, "--progress" will help you keep track:
  $ rsync -avrz --progress ~/scipts jelmer@pitzer.osc.edu:~/scripts
  ```

---
background-color: #f2f5eb

## More `rsync`

.content-box-warning[
Presence/absence of a trailing slash for source directories makes a
difference to `rsync`.
]

This will sync the two dirs below, so the content of each 
"scripts" dir will end up being identical:

```sh
$ rsync -avz scripts backup/scripts
$ ls scripts
#> fastq.sh
$ ls backup/scripts
#> fastq.sh
```

This will copy the *contents* of the source dir "scripts"
into the target dir "scripts":

```sh
$ rsync -avz scripts/ backup/scripts
$ ls scripts
#> fastq.sh
$ ls backup/scripts
#> scripts
```

---
background-color: #f2f5eb

## Transferring files in the shell: any transfer size

You can also use `sftp`, which doesn't run on a login node,
so large transfers are permitted.

  - To log in:
  ```bash
  $ sftp sftp.osc.edu
  ```

  Now, you have an `sftp` prompt instead of a regular bash prompt.

- To upload files to remote, use the `put` command &ndash; and note that you
  don't need the `user@pitzer.osc.edu:` to designate remote:

  ```sh
  sftp> put scripts/fastqc.sh /fs/ess/PAS1855/users/$USER/
  
  # If you don't specify a remote location, it be put in $HOME:
  sftp> put file.txt
  ```

- To download files from remote, use the `get` command:

  ```sh
  sftp> get /fs/ess/PAS1855/users/$USER/ ~/PP8300/OSC/
  ```

---
background-color: #f2f5eb
name: tmpdir

## Working with `$TMPDIR` in scripts

Copy input to `$TMPDIR`, copy output to home:

```bash
cp -R $HOME/my/data/ $TMPDIR/

#[...analyze data with I/O happening in $TMPDIR....]

cp -R $TMPDIR/* $HOME/my/data/
```

<br>

One potential problem with this approach is that your script may not reach
the final line, where the data is copied back: perhaps it encountered an error
or was killed. But you may still be interesting in getting the output data.

The following trick with `trap` will copy the data even if the job is killed:

```bash
trap "cd $PBS_O_WORKDIR;mkdir $PBS_JOBID;cp -R $TMPDIR/* $PBS_JOBID;exit" TERM
```

---
class: inverse middle center

# Working with Singularity containers

----

<br> <br> <br> <br>

---
background-color: #f2f5eb
name: containers

## Containers versus Virtual Machines (VMs)

- Containers don't virtualize entire Operating Systems:
  - Containers are smaller.
  - Containers are more lightweight to run.
  
<p align="center">
<img src=img/containers.png width="65%">
</p>

.content-box-info[
- Use a VM for long interactive sessions with multiple programs.

- Use a container to run one or a few programs non-interactively.
]

---
background-color: #f2f5eb

## Docker versus Singularity containers

Docker and Singularity are two different container platforms;
Docker is by far the most widely used.
  
However, Singularity:
  
  - Was specifically designed for HPCs & is available at OSC
  
  - Needs no admin rights to run containers
  
  - Can work with Docker images but not vice versa
  
  - Integration with several host directories by default
  
  - Has single-file container images
  
  - Is open-source

---
background-color: #f2f5ebd

## Getting a container

Container images for most software can be found at these registries:

- Docker Hub: <https://hub.docker.com/>

- Sylabs Cloud Library: <https://cloud.sylabs.io/>

- Biocontainers (for bioinformatics software): <https://biocontainers.pro/>

If no suitable image is available, you can build your own!

<br>

--

.center[
**Container terminology**
]

| Term          | Meaning
|---------------|---------
| Image         | File(s) containing container application
| Container     | A *running* container image
| Definition file (Singularity) <br> / Dockerfile (Docker) | Recipe to build a container image


---
background-color: #f2f5eb

## Downloading a container image

- Download (`pull`) a container with MultiQC:

  ```sh
  $ singularity pull <registry>://<user>/<project>/<name>:<tag>
  
  $ singularity pull docker://quay.io/biocontainers/multiqc:1.9--py_1
  #> INFO:      Converting OCI blobs to SIF format
  ```
  
  (Note that it pulls a *Docker* image and converts it automatically.)

- Now we have `.sif` container image:

  ```sh
  $ ls multiqc*
  #> multiqc_1.9--py_1.sif
  ```

.content-box-info[
An example of downloading a container from Singularity hub ("shub"):

```sh
singularity pull shub://singularityhub/hello-world
#> INFO:      Downloading shub image
```
]

---
background-color: #f2f5eb

## Interacting with a container image

- To "run" the container, i.e. run whatever is defined as the default action:

  ```sh
  $ singularity run multiqc_1.9--py_1.sif
  Singularity> # In this case, opens terminal inside container 
  
  Singularity> multiqc --help # In which we can run multiqc
  #> Usage: multiqc [OPTIONS] <analysis directory>
  ```

- The `shell` sub-command *always* opens a terminal inside the container:

  ```sh
  $ singularity shell multiqc_1.9--py_1.sif
  Singularity>
  ```
  
- **Most useful is the the `exec` sub-command**,
  which lets you directly execute a shell command – and then exits:
  
  ```sh
  $ singularity exec multiqc_1.9--py_1.sif multiqc --help
  ```

---
background-color: #f2f5eb

## "Containerizing" an analysis or script

- Running a locally installed program:

  ```sh
  $ multiqc --help
  ```
  
- Run the same command inside a container:

  ```sh
  $ singularity exec multiqc_1.9--py_1.sif \
      multiqc --help
  ```
