---
title: "Week 6 - OSC - I"
output:
  xaringan::moon_reader:
    seal: false
    css: ["default", "default-fonts", "slides.css", "slides_copy.css"]
    lib_dir: libs
    nature:
      highlightStyle: rainbow
      highlightLines: true
      countIncrementalSlides: false
---
class:inverse middle center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(eval = FALSE)
```

## *Week 6: OSC jobs with SLURM*

----

# Part I: <br> An introduction to OSC

<br> <br> <br> <br> <br>

### Jelmer Poelstra
### 2021/02/16 (updated: `r Sys.Date()`)

---
class: inverse middle center

# Overview

----

.left[
- ### [Introduction](#introduction)
- ### [Connecting to OSC](#connect)
- ### [OSC file systems](#filesystems)
- ### [Using software at OSC](#software)
- ### [Compute job options](#jobs)
]

---
background-color: #f2f5eb

## Some quick information about your Final Project

**The project should:**

- Be well-organized, and well-documented using Markdown READMEs.

- Be version-controlled throughout, and repeatedly "pushed" to Github.

--

- Be re-runnable using a single Snakemake control script ("Snakefile").

- Should have (most) analysis or data processing done using scripts  
  in bash and/or Python.

- Optionally run jobs at OSC.

---
background-color: #f2f5eb

## Some quick information about your Final Project

- So, lots of "infrastructure" requirements &mdash;
  therefore, no need to devise a complicated project with many scripts.

<br>

--

- We encourage you to *use your own data or find a dataset yourself*,  
  but Zach will also provide a dataset with accompanying project ideas if
  you prefer to do that. (And we can dig up other datasets if needed.)

<br>

--

- You are quite free regarding the **topic** of the project:
  
  - From answering an actual or contrived research question...
  
  - ...to automating something "boring" that you until now always did manually. 

---
background-color: #f2f5eb

## Some quick information about your Final Project

Your project proposal is not due until the week of March 23rd,
but you may want to slowly start thinking about this.

---
class: inverse middle center
name: introduction

# Introduction

-----

<br><br><br><br>

---

## What is the OSC?

- The **Ohio Supercomputer Center (OSC)** provides computing
  resources across the state of Ohio (it is not a part of OSU).

- OSC has two supercomputers and lots of infrastructure for their usage.

--

<br>

- Research usage is charged but via institutions
  at [heavily subsidized rates](https://www.osc.edu/content/academic_fee_model_faq).
  
- Educational usage, like our `PAS1855` project, is free!

---

## Supercomputer?

- A highly interconnected set of many processors and storage units.

- Also known as:
  - A **cluster**
  - A High-Performance Computing cluster (**HPC cluster**)

<br>

--

### When do we benefit from using a supercomputer?

- Our dataset is too large to be handled efficiently, or even at all,
  by our computer.

- Long-running analyses can be sped up by using more computing power.
  
- We need to repeat a computation many times.
  
---

## Learning about OSC

- OSC regularly has online introductory sessions,   
  both overviews and more hands-on sessions &ndash;
  see the [OSC Events page](https://www.osc.edu/events).

--

- OSC provides excellent introductory material at their   
  [Getting Started Page](https://www.osc.edu/resources/getting_started).
**Do read these!**

<p align="center">
<img src=img/gettingstarted.png width="560">
</p>

---

## Learning about OSC (cont.)

- Also have a look at all the
  ["HOWTO" pages](https://www.osc.edu/resources/getting_started/howto) &ndash;  
  very specific and includes more advanced material.

<br>

<p align="center">
<img src=img/HOWTOs.png width="500">
</p>

---

## Terminology: cluster and node

#### Cluster

- Many processors and storage units tied together.

- OSC currently has two clusters: **_Pitzer_** and **_Owens_**.   
  (Largely separate, but same long-term storage space is accessed by both.)

<br>

--

#### Node

- Essentially a powerful computer &ndash; one of many in a cluster.

  - **Login node**: A node you end up on after logging in, which is not meant
  for computing. *(A few per cluster.)*
  
  - **Compute node**: A node for compute jobs. *(Hundreds per cluster.)*

---

## Terminology: core

#### Core

- One of often many *processors* in a node &ndash;  
  e.g. standard *Pitzer* nodes have 40-48 cores.

--

- Each core can run and be reserved independently.

- But, using multiple cores for a job is also common, to:
  - Parallelize computations across cores.
  - Access more memory.


---

## Terminology: cluster > node > core

<br>

<p align="center">
<img src=img/cluster-node-core.png width="100%">
</p>


---
class:inverse middle center
name:connect

# Connecting to OSC

-----

<br><br><br><br>

---

## Connecting to OSC with "OnDemand"

- You can make use of OSC not only through `ssh` at the command line,   
but also through the web browser: from https://ondemand.osc.edu.

<p align="center">
<img src=img/ondemand1.png width="900">
</p>

--

- At OnDemand, you can, for example:

  - Access a terminal directly in your browser.
  
  - Run "Interactive Apps" like RStudio, Jupyter Notebooks, and VS Code.
  
  - Browse and upload files.

  .content-box-diy[
  **Interface Demo**
  ]

---

## Connecting to OSC with `ssh`

- If you want to log in from your local terminal,   
  you need to **connect through `ssh`.**
  
- Basic `ssh` usage:
  
  ```sh
  $ ssh <username>@pitzer.osc.edu  # e.g. jelmer@pitzer.osc.edu     
  $ ssh <username>@owens.osc.edu   # e.g. jelmer@ownes.osc.edu     
  ```
  
  You will be prompted for your OSC password.

---

## Login nodes: best practices

After logging in to OSC, you're on a **login node**.
  
- Use login nodes for *navigation*, *housekeeping*, and *job submission*.

- Any process that is active for >20 minutes or uses >1GB **will be killed**.

- It is good practice to avoid even getting close to these limits:   
  login nodes are shared by everyone, and can get clogged.   

<br>

.content-box-info[
For more info, see OSC's page [Login environment at OSC](https://www.osc.edu/supercomputing/login-environment-at-osc).
]

---

## Transferring files to and from OSC

#### For small transfers (<1 GB): use OnDemand

<p align="center">
<img src=img/ondemand2_circle.png width="95%">
</p>

----

<p align="center">
<img src=img/ondemand3_circle.png width="95%">
</p>

--

-----

#### For large transfers (>1 GB): use Globus

- Especially useful for very large and/or complex transfers.
- Does need a
  [local installation](https://www.osc.edu/resources/getting_started/howto/howto_transfer_files_using_globus_connect)    and some set-up.

.content-box-info[
See [these bonus slides](#shell-transfer) for small and large transfers in the shell.
]


---
class: inverse center middle
name: filesystems

# File systems at OSC

----

<br><br><br><br>

---

## File systems at OSC

OSC has several file systems with different typical use cases:
we'll discuss in terms of short-term versus long-terms storage. 

<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>

.content-box-info[
See [this OSC documentation page](https://www.osc.edu/supercomputing/storage-environment-at-osc/available-file-systems)
for more info about file systems at OSC.
]

---

## File systems: long-term storage with daily back-ups

**Home dir**

- Can always be accessed with shortcuts **`$HOME`** and **`~`**.

- 500 GB limit per user.

- Your home dir will include your first project ID, e.g. `/users/PAS0471/jelmer`.
  (However, this is not the project dir!)

--

<br>

**Project dir**
  
- `/fs/project/<project-ID>` (MCIC project: `/fs/project/PAS0471`) **OR**  

- `/fs/ess/<project-ID>` (Course project: `/fs/ess/PAS1855`)
  
- The total storage limit is whatever a PI sets this too and pays for.   
  Note that charges are for the reserved space, not actual usage.
  
---

## File systems: short-term storage locations

**scratch**

- `/fs/scratch/<projectID>` or `/fs/ess/scratch/<projectID>`.

- Fast I/O &ndash; good when working with large input and output files.

- **Temporary**: deleted after 120 days, and not backed up.

--

<br>

**`$TMPDIR`**

- Represents temporary storage space *on your compute node*. 1 TB max.

- Available in the job script through the environment variable `$TMPDIR`.

- **Deleted after job ends** &ndash; so copy to/from in job scripts!

- See [this bonus slide](#tmpdir) for some example code for working with `$TMPDIR`.

---
class: inverse middle center
name: software

# Using software at OSC

-----

<br><br><br><br>

---

## Using software at OSC
 
- At OSC, software is managed with the `Lmod` system.
  While a lot of software is installed, **much is only available to use after
  we explicitly load it.**
  
  This enables the use of different versions of the same software,
  as well as mutually incompatible software, one one system. 

- For a list of software that has been installed at OSC, see:      
  <https://www.osc.edu/resources/available_software>.
  
--

- You can also search for available software ("modules") in the shell:
  
  - `module spider` list all/matching modules that are installed.
    
  - `module avail` lists all/matching modules that *can be directly loaded*,
    given the current environment.
  
  ```sh
  $ module spider
  $ module spider python
  
  $ module avail
  $ module spider python
  ```

---

## Using software at OSC

- *Loading* and unloading installed software is also done with `module` commands:
  
  ```bash
  # Load a module:
  $ module load R               # Load default version
  $ module load R/4.0.2-gnu9.1  # Load a specific version
  
  # Unload a module:
  $ module unload R
  ```
  
- To check which modules have been loaded:
  
  ```sh
  $ module list
  ```

---

## What if software isn't installed at OSC?

**Install it yourself or get others to install it.**

- Installing yourself is possible but can be tricky (no admin rights...).

- For commonly used software, send an email to <oschelp@osc.edu>.

- People like me can help with (more niche) bioinformatics software.

--

<br>

**Alternative approaches (recommended):**

- The **Conda** software management system (next slides)

- `Singularity` **containers**
  
- These alternatives are useful not only at OSC – but more generally, for:
  
  - Reproducible environments
  
  - Portable environments (especially containers)
  
  - Avoidance of dependency hell

---

## Conda: a software environment manager

- Creates *environments* with one (best-practice) or more software packages.

- Lots of bioinformatics software is available as a Conda package.

- Handles dependencies but does not require admin rights.

- Environments are activated and deactivated like with the `module` system.

---

## Conda: a software environment manager

**Benefits:**

- Can easily maintain distinct "environments" each with a different
  version of the same software, or with mutually incompatible software.

- Not much of a learning curve.

- Can save and share environment instructions as very simple text files.

<br>

--

**Limits &ndash; Conda does not:**

- Create an isolated environment that can be exported and used anywhere.

- Allow running software requiring a different OS / OS version.


.content-box-info[
**Containers** address these limitations &ndash; see
[these bonus slides](#containers).
]

---

## Conda in practice, briefly (more on Thursday)

- Load the Conda module at OSC (comes with the Python module): 

  ```bash
  $ module load python/3.6-conda5.2
  ```

- Example: **create** a new environment with "`multiqc`" installed:

  ```bash
  $ conda create -y -n multiqc-env -c bioconda multiqc
  ```

--

- **Activate** the `multiqc` environment and see if we can run MultiQC:

  ```bash
  [<user>@pitzer-login01 ~]$ source activate multiqc-env
  
  (multiqc) [<user>@pitzer-login01 ~]$ multiqc --help
  #> Usage: multiqc [OPTIONS] <analysis directory>
  ```

  *(Note above that Conda will indicate the active environment in the prompt!)*

--

.content-box-info[
Here, we use `source activate` instead of the more standard
`conda activate` because we otherwise need to run a setup script.
]

---
class: inverse middle center
name: jobs

# Compute jobs

-----

<br><br><br>

---

## Starting a compute job: background

To do analyses at OSC, you need access to a **compute node**.
  
- **Automated scheduling software** allows 100s of people with different
  requirements to access compute nodes effectively and fairly.

- Since December 2020, both Pitzer and Owens use the **_SLURM_ scheduler**   
  (see [here](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration)
  for OSC SLURM info).

---

## Starting a compute job: how?

Three main options:

- In OnDemand, start an "**Interactive App**" like RStudio Server.   

- Run an **interactive shell job**.

- Provide *SLURM* directives in or along with **scripts**.

<br> <br>

.content-box-info[
Both interactive and non-interactive jobs start in (i.e., have as the working
directory) the directory that they were submitted from.
]

---

## Passing on requests/instructions to SLURM

You can provide instructions to the *SLURM* scheduler to specify the resources
(cores/memory/time/etc) that you need.

These can be provided at the command-line:
```sh
$ sinteractive -A PAS1855 -t 60
$ sbatch myscript.sh -A PAS1855 -t 60
```

As well as **at the start of** scripts in lines with the `#SBATCH` keyword:
```sh
#!/bin/bash
#SBATCH --account=PAS1855
#SBATCH --time=60
```

--

.content-box-info[
Many sbatch options have a long format (`--account=PAS1855`) and a short
format (`-A PAS1855`), which can generally be used interchangeably.
]

---

## Starting compute jobs: Interactive shell jobs

Interactive shell jobs will get you interactive shell access on a compute node.

- For short interactive shell jobs, use OSC's custom
  [`sinteractive`](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration/how_to_monitor_and_manage_jobs)
  wrapper:

  ```sh
  $ sinteractive -A PAS1855 # Default: 30-mins, 1-core, 1-node
  
  $ sinteractive -A PAS1855 -t 60 # 60 mins - max for sinteractive
  ```

  .content-box-info[
  `sinteractive` only accepts a selection of `sbatch` options and they have to be
  specified in short format.
  ]

--

<br>

- You can also use `srun` / `salloc` which support all options:

  ```sh
  $ srun -A PAS0471 -t 60 --pty /bin/bash
  
  # Equivalent -- syntax that OSC uses in their docs:
  $ salloc -A PAS0471 -t 60 srun --pty /bin/bash
  ```

---

## Starting compute jobs: Shell scripts

- To submit a job, we preface the call to script with `sbatch`:

  ```sh
  $ sbatch [sbatch-options] script.sh [script-arguments]
  
  $ sbatch fastq.sh
  # Submitted batch job 2526085 
  
  $ sbatch fastq.sh sampleA.fastq.gz
  # Submitted batch job 2526086
  
  $ sbatch -t 60 -A PAS1855 --mem=20G fastq.sh sampleA.fastq.gz
  # Submitted batch job 2526085 
  ```

--

.content-box-info[
Any `sbatch` options provided on the command-line will override the equivalent
options provided in the script itself:

```sh
#!/bin/bash
#SBATCH --account=PAS0471
#SBATCH --time=00:45:00
#SBATCH --mem=8G
```
]

---

## Compute job options: Project

- Your jobs need to be billed to a project, and you *always* need to specify one.  

<br> <br> <br> <br> <br> <br> <br> <br> <br> <br>

| Resource/use                 | short    | long      | default
|------------------------------|----------|-----------|:--------:|
| Project to be billed         | -A PAS1855 | --account=PAS1855 | N/A

```sh
#!/bin/bash
#SBATCH --account=PAS0471
```

---

## Compute job options: Nodes and cores

- Only ask for **>1 node** when you have explicit parallelization.

| Resource/use                  | short    | long                    | default
|-------------------------------|----------|-------------------------|:--------:| 
| Number of nodes               | -N 1     | --nodes=1               | 1
| Number of cores               | -c 1     | --cpus-per-task=1       | 1
| Number of "tasks" (processes) | -n 1     | --ntasks=1              | 1
| Number of tasks per node      | -        | --ntasks-per-node=1     | 1

```sh
$ sbatch -N 1 -c 2 myscript.sh
```

--

```sh
#!/bin/bash
#SBATCH --cpus-per-task=2       # Preferred for multi-threading
```

```sh
#!/bin/bash
#SBATCH --ntasks=2              # Preferred for multi-process
```

```sh
#!/bin/bash
#SBATCH --ntasks-per-node=2     # (Equivalent for single-node jobs)
```

---

## Compute job options: Time

You need to specify a **time limit** for your job (= "wall time"),
and your job gets killed as soon as it hits the time limit!

If you are uncertain about the time your job will take,
ask for (much) more time than you think you will need:
your project will be charged for the time *actually used*.
(But slight trade-off: shorter jobs are likely to start sooner.)

.content-box-info[
For single-node jobs, up to 168 hours (7 days) can be requested.
If that's not enough, you can request access to the `longserial` queue for jobs
of up to 336 hours (14 days).
]

---

## Compute job options: Time (cont.)

Acceptable time formats include:
  - `minutes`
  - `minutes:seconds`
  - `hours:minutes:seconds`
  - `days-hours`
  - `days-hours:minutes`
  - `days-hours:minutes:seconds`

<br> <br>

| Resource/use                 | short    | long               | default
|------------------------------|----------|--------------------|:-----:
| Time limit (4 h)             | -t 4:00:00 | --time=4:00:00   | ?

```sh
$ sbatch -t 30 myscript.sh   # 30 minutes
```

```sh
#!/bin/bash
#SBATCH --time=60            # 1 hour
```

---

## Compute job options: Memory

Specify a maximum amount of RAM (Random Access Memory;
not to be confused with disk storage).

Like with time, your job **gets killed** when it hits the memory limit!
But, as the [OSC documentation](https://www.osc.edu/supercomputing/batch-processing-at-osc/job-scripts)
says:

> There is no need to specify a memory limit unless your memory requirements
> are disproportionate to the number of cores you are requesting or you need a large-memory node.

---

## Compute job options: Memory (cont.)

- The default unit is MB (MegaBytes) &ndash; use "G" for GB.

<br> <br> <br> <br> <br> <br> <br> <br>

| Resource/use                 | short    | long      | default |
|------------------------------|----------|-----------|:-----|
| Memory limit per node (8 GB) | -        | --mem=8G  | single-core value

```sh
$ sbatch --mem=20G myscript.sh   # 20 GB
```

```sh
#!/bin/bash
#SBATCH --mem=20G
```

---
class: center middle inverse

# Questions?

-----

<br> <br> <br> <br>

---
class: inverse center middle
name:bonus

# Bonus material

-----

<br>

.left[
- ### [Transferring files in the shell](#shell-transfer)
- ### [ssh setup for quicker access](#shell)
- ### Working with $TMPDIR in SLURM scripts
- ### Working with Singularity contaners
]

<br><br>

---
name: shell-transfer
background-color: #f2f5eb

## Transferring files in the shell: small transfers (<1 GB)

With **`scp`** (secure copy), which works much like the regular `cp`:

- Local to remote:
  
  ```bash
  $ scp <local-path> <user>@pitzer.osc.edu:<remote-path>
  ```
  
- Remote to local is simply the reverse:
  
  ```sh
  $ scp <user>@pitzer.osc.edu:<remote-path> <local-path>
  ```
  
- For instance:
  
  ```sh
  $ scp -r scripts/ jelmer@pitzer.osc.edu:~/scripts
  $ scp -r scripts/ jelmer@pitzer.osc.edu:/fs/ess/PAS1855/users/jelmer/scripts
  
  $ scp -r jelmer@pitzer.osc.edu:~/scripts scripts/
  ```

.content-box-info[
Transfers in either direction should be done from your *local* shell.
]

---
background-color: #f2f5eb

## Transferring files in the shell: small transfers (<1 GB)

Another option is the **`rsync`** command, which I recommend,
especially when you want to keep certain folders synced: `rsync` won't copy
any files that are present & identical in source and destination.

The basic from-to syntax is the same as with `scp`.

- A useful combination of options is `-avz`:
  - `-a` enables archival mode (also makes `-r` unnecessary);
  - `-v` increases verbosity &ndash; tells you what is being copied.
  - `-z` enables compressed file transfer (=> faster).

  ```bash
  $ rsync -avrz ~/scripts jelmer@pitzer.osc.edu:~/scripts
  
  # If the files are large, "--progress" will help you keep track:
  $ rsync -avrz --progress ~/scipts jelmer@pitzer.osc.edu:~/scripts
  ```

---
background-color: #f2f5eb

## More `rsync`

.content-box-info[
Presence/absence of a trailing slash for source dirs in `rsync` makes a
difference: `rsync scripts bk/scripts` will sync these two dirs,
but `rsync scripts/ bk/scripts` will copy the **contents** of script:
you end up with `bk/scripts/scripts/` in the destination. 
]

---
background-color: #f2f5eb

## Transferring files in the shell: any transfer size

You can also use `sftp`, which doesn't run on a login node,
so large transfers are permitted.

  - To log in:
  ```bash
  $ sftp sftp.osc.edu
  ```

  Now, you have an `sftp` prompt instead of a regular bash prompt.

- To upload files to remote, use the `put` command &ndash; and note that you
  don't need the `user@pitzer.osc.edu:` to designate remote:

  ```sh
  sftp> put scripts/fastqc.sh /fs/ess/PAS1855/users/$USER/
  
  # If you don't specify a remote location, it be put in $HOME:
  sftp> put file.txt
  ```

- To download files from remote, use the `get` command:

  ```sh
  sftp> get /fs/ess/PAS1855/users/$USER/ ~/PP8300/OSC/
  ```

---
background-color: #f2f5eb
name: tmpdir

## Working with `$TMPDIR` in scripts

**Copy input to `$TMPDIR`, copy output to home:**

```bash
cp -R $HOME/my/data/ $TMPDIR/
#[...analyze data with I/O happening in $TMPDIR....]
cp -R $TMPDIR/* $HOME/my/data/
```

<br>

This trick will copy the data even if the job is killed:

```bash
trap "cd $PBS_O_WORKDIR;mkdir $PBS_JOBID;cp -R $TMPDIR/* $PBS_JOBID;exit" TERM
```

---
background-color: #f2f5eb
name: containers

## Containers versus Virtual Machines (VMs)

- Containers don't virtualize entire Operating Systems:
  - Containers are smaller.
  - Containers are more lightweight to run.
  
<p align="center">
<img src=img/containers.png width="65%">
</p>

.content-box-info[
- Use a VM for long interactive sessions with multiple programs.

- Use a container to run one or a few programs non-interactively.
]

---
background-color: #f2f5eb

## Docker versus Singularity containers

Docker and Singularity are two different container platforms;
Docker is by far the most widely used.
  
However, Singularity:
  
  - Was specifically designed for HPCs & is available at OSC
  
  - Needs no admin rights to run containers
  
  - Can work with Docker images but not vice versa
  
  - Integration with several host directories by default
  
  - Has single-file container images
  
  - Is open-source

---
background-color: #f2f5ebd

## Getting a container

Container images for most software can be found at these registries:

- Docker Hub: <https://hub.docker.com/>

- Sylabs Cloud Library: <https://cloud.sylabs.io/>

- Biocontainers (for bioinformatics software): <https://biocontainers.pro/>

If no suitable image is available, you can build your own!

<br>

--

.center[
**Container terminology**
]

| Term          | Meaning
|---------------|---------
| Image         | File(s) containing container application
| Container     | A *running* container image
| Definition file (Singularity) <br> / Dockerfile (Docker) | Recipe to build a container image


---
background-color: #f2f5eb

## Downloading a container image

- Download (`pull`) a container with MultiQC:

  ```sh
  $ singularity pull <registry>://<user>/<project>/<name>:<tag>
  
  $ singularity pull docker://quay.io/biocontainers/multiqc:1.9--py_1
  #> INFO:      Converting OCI blobs to SIF format
  ```
  
  (Note that it pulls a *Docker* image and converts it automatically.)

- Now we have `.sif` container image:

  ```sh
  $ ls multiqc*
  #> multiqc_1.9--py_1.sif
  ```

.content-box-info[
An example of downloading a container from Singularity hub ("shub"):

```sh
singularity pull shub://singularityhub/hello-world
#> INFO:      Downloading shub image
```
]

---
background-color: #f2f5eb

## Interacting with a container image

- To "run" the container, i.e. run whatever is defined as the default action:

  ```sh
  $ singularity run multiqc_1.9--py_1.sif
  Singularity> # In this case, opens terminal inside container 
  
  Singularity> multiqc --help # In which we can run multiqc
  #> Usage: multiqc [OPTIONS] <analysis directory>
  ```

- The `shell` sub-command *always* opens a terminal inside the container:

  ```sh
  $ singularity shell multiqc_1.9--py_1.sif
  Singularity>
  ```
  
- **Most useful is the the `exec` sub-command**,
  which lets you directly execute a shell command – and then exits:
  
  ```sh
  $ singularity exec multiqc_1.9--py_1.sif multiqc --help
  ```

---
background-color: #f2f5eb

## "Containerizing" an analysis or script

- Running a locally installed program:

  ```sh
  $ multiqc --help
  ```
  
- Run the same command inside a container:

  ```sh
  $ singularity exec multiqc_1.9--py_1.sif \
      multiqc --help
  ```
  
---
background-color: #f2f5eb

## `ssh` set-up: avoid being prompted for password

1. On your own computer, generate a key:
```bash
$ ssh-keygen -t rsa
```

1. On your own computer, transfer the key to the remote computer:
```bash
$ cat ~/.ssh/id_rsa.pub | ssh <user>@owens.osc.edu 'mkdir -p .ssh && cat >> .ssh/authorized_keys'
```

1. On remote computer (OSC), set permissions:
```bash
$ chmod 700 .ssh; chmod 640 .ssh/authorized_keys
```

1. Tell `ssh-agent` about the keys:
```bash
ssh-add
```

<br>

- For more details, see this
  [Tecmint post](https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/).

---
background-color: #f2f5eb

## `ssh` set-up: use shortcuts

1. Create a file called `~/.ssh/config`:
```bash
$ touch `~/.ssh/config`
```

1. Open the file and add your alias(es):
```bash
> Host <arbitrary-alias-name>    
>     HostName <remote-name>
>     User <user-name>
```

--

<br>

- This is what it looks like on my machine, so I can login using "`ssh jo`":
<br/>

<p align="center">
<img src=img/ssh.png width="400">
</p>

