<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 6 - Computing at OSC - I</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
    <link rel="stylesheet" href="slides_copy.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class:inverse middle center



## *Week 6: OSC jobs with SLURM*

----

# Part I: &lt;br&gt; An introduction to using OSC

&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;

### Jelmer Poelstra
### 2021/02/16 (updated: 2021-02-18)

---

## Overview of this week

Now that you have a solid basis in working in the shell and in shell scripting,
it's time to learn about OSC (beyond OnDemand) to enable you to run
analyses on the cluster.

- Today: An introduction to using OSC,
  including software usage and the SLURM resource manager.

- Thursday: SLURM jobs in practice.

---
class: inverse middle center

# Overview of this presentation

----

.left[
- ### [Introduction](#introduction)
- ### [Connecting to OSC](#connect)
- ### [OSC file systems](#filesystems)
- ### [Using software at OSC](#software)
- ### [Compute job options](#jobs)
]

---
class: inverse middle center
name: introduction

# Introduction

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

---

## What is the OSC?

- The **Ohio Supercomputer Center (OSC)** provides computing
  resources across the state of Ohio (it is not a part of OSU).

- OSC has two supercomputers and lots of infrastructure for their usage.

--

&lt;br&gt;

- Research usage is charged but via institutions
  at [heavily subsidized rates](https://www.osc.edu/content/academic_fee_model_faq).
  
- Educational usage, like our `PAS1855` project, is free!

---

## Supercomputer?

- A highly interconnected set of many processors and storage units.

- Also known as:
  - A **cluster**
  - A High-Performance Computing cluster (**HPC cluster**)

&lt;br&gt;

--

### When do we benefit from using a supercomputer?

- Our dataset is too large to be handled efficiently, or even at all,
  by our computer.

- Long-running analyses can be sped up by using more computing power.
  
- We need to repeat a computation many times.
  
---

## Learning about OSC

- OSC regularly has online introductory sessions,   
  both overviews and more hands-on sessions &amp;ndash;
  see the [OSC Events page](https://www.osc.edu/events).

--

- OSC provides excellent introductory material at their   
  [Getting Started Page](https://www.osc.edu/resources/getting_started).

&lt;p align="center"&gt;
&lt;img src=img/gettingstarted.png width="560"&gt;
&lt;/p&gt;

---

## Learning about OSC (cont.)

- Also have a look at all the
  ["HOWTO" pages](https://www.osc.edu/resources/getting_started/howto) &amp;ndash;  
  very specific and includes more advanced material.

&lt;br&gt;

&lt;p align="center"&gt;
&lt;img src=img/HOWTOs.png width="500"&gt;
&lt;/p&gt;

---

## Terminology: cluster and node

#### Cluster

- Many processors and storage units tied together.

- OSC currently has two clusters: **_Pitzer_** and **_Owens_**.   
  (Largely separate, but same long-term storage space is accessed by both.)

&lt;br&gt;

--

#### Node

- Essentially a powerful computer &amp;ndash; one of many in a cluster.

  - **Login node**: A node you end up on after logging in, which is not meant
  for computing. *(A few per cluster.)*
  
  - **Compute node**: A node for compute jobs. *(Hundreds per cluster.)*

---

## Terminology: core

#### Core

- One of often many *processors* in a node &amp;ndash;  
  e.g. standard *Pitzer* nodes have 40-48 cores.

--

- Each core can run and be reserved independently for jobs.

- But, using multiple cores for a job is also common, to:
  - Parallelize computations across cores.
  - Access more memory.


---

## Terminology: cluster &gt; node &gt; core

&lt;br&gt;

&lt;p align="center"&gt;
&lt;img src=img/cluster-node-core.png width="100%"&gt;
&lt;/p&gt;


---
class:inverse middle center
name:connect

# Connecting to OSC

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

---

## Connecting to OSC with "OnDemand"

- You can make use of OSC not only through `ssh` at the command line,   
but also through the web browser: from https://ondemand.osc.edu.

&lt;p align="center"&gt;
&lt;img src=img/ondemand1.png width="900"&gt;
&lt;/p&gt;

--

&lt;br&gt;

- At OnDemand, you can, for example:

  - Access a terminal directly in your browser.
  
  - Run "Interactive Apps" like RStudio, Jupyter Notebooks, and VS Code.
  
  - Browse and upload files.

  .content-box-diy[
  **Interface Demo**
  ]

---

## Connecting to OSC with SSH (Secure Shell)

- If you want to log in from your local terminal,   
  you need to **connect through SSH.**
  
- Basic `ssh` usage:
  
  ```sh
  $ ssh &lt;username&gt;@pitzer.osc.edu  # e.g. jelmer@pitzer.osc.edu     
  $ ssh &lt;username&gt;@owens.osc.edu   # e.g. jelmer@owens.osc.edu     
  ```
  
  You will be prompted for your OSC password.

--

.content-box-info[
See [this optional ungraded assignment](https://mcic-osu.github.io/pracs-sp21/)
for some SSH setup you can do to make this quicker:

- Avoid being prompted for your password.

- Use a *shortcut* for `&lt;username&gt;@&lt;cluster&gt;.osc.edu`.

There are also a few steps you can take to SSH tunnel to OSC with you local
VS Code installation.
]

---

## Login nodes: best practices

After logging in to OSC, you're on a **login node**.
  
- Use login nodes for *navigation*, *housekeeping*, and *job submission*.

- Any process that is active for &gt;20 minutes or uses &gt;1GB **will be killed**.

- It is good practice to avoid even getting close to these limits:   
  login nodes are shared by everyone, and can get clogged.  
  (See
  [this OSC page]((https://www.osc.edu/supercomputing/login-environment-at-osc))
  for more info.)

---

## Transferring files to and from OSC with GUIs

#### For small transfers (&lt;1 GB): use OnDemand

&lt;p align="center"&gt;
&lt;img src=img/ondemand2_circle.png width="85%"&gt;
&lt;/p&gt;

----

&lt;p align="center"&gt;
&lt;img src=img/ondemand3_circle.png width="85%"&gt;
&lt;/p&gt;

--

&lt;br&gt;

#### For large transfers (&gt;1 GB): use Globus

- Especially useful for very large and/or complex transfers.

- Does need a
  [local installation](https://www.osc.edu/resources/getting_started/howto/howto_transfer_files_using_globus_connect)
  and some set-up.

.content-box-info[
Next week, we'll talk about transferring data from the shell.
]


---
class: inverse center middle
name: filesystems

# File systems at OSC

----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

---

## File systems at OSC

OSC has [several file systems]((https://www.osc.edu/supercomputing/storage-environment-at-osc/available-file-systems) with different typical use cases:
we'll discuss this in terms of short-term versus long-terms storage. 

---

## File systems: long-term storage with daily back-ups

**Home dir**

- Can always be accessed with shortcuts **`$HOME`** and **`~`**.

- 500 GB limit per user.

- Your home dir will include your first project ID, e.g. `/users/PAS0471/jelmer`.
  (However, this is not the project dir!)

--

&lt;br&gt;

**Project dir**
  
- `/fs/project/&lt;project-ID&gt;` (MCIC project: `/fs/project/PAS0471`) **OR**  

- `/fs/ess/&lt;project-ID&gt;` (Course project: `/fs/ess/PAS1855`)
  
- The total storage limit is whatever the project's PI sets this to
  (and pays for...).
  Note that *charges are for the reserved space*, not the actual usage.
  
---

## File systems: short-term storage locations

**scratch**

- `/fs/scratch/&lt;projectID&gt;` or `/fs/ess/scratch/&lt;projectID&gt;`.

- Fast I/O &amp;ndash; good when working with large input and output files.

- **Temporary**: deleted after 120 days, and not backed up.

--

&lt;br&gt;

**`$TMPDIR`**

- Represents temporary storage space *on your compute node*. 1 TB max.

- Available in the job script through the environment variable `$TMPDIR`.

- **Deleted after job ends** &amp;ndash; so copy to/from in job scripts!

- See [this bonus slide](#tmpdir) for some example code for working with `$TMPDIR`.

---
class: inverse middle center
name: software

# Using software at OSC

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

---

## Software at OSC with Lmod (modules) 
 
- At OSC, software is managed with the Lmod system.
  While a lot of software is installed, **much is only available to use after
  we explicitly load it.**
  
  This enables the use of different versions of the same software
  and of mutually incompatible software one a single system. 

- For a list of software that has been installed at OSC, see:      
  &lt;https://www.osc.edu/resources/available_software&gt;.
  
---

## Software at OSC with Lmod (modules) (cont.)

- You can also search for available software ("modules") in the shell:
  
  - `module spider` list all/matching modules that are installed.
    
  - `module avail` lists all/matching modules that *can be directly loaded*,
    given the current environment.
  
  ```sh
  $ module spider
  #&gt; [long list ...]
  $ module spider python
  #&gt;    Versions:
  #&gt;      python/2.7-conda5.2
  #&gt;      python/3.6-conda5.2
  #&gt;      python/3.7-2019.10
  
  $ module avail
  #&gt; [long list ...]
  $ module avail python
  #&gt; python/2.7-conda5.2  python/3.6-conda5.2 (D)  python/3.7-2019.10
  ```

---

## Software at OSC with Lmod (modules) (cont.)

- *Loading* and unloading installed software is also done with `module` commands:
  
  ```bash
  # Load a module:
  $ module load python              # Load default version
  $ module load python/3.7-2019.10  # Load a specific version
  
  # Unload a module:
  $ module unload python
  ```
  
- To check which modules have been loaded:
  
  ```sh
  $ module list
  ```
  
  *(The list will include some modules that have been loaded automatically.)*
  
---

## What if software isn't installed at OSC?

**Install it yourself or get others to install it:**

- Installing yourself is possible but can be tricky (no admin rights...).

- For commonly used software, send an email to &lt;oschelp@osc.edu&gt;.

- People like me can help with (more niche) bioinformatics software.

--

&lt;br&gt;

**Alternative approaches (recommended):**

- The **Conda** software management system (next slides).

- `Singularity` **containers**.
  
- These alternatives are useful not only at OSC – but more generally, for:
  
  - Reproducible environments.
  
  - Portable environments (especially containers).
  
  - Avoidance of dependency hell.

---

## Conda: a software environment manager

Conda creates *environments* with one (best-practice) or more software packages,
which are activated and deactivated like with the `module` system.

When creating environments, Conda handles dependencies yet it does not require
admin rights.

---

## Conda: a software environment manager

**Benefits:**

- Can easily maintain distinct "environments" each with a different
  version of the same software, or with mutually incompatible software.

- Not much of a learning curve.

- Can save and share environment instructions as very simple text files.

- Lots of bioinformatics software is available as a Conda package.

&lt;br&gt;

--

**Limits &amp;ndash; Conda does not:**

- Create an isolated environment that can be exported and used anywhere.

- Allow you to run software that requires a different OS / OS version.


.content-box-info[
**Containers** address these limitations &amp;ndash; see
[these bonus slides](#containers).
]

---

## Conda in practice, briefly (more on Thursday)

- Load the Conda module at OSC (comes with the Python module): 

  ```bash
  $ module load python/3.6-conda5.2
  ```

- Example: **create** a new environment with "`cutadapt`" installed:

  ```bash
  $ conda create -y -n cutadapt-env cutadapt
  ```

--

- **Activate** the `cutadapt-env` environment and see if we can run Cutadapt:

  ```bash
  $ source activate cutadapt-env
  
  (cutadapt-env) $ cutadapt --version
  #&gt; 3.2
  ```

  *(Note above that Conda will indicate the active environment in the prompt!)*

---
class: inverse middle center
name: jobs

# Compute jobs

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;

---

## Starting a compute job: background

To do analyses at OSC, you need access to a **compute node**.
  
- Automated scheduling software allows 100s of people with different
  requirements to access compute nodes effectively and fairly.

- OSC uses the **SLURM** scheduler  
  (**S**imple **L**inux **U**tility for **R**esource **M**anagement).

&lt;br&gt;

.content-box-info[
Last year, OSC switched from PBS to SLURM &amp;ndash; see
[here](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration)
for migration info and a comparison of commands and options between the two
systems.
]

---

## Starting a compute job: how?

Three main options:

- In OnDemand, start an "**Interactive App**" like Jupyter Notebook or RStudio.   

- Run an **interactive shell job**.

- Provide *SLURM* directives in or along with **scripts**.

&lt;br&gt; &lt;br&gt;

.content-box-info[
Both interactive and non-interactive jobs start in the directory that they were submitted from: i.e., your working directory will remain the same.
]

---

## Passing on requests/instructions to SLURM

You can provide instructions to the *SLURM* scheduler to specify the resources
(cores/memory/time/etc) that you need.

These can be provided at the command-line:
```sh
$ sinteractive -A PAS1855 -t 60
$ sbatch -A PAS1855 -t 60 myscript.sh
```

As well as **at the start of** scripts in lines with the `#SBATCH` keyword:
```sh
#!/bin/bash
#SBATCH --account=PAS1855
#SBATCH --time=60
```

--

.content-box-info[
Many SLURM options have a long format (`--account=PAS1855`) and a short
format (`-A PAS1855`), which can generally be used interchangeably.
]

---

## Starting compute jobs: Interactive shell jobs

Interactive shell jobs will get you interactive shell access on a compute node.

- For short interactive shell jobs, use OSC's custom
  [`sinteractive`](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration/how_to_monitor_and_manage_jobs)
  wrapper:

  ```sh
  $ sinteractive -A PAS1855 # Default: 30-mins, 1-core, 1-node
  
  $ sinteractive -A PAS1855 -t 60 # 60 mins - max for sinteractive
  ```

  .content-box-info[
  `sinteractive` only accepts a selection of SLURM options and they have to be
  specified in short format.
  ]

--

&lt;br&gt;

- You can also use `srun` / `salloc`, which support all SLURM options:

  ```sh
  $ srun -A PAS1855 -t 60 --pty /bin/bash
  
  # Equivalent -- syntax that OSC uses in their docs:
  $ salloc -A PAS1855 -t 60 srun --pty /bin/bash
  ```

---

## Starting compute jobs: Shell scripts

- To submit a job, we simply preface the call to our script with `sbatch`:

  ```sh
  # Run a script on whatever node we are on:
  $ ./myscript.sh
  
  # Same, but now submit it to the queue:
  $ sbatch myscript.sh
  # Submitted batch job 2526085
  ```

--

&lt;br&gt;

- We can also provide `sbatch` options and/or script arguments:
  
  ```sh
  $ sbatch [sbatch-options] &lt;script&gt; [script-arguments]
  
  $ sbatch myscript.sh sampleA.fastq.gz
  # Submitted batch job 2526086
  
  $ sbatch -t 60 -A PAS1855 --mem=20G myscript.sh
  # Submitted batch job 2526087
  ```

---

## Starting compute jobs: Shell scripts (cont.)

.content-box-info[
Any `sbatch` options provided on the command-line will override the equivalent
options provided in the script itself.

Our script, `fastqc.sh`:

```sh
#!/bin/bash
#SBATCH --account=PAS1855
#SBATCH --time=00:45:00
#SBATCH --mem=8G
```

Submitting the script:

```sh
$ sbatch --mem=20G myscript.sh
```

In this case, the memory request will be `20G` and not `8G`.
]

---

## Compute job options: Project

- Your jobs need to be billed to a project ("account"),
  and you *always* need to specify one.  

&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;

| Resource/use                 | short    | long      | default
|------------------------------|----------|-----------|:--------:|
| Project to be billed         | -A PAS1855 | --account=PAS1855 | N/A

```sh
$ sbatch -A PAS1855 myscript.sh 
```

```sh
#!/bin/bash
#SBATCH --account=PAS1855
```

---

## Compute job options: Time

You need to specify a **time limit** for your job (= "wall time"),
and your job gets killed as soon as it hits the time limit!

- You will only be charged for the time your job *actually used*.

- Shorter jobs are likely to start sooner.

**If you are uncertain about the time your job will take,
ask for (much) more time than you think you will need.**

--

&lt;br&gt;

.content-box-info[
For single-node jobs, up to 168 hours (7 days) can be requested.
If that's not enough, you can request access to the `longserial` queue for jobs
of up to 336 hours (14 days).
]

---

## Compute job options: Time (cont.)

Acceptable time formats include:
  - `minutes`
  - `minutes:seconds`
  - `hours:minutes:seconds`
  - `days-hours`
  - `days-hours:minutes`
  - `days-hours:minutes:seconds`

&lt;br&gt; &lt;br&gt;

| Resource/use                 | short    | long               | default
|------------------------------|----------|--------------------|:-----:
| Time limit (4 h)             | -t 4:00:00 | --time=4:00:00   | 1:00:00

```sh
$ sbatch -t 30 myscript.sh   # 30 minutes
```

```sh
#!/bin/bash
#SBATCH --time=1:00:00       # 1 hour
```

---

## Compute job options: Memory

Specify a maximum amount of RAM (Random Access Memory) that your job can use.

- Like with the time limit, your job **gets killed** when it hits the memory limit.
  But this is not that common &amp;ndash; as the
  [OSC documentation](https://www.osc.edu/supercomputing/batch-processing-at-osc/job-scripts)
  says:

&gt; There is no need to specify a memory limit unless your memory requirements
&gt; are disproportionate to the number of cores you are requesting or you need a large-memory node.

- The default unit is MB (MegaBytes) &amp;ndash; use "G" for GB.

&lt;br&gt;

--

| Resource/use                 | short    | long      | default |
|------------------------------|----------|-----------|:-----|
| Memory limit per node (8 GB) | -        | --mem=8G  | *(usually 4G)*

```sh
$ sbatch --mem=20G myscript.sh   # 20 GB
```

```sh
#!/bin/bash
#SBATCH --mem=20G
```

---

## Compute job options: Nodes, cores, and tasks

| Resource/use                  | short    | long                    | default
|-------------------------------|----------|-------------------------|:--------:| 
| Number of nodes               | -N 1     | --nodes=1               | 1
| Number of cores (per task)    | -c 1     | --cpus-per-task=1       | 1
| Number of "tasks" (processes) | -n 1     | --ntasks=1              | 1
| Number of tasks per node      | -        | --ntasks-per-node=1     | 1

&lt;br&gt;

--

.content-box-warning[
- SLURM for the most part uses "**core**" and "**CPU**"
  (which often technically contains multiple cores) interchangeably.

- In this context, "**thread**" (a single core may contain multiple threads)
  is *also* used in the same way, so when we talk
  about *multi-threading*, think about using multiple cores.
  
  Similarly, when a program asks for `nThreads` or similar,
  this should be equal to the number of *cores* you have asked for (`-c`).
]

---

## Compute job options: Nodes, cores, and tasks

| Resource/use                  | short    | long                    | default
|-------------------------------|----------|-------------------------|:--------:| 
| Number of nodes               | -N 1     | --nodes=1               | 1
| Number of cores               | -c 1     | --cpus-per-task=1       | 1
| Number of "tasks" (processes) | -n 1     | --ntasks=1              | 1
| Number of tasks per node      | -        | --ntasks-per-node=1     | 1

.content-box-info[
- Only ask for **&gt;1 node** when you have explicit parallelization with
  something like MPI (uncommon in bioinformatics).

- Multi-threading, i.e. a single program run using multiple threads/cores,
  is much more common than having multiple processes in a single job.
  
- **For jobs with multi-threading, use `--cpus-per-task=n`  
  (and keep `--nodes` and `ntasks` at their defaults of 1).**
  
- For jobs with multiple processes (tasks), use `--ntasks=n` or
  `--ntasks-per-node=n` (but keep `--nodes=1` unless you use MPI or similar).
]

---

## Compute job options: Nodes, cores, and tasks

| Resource/use                  | short    | long                    | default
|-------------------------------|----------|-------------------------|:--------:| 
| Number of nodes               | -N 1     | --nodes=1               | 1
| Number of cores               | -c 1     | --cpus-per-task=1       | 1
| Number of "tasks" (processes) | -n 1     | --ntasks=1              | 1
| Number of tasks per node      | -        | --ntasks-per-node=1     | 1

```sh
#!/bin/bash
#SBATCH --cpus-per-task=2       # Preferred for multi-threading
```

```sh
#!/bin/bash
#SBATCH --ntasks=2              # Preferred for multi-process
                                # Or --ntasks-per-node=2, which is
                                # equivalent for single-node jobs
```

---
class: center middle inverse

# Questions?

-----

&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;

---
class: inverse center middle
name:bonus

# Bonus material

-----

&lt;br&gt;

.left[
- ### [Working with $TMPDIR in SLURM scripts](#tmpdir)
- ### [Working with Singularity containers](#containers)
]

&lt;br&gt;&lt;br&gt;

---
background-color: #f2f5eb
name: tmpdir

## Working with `$TMPDIR` in scripts

Copy input to `$TMPDIR`, copy output to home:

```bash
cp -R $HOME/my/data/ $TMPDIR/

#[...analyze data with I/O happening in $TMPDIR....]

cp -R $TMPDIR/* $HOME/my/data/
```

&lt;br&gt;

One potential problem with this approach is that your script may not reach
the final line, where the data is copied back: perhaps it encountered an error
or was killed. But you may still be interesting in getting the output data.

The following trick with `trap` will copy the data even if the job is killed:

```bash
trap "cd $PBS_O_WORKDIR;mkdir $PBS_JOBID;cp -R $TMPDIR/* $PBS_JOBID;exit" TERM
```

---
class: inverse middle center

# Working with Singularity containers

----

&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;

---
background-color: #f2f5eb
name: containers

## Containers versus Virtual Machines (VMs)

- Containers don't virtualize entire Operating Systems:
  - Containers are smaller.
  - Containers are more lightweight to run.
  
&lt;p align="center"&gt;
&lt;img src=img/containers.png width="65%"&gt;
&lt;/p&gt;

.content-box-info[
- Use a VM for long interactive sessions with multiple programs.

- Use a container to run one or a few programs non-interactively.
]

---
background-color: #f2f5eb

## Docker versus Singularity containers

Docker and Singularity are two different container platforms;
Docker is by far the most widely used.
  
However, Singularity:
  
  - Was specifically designed for HPCs &amp; is available at OSC
  
  - Needs no admin rights to run containers
  
  - Can work with Docker images but not vice versa
  
  - Integration with several host directories by default
  
  - Has single-file container images
  
  - Is open-source

---
background-color: #f2f5ebd

## Getting a container

Container images for most software can be found at these registries:

- Docker Hub: &lt;https://hub.docker.com/&gt;

- Sylabs Cloud Library: &lt;https://cloud.sylabs.io/&gt;

- Biocontainers (for bioinformatics software): &lt;https://biocontainers.pro/&gt;

If no suitable image is available, you can build your own!

&lt;br&gt;

--

.center[
**Container terminology**
]

| Term          | Meaning
|---------------|---------
| Image         | File(s) containing container application
| Container     | A *running* container image
| Definition file (Singularity) &lt;br&gt; / Dockerfile (Docker) | Recipe to build a container image


---
background-color: #f2f5eb

## Downloading a container image

- Download (`pull`) a container with MultiQC:

  ```sh
  $ singularity pull &lt;registry&gt;://&lt;user&gt;/&lt;project&gt;/&lt;name&gt;:&lt;tag&gt;
  
  $ singularity pull docker://quay.io/biocontainers/multiqc:1.9--py_1
  #&gt; INFO:      Converting OCI blobs to SIF format
  ```
  
  (Note that it pulls a *Docker* image and converts it automatically.)

- Now we have `.sif` container image:

  ```sh
  $ ls multiqc*
  #&gt; multiqc_1.9--py_1.sif
  ```

.content-box-info[
An example of downloading a container from Singularity hub ("shub"):

```sh
singularity pull shub://singularityhub/hello-world
#&gt; INFO:      Downloading shub image
```
]

---
background-color: #f2f5eb

## Interacting with a container image

- To "run" the container, i.e. run whatever is defined as the default action:

  ```sh
  $ singularity run multiqc_1.9--py_1.sif
  Singularity&gt; # In this case, opens terminal inside container 
  
  Singularity&gt; multiqc --help # In which we can run multiqc
  #&gt; Usage: multiqc [OPTIONS] &lt;analysis directory&gt;
  ```

- The `shell` sub-command *always* opens a terminal inside the container:

  ```sh
  $ singularity shell multiqc_1.9--py_1.sif
  Singularity&gt;
  ```
  
- **Most useful is the the `exec` sub-command**,
  which lets you directly execute a shell command – and then exits:
  
  ```sh
  $ singularity exec multiqc_1.9--py_1.sif multiqc --help
  ```

---
background-color: #f2f5eb

## "Containerizing" an analysis or script

- Running a locally installed program:

  ```sh
  $ multiqc --help
  ```
  
- Run the same command inside a container:

  ```sh
  $ singularity exec multiqc_1.9--py_1.sif \
        multiqc --help
  ```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "rainbow",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
