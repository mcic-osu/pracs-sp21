---
title: "Week 6 - OSC - I"
output:
  xaringan::moon_reader:
    seal: false
    css: ["default", "default-fonts", "slides.css", "slides_copy.css"]
    lib_dir: libs
    nature:
      highlightStyle: rainbow
      highlightLines: true
      countIncrementalSlides: false
---
class:inverse middle center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(eval = FALSE)
```

## *Week 6: OSC jobs with SLURM*

----

# Part I: <br> An introduction to OSC

<br> <br> <br> <br> <br>

### Jelmer Poelstra
### 2021/02/16 (updated: `r Sys.Date()`)

---
class: inverse middle center

# Overview

----

.left[
- ### [Introduction](#introduction)
- ### [Connecting to OSC](#connect)
- ### [OSC file systems](#filesystems)
- ### [Transferring files](#transfer)
- ### [Using software at OSC](#software)
- ### [Compute job options](#jobs)
]

---
background-color: #f2f5eb

## Some quick information about your Final Project

**The project should:**

- Be well-organized, and well-documented using Markdown READMEs.

- Be version-controlled throughout, and repeatedly "pushed" to Github.

--

- Be re-runnable using a single Snakemake control script ("Snakefile").

- Should have (most) analysis or data processing done using scripts  
  in bash and/or Python.

- Optionally run jobs at OSC.

---
background-color: #f2f5eb

## Some quick information about your Final Project

- So, lots of "infrastructure" requirements &mdash;
  therefore, no need to devise a complicated project with many scripts.

<br>

--

- We encourage you to *use your own data or find a dataset yourself*,  
  but Zach will also provide a dataset with accompanying project ideas if
  you prefer to do that. (And we can dig up other datasets if needed.)

<br>

--

- You are quite free regarding the **topic** of the project:
  
  - From answering an actual or contrived research question...
  
  - ...to automating something "boring" that you until now always did manually. 

---
background-color: #f2f5eb

## Some quick information about your Final Project

Your project proposal is not due until the week of March 23rd,
but you may want to slowly start thinking about this.

---
class: inverse middle center
name: introduction

# Introduction

-----

<br><br><br><br>

---

## What is the OSC?

- The **Ohio Supercomputer Center (OSC)** provides computing
  resources across the state of Ohio (it is not a part of OSU).

- OSC has two supercomputers and lots of infrastructure for their usage.

--

<br>

- Research usage is charged but via institutions
  at [heavily subsidized rates](https://www.osc.edu/content/academic_fee_model_faq).
  
- Educational usage, like our `PAS1855` project, is free!

---

## Supercomputer?

- A highly interconnected set of many processors and storage units.

- Also known as:
  - A **cluster**
  - A High-Performance Computing cluster (**HPC cluster**)

<br>

--

### When do we need a supercomputer?

- Our dataset is too large to be handled (efficiently) by our computer.

- Long-running analyses can be sped up by using more computing power.
  
- We need to repeat a computation (or have many datasets).
  
---

## Learning about OSC

- OSC regularly has online introductory sessions,   
  both overviews and more hands-on sessions &ndash;
  see the [OSC Events page](https://www.osc.edu/events).

--

- OSC provides excellent introductory material at their   
  [Getting Started Page](https://www.osc.edu/resources/getting_started).
**Do read these!**

<p align="center">
<img src=img/gettingstarted.png width="560">
</p>

---

## Learning about OSC (cont.)

- Also have a look at all the
  ["HOWTO" pages](https://www.osc.edu/resources/getting_started/howto) &ndash;  
  very specific and includes more advanced material.

<br>

<p align="center">
<img src=img/HOWTOs.png width="500">
</p>

---

## Terminology: cluster and node

#### Cluster

- Many processors and storage units tied together.

- OSC currently has two clusters: **_Pitzer_** and **_Owens_**.   
  (Largely separate, but same long-term storage space is accessed by both.)

<br>

--

#### Node

- Essentially a powerful computer &ndash; one of many in a cluster.

  - **Login node**: A node you end up on after logging in, which is not meant
  for computing. *(A few per cluster.)*
  
  - **Compute node**: A node for compute jobs. *(Hundreds per cluster.)*

---

## Terminology: core

#### Core

- One of often many *processors* in a node &ndash;  
  e.g. standard *Pitzer* nodes have 40-48 cores.

--

- Each core can run and be reserved independently.

- But, using multiple cores for a job is also common, to:
  - Parallelize computations across cores.
  - Access more memory.


---

## Terminology: cluster > node > core

<br>

<p align="center">
<img src=img/cluster-node-core.png width="100%">
</p>


---
class:inverse middle center
name:connect

# Connecting to OSC

-----

<br><br><br><br>

---

## Connecting to OSC with "OnDemand"

- You can make use of OSC not only through `ssh` at the command line,   
but also through the web browser: from https://ondemand.osc.edu.

<p align="center">
<img src=img/ondemand1.png width="900">
</p>

--

- At OnDemand, you can, for example:

  - Access a terminal directly in your browser
  
  - Run "Interactive Apps" like RStudio, Jupyter Notebooks, and VS Code.
  
  - Browse and upload files

  .content-box-diy[
  **Interface Demo**
  ]

---

## Connecting to OSC with `ssh`

- If you want to log in from your local terminal,   
  you need to **connect through `ssh`.**
  
- Basic `ssh` usage:
  
  ```sh
  $ ssh <username>@pitzer.osc.edu  # e.g. jelmer@pitzer.osc.edu     
  $ ssh <username>@owens.osc.edu   # e.g. jelmer@ownes.osc.edu     
  ```
  
  - You will be prompted for your OSC password.

---

## Login nodes: best practices

After logging in to OSC, you're on a **login node**.
  
- Use login nodes for *navigation*, *housekeeping*, and *job submission*.

- Any process that is active for >20 minutes or uses >1GB **will be killed**.

- It is good practice to avoid even getting close to these limits:   
  login nodes are shared by everyone, and can get clogged.   

<br>

.content-box-info[
For more info, see OSC's page [Login environment at OSC](https://www.osc.edu/supercomputing/login-environment-at-osc).
]

---
class: inverse center middle
name: filesystems

# OSC File Systems

<br><br><br><br>

---

## OSC file systems

- OSC has several file systems with different typical use cases:
  we'll discuss in terms of short-term versus long-terms storage. 

<br> <br> <br> <br>

.content-box-info[
See [here](https://www.osc.edu/supercomputing/storage-environment-at-osc/available-file-systems)
for more info about the OSC file systems.
]

---

## File systems: long-term storage with daily back-ups

**Home dir**

- Can always be accessed with shortcuts **`$HOME`** and **`~`**.

- 500 GB limit per user.

- Your home dir will include your first project ID, e.g. `/users/PAS0471/jelmer`.
  (However, this is not the project dir!)

--

<br>

**Project dir**
  
- `/fs/project/<project-ID>` (MCIC project: `/fs/project/PAS0471`) **OR**  

- `/fs/ess/<project-ID>` (Course project: `/fs/ess/PAS1855`)
  
- The total storage limit is whatever a PI sets this too.   
  Charges are for the reserved space, not actual usage.
  
---

## File systems: short-term storage locations

**scratch**

- `/fs/scratch/<projectID>` or `/fs/ess/scratch/<projectID>`.

- Fast I/O &ndash; good when working with large input and output files.

- **Temporary**: deleted after 120 days, and not backed up.

--

<br>

**`$TMPDIR`**

- Represents storage space *on your compute node*. 1 TB max.

- Available in the job script through the environment variable `$TMPDIR`.

- **Deleted after job ends** &ndash; so copy to/from in job scripts!

- See [this bonus slide](#tmpdir) for some example code for working with `$TMPDIR`.

---
class: inverse middle center
name: transfer

# Transferring files to and from OSC

-----

<br><br><br><br>

---

## Transferring files: OnDemand and Globus

#### For small transfers (<1 GB): use OnDemand

<p align="center">
<img src=img/ondemand2_circle.png width="95%">
</p>

----

<p align="center">
<img src=img/ondemand3_circle.png width="95%">
</p>

--

-----

<br>

#### For large transfers (>1 GB): use Globus

- Especially useful for very large and/or complex transfers.

- Does need a [local installation](https://www.osc.edu/resources/getting_started/howto/howto_transfer_files_using_globus_connect) and some set-up.

---
name: shell_transfer

## Transferring files in the shell: small transfers (<1 GB)

**Transfers in either direction should be done from your local shell.**

- With `scp` (secure copy), which works much like the regular `cp`:

  ```bash
  # Local to remote:
  $ scp <local-path> <user>@pitzer.osc.edu:<remote-path>
  
  # Remote to local is simply the reverse:
  $ scp <user>@pitzer.osc.edu:<remote-path> <local-path>
  
  # For instance:
  $ scp -r scripts/ jelmer@pitzer.osc.edu:~/scripts
  $ scp -r scripts/ jelmer@pitzer.osc.edu:/fs/ess/PAS1855/users/jelmer/scripts
  
  $ scp -r jelmer@pitzer.osc.edu:~/scripts scripts/
  ```

---

## Transferring files in the shell: small transfers (<1 GB)

**Transfers in either direction should be done from your local shell.**

- With `rsync`, which is recommended, especially when you want to keep folder
  synced:

  ```bash
  $ rsync -avrz --progress /path/in/local/dir \
      <user>@owens.osc.edu:/path/in/remote/
  ```

## Finish rsync

---

## Transferring files in the shell: any transfer size

You can also use `sftp`, which doesn't run on a login node,
so large transfers are permitted.

```bash
$ sftp sftp.osc.edu

sftp> put /path/in/local/file.txt /path/in/remote

sftp> put file.txt    # From local working dir to $HOME in remote

sftp> get /path/in/remote/file.txt /path/in/local/
```

## Finish sftp

---
class: inverse middle center
name: software

# Using software at OSC

-----

<br><br><br><br>

---

## Using software at OSC
 
- At OSC, software is managed with the `Lmod` system.
  While a lot of software is installed, **much is only available to use after
  we explicitly load it.**
  
  This enables the use of different versions of the same software,
  as well as mutually incompatible software, one one system. 

- For a list of software that has been installed at OSC, see:      
  <https://www.osc.edu/resources/available_software>.
  
--

- You can also search for available software ("modules") in the shell:
  
  - `module avail` lists all/ mtaching modules that *can be directly loaded*,
    given the current environment.
  
  - `module spider` list all / matching modules that *could potentially be loaded*.
  
  ```sh
  $ module avail
  $ module spider python
  
  $ module spider
  $ module spider python
  ```

---

## Using software at OSC

- *Loading* installed software is also done with `module` commands:
  
  ```bash
  # Load a module:
  $ module load R               # Load default version
  $ module load R/4.0.2-gnu9.1  # Load a specific version
  
  # Unload a module:
  $ module unload R
  
  # Check which modules have been loaded:
  $ module list
  ```

---

## What if software isn't installed at OSC?

**Install it yourself or get others to install it.**

- Installing yourself is possible but can be tricky (no admin rights...).

- For commonly used software, send an email to <oschelp@osc.edu>.

- People like me can help with (more niche) bioinformatics software.

--

<br>

**Alternative approaches (recommended):**

- The **`conda`** software management system (next slides)

- `Singularity` **containers**
  
- These alternatives are useful not only at OSC – but more generally, for:
  
  - Reproducible environments
  
  - Portable environments (especially containers)
  
  - Avoidance of dependency hell

---

## `conda`: a software environment manager

- Creates *environments* with one (best-practice) or more software packages.

- Lots of bioinformatics software is available as a `conda` package.

- Handles dependencies but does not require admin rights.

- Environments are activated and deactivated like with the `module` system.

---

## `conda`: a software environment manager

**Benefits:**

- Can easily maintain distinct "environments" each with a different
  version of the same software, or with mutually incompatible software.

- Not much of a learning curve.

- Can save and share environment instructions as very simple text files.

<br>

--

**Limits &ndash; `conda` does not:**

- Create an isolated environment that can be exported and used anywhere.

- Allow running software requiring a different OS / OS version.


.content-box-info[
**Containers** address these limitations &ndash; see
[these bonus slides](#containers).
]

---

## `conda` in practice

- Load the `conda` module at OSC (comes with the Python module): 

  ```bash
  $ module load python/3.6-conda5.2
  ```

- Example: **create** a new environment with "`multiqc`" installed:

  ```bash
  $ conda create -y -n multiqc-env -c bioconda multiqc
  ```

--

- **Activate** the `multiqc` environment and see if we can run MultiQC:

  ```bash
  [<user>@pitzer-login01 ~]$ conda activate multiqc-env
  (multiqc) [<user>@pitzer-login01 ~]$ multiqc --help
  #> Usage: multiqc [OPTIONS] <analysis directory>
  ```

  *(Note above that `conda` will indicate the active environment in the prompt!)*

.content-box-info[
There is also a bit of one-time `conda` setup that needs to be done,
which we will do on Thursday.
]

---
class: inverse middle center
name: jobs

# Compute jobs

-----

<br><br><br>

---

## Starting a compute job: background

To do analyses at OSC, you need access to a **compute node**.
  
- **Automated scheduling software** allows 100s of people with different
  requirements to access compute nodes effectively and fairly.

- Since December 2020, both Pitzer and Owens use the **_SLURM_ scheduler**   
  (see [here](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration)
  for OSC SLURM info).

---

## Starting a compute job: how?

Three main options:

- In OnDemand, start an "**Interactive App**" like RStudio Server.   

- Run an **interactive shell job**.

- Provide *SLURM* directives in or along with **scripts**.

--

.content-box-info[
You can provide instructions to the *SLURM* scheduler to specify the resources
(cores/memory/time/etc) that you need.

These can be provided at the command-line:
```sh
$ sinteractive -A PAS1855 -t 60
$ sbatch myscript.sh -A PAS1855 -t 60
```

As well as **at the start of** scripts in lines with the `#SBATCH` keyword:
```sh
#!/bin/bash
#SBATCH --account=PAS1855
#SBATCH --time=60
```
]

---

## Starting compute jobs: Interactive shell jobs

Interactive shell jobs will get you interactive shell access on a compute node.

- For short interactive shell jobs, you can use the
  [`sinteractive`](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration/how_to_monitor_and_manage_jobs) wrapper:

  ```sh
  $ sinteractive -A PAS1855 # Default: 30-mins, 1-core, 1-node
  
  $ sinteractive -A PAS1855 -t 60 # 60 mins - max for sinteractive
  ```

## srun

.content-box-info[
Jobs (interactive and non-interactive) start in the directory that they
were submitted from.
]

---

## Starting compute jobs: Shell scripts

- To submit a job:

  ```sh
  $ sbatch [sbatch-options] script.sh [script-arguments]
  
  $ sbatch fastq.sh
  # Submitted batch job 2526085 
  
  $ sbatch fastq.sh sampleA.fastq.gz
  # Submitted batch job 2526086
  
  $ sbatch -t 60 -A PAS1855 --mem=20G fastq.sh sampleA.fastq.gz
  # Submitted batch job 2526085 
  ```

.content-box-info[
Any `sbatch` options provided on the command-line will override the equivalent
options provided in the script itself:

```sh
#!/bin/bash
#SBATCH --account=PAS0471
#SBATCH --time=00:45:00
#SBATCH --mem=8G
```
]

---

## Compute job options: Project

- Your jobs need to be billed to a project, and you *always* need to specify one.  

<br> <br> <br> <br> <br> <br> <br> <br> <br> <br>

| Resource/use                 | short    | long      | default
|------------------------------|----------|-----------|:--------:|
| Project to be billed &ndash; **required** | -A PAS1855 | --account=PAS1855 | N/A

```sh
#!/bin/bash
#SBATCH --account=PAS0471
```

---

## Compute job options: Nodes and cores

- Only ask for >1 **node** when you have explicit parallelization.

| Resource/use                  | short    | long                    | default
|-------------------------------|----------|-------------------------|:--------:| 
| Number of nodes               | -N 1     | --nodes=1               | 1
| Number of cores               | -c 1     | --cpus-per-task=1       | 1
| Number of "tasks" (processes) | -n 1     | --ntasks=1              | 1
| Number of tasks per node      | -        | --ntasks-per-node=1     | 1

```sh
$ sbatch -N 1 -c 2 myscript.sh
```

```sh
#!/bin/bash
#SBATCH --cpus-per-task=2       # Preferred for multi-threading
```

```sh
#!/bin/bash
#SBATCH --ntasks=2              # Preferred for multi-process
```

```sh
#!/bin/bash
#SBATCH --ntasks-per-node=2     # (Equivalent for single-node jobs)
```

---

## Compute job options: Time

You need to specify a **time limit** for your job (= "wall time"),
and your job gets killed as soon as it hits the time limit!

If you are uncertain about the time your job will take,
ask for (much) more time than you think you will need:
your project will be charged for the time *actually used*.
(But slight trade-off: shorter jobs are likely to start sooner.)

.content-box-info[
For single-node jobs, up to 168 hours (7 days) can be requested.
If that's not enough, you can request access to the `longserial` queue for jobs
of up to 336 hours (14 days).
]

--

| Resource/use                 | short    | long               | default
|------------------------------|----------|--------------------|:-----:
| Time limit (4 h)             | -t 4:00:00 | --time=4:00:00   | ?

```sh
$ sbatch -t 30 myscript.sh   # 30 minutes
```

```sh
#!/bin/bash
#SBATCH --time=60            # 1 hour
```

.content-box-info[
Acceptable time formats include `minutes`, `minutes:seconds`,
`hours:minutes:seconds`,`days-hours`, `days-hours:minutes` and
`days-hours:minutes:seconds`.  
]

---

## Compute job options: Memory

Specify a maximum amount of RAM (Random Access Memory;
not to be confused with disk storage).

Like with time, your job **gets killed** when it hits the memory limit!
But, as the [OSC documentation](https://www.osc.edu/supercomputing/batch-processing-at-osc/job-scripts)
says:

> There is no need to specify a memory limit unless your memory requirements
> are disproportionate to the number of cores you are requesting or you need a large-memory node.

<br>

| Resource/use                 | short    | long      | default |
|------------------------------|----------|-----------|:-----|
| Memory limit per node (8 GB) | -        | --mem=8G  | single-core value

```sh
$ sbatch --mem=20G myscript.sh   # 20 GB
```

```sh
#!/bin/bash
#SBATCH --mem=20G
```

.content-box-info[
Default unit is MB (MegaBytes); use "G" for GB
]

---
class: center middle inverse

# Questions?

-----

<br> <br> <br> <br>

---
class: inverse center middle
name:bonus

# Bonus material

-----

<br>

.left[
- ### [ssh setup for quicker access](#shell)
]

<br><br>

---
background-color: #ededed

## `ssh` set-up: avoid being prompted for password

1. On your own computer, generate a key:
```bash
$ ssh-keygen -t rsa
```

1. On your own computer, transfer the key to the remote computer:
```bash
$ cat ~/.ssh/id_rsa.pub | ssh <user>@owens.osc.edu 'mkdir -p .ssh && cat >> .ssh/authorized_keys'
```

1. On remote computer (OSC), set permissions:
```bash
$ chmod 700 .ssh; chmod 640 .ssh/authorized_keys
```

1. Tell `ssh-agent` about the keys:
```bash
ssh-add
```

<br>

- For more details, see this
  [Tecmint post](https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/).

---
background-color: #ededed

## `ssh` set-up: use shortcuts

1. Create a file called `~/.ssh/config`:
```bash
$ touch `~/.ssh/config`
```

1. Open the file and add your alias(es):
```bash
> Host <arbitrary-alias-name>    
>     HostName <remote-name>
>     User <user-name>
```

--

<br>

- This is what it looks like on my machine, so I can login using "`ssh jo`":
<br/>

<p align="center">
<img src=img/ssh.png width="400">
</p>
  
---
background-color: #ededed
name: tmpdir

## Working with `$TMPDIR` in scripts

**Copy input to `$TMPDIR`, copy output to home:**

```bash
cp -R $HOME/my/data/ $TMPDIR/
#[...analyze data with I/O happening in $TMPDIR....]
cp -R $TMPDIR/* $HOME/my/data/
```

<br>

This trick will copy the data even if the job is killed:

```bash
trap "cd $PBS_O_WORKDIR;mkdir $PBS_JOBID;cp -R $TMPDIR/* $PBS_JOBID;exit" TERM
```

---
background-color: #ededed
name: containers

## Containers versus Virtual Machines (VMs)

- Containers don't virtualize entire Operating Systems:
  - Containers are smaller.
  - Containers are more lightweight to run.
  
<p align="center">
<img src=img/containers.png width="65%">
</p>

.content-box-info[
- Use a VM for long interactive sessions with multiple programs.

- Use a container to run one or a few programs non-interactively.
]

---
background-color: #ededed

## Docker versus Singularity containers

Docker and Singularity are two different container platforms;
Docker is by far the most widely used.
  
However, Singularity:
  
  - Was specifically designed for HPCs & is available at OSC
  
  - Needs no admin rights to run containers
  
  - Can work with Docker images but not vice versa
  
  - Integration with several host directories by default
  
  - Has single-file container images
  
  - Is open-source

---
background-color: #ededed

## Getting a container

Container images for most software can be found at these registries:

- Docker Hub: <https://hub.docker.com/>

- Sylabs Cloud Library: <https://cloud.sylabs.io/>

- Biocontainers (for bioinformatics software): <https://biocontainers.pro/>

If no suitable image is available, you can build your own!

<br>

--

.center[
**Container terminology**
]

| Term          | Meaning
|---------------|---------
| Image         | File(s) containing container application
| Container     | A *running* container image
| Definition file (Singularity) <br> / Dockerfile (Docker) | Recipe to build a container image


---
background-color: #ededed

## Downloading a container image

- Download (`pull`) a container with MultiQC:

  ```sh
  $ singularity pull <registry>://<user>/<project>/<name>:<tag>
  
  $ singularity pull docker://quay.io/biocontainers/multiqc:1.9--py_1
  #> INFO:      Converting OCI blobs to SIF format
  ```
  
  (Note that it pulls a *Docker* image and converts it automatically.)

- Now we have `.sif` container image:

  ```sh
  $ ls multiqc*
  #> multiqc_1.9--py_1.sif
  ```

.content-box-info[
An example of downloading a container from Singularity hub ("shub"):

```sh
singularity pull shub://singularityhub/hello-world
#> INFO:      Downloading shub image
```
]

---
background-color: #ededed

## Interacting with a container image

- To "run" the container, i.e. run whatever is defined as the default action:

  ```sh
  $ singularity run multiqc_1.9--py_1.sif
  Singularity> # In this case, opens terminal inside container 
  
  Singularity> multiqc --help # In which we can run multiqc
  #> Usage: multiqc [OPTIONS] <analysis directory>
  ```

- The `shell` sub-command *always* opens a terminal inside the container:

  ```sh
  $ singularity shell multiqc_1.9--py_1.sif
  Singularity>
  ```
  
- **Most useful is the the `exec` sub-command**,
  which lets you directly execute a shell command – and then exits:
  
  ```sh
  $ singularity exec multiqc_1.9--py_1.sif multiqc --help
  ```

---
background-color: #ededed

## "Containerizing" an analysis or script

- Running a locally installed program:

  ```sh
  $ multiqc --help
  ```
  
- Run the same command inside a container:

  ```sh
  $ singularity exec multiqc_1.9--py_1.sif \
      multiqc --help
  ```
