---
title: "Week 13 - Reproducible workflows with Snakemake"
output:
  xaringan::moon_reader:
    seal: false
    css: ["default", "default-fonts", "slides.css", "slides_copy.css"]
    lib_dir: libs
    nature:
      highlightStyle: rainbow
      highlightLines: true
      countIncrementalSlides: false
---
class:inverse middle center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(eval = FALSE)

icon::fa("user-edit")
```

```{r xaringan-extra, echo=FALSE, eval=TRUE}
#xaringanExtra::use_scribble()
#xaringanExtra::use_panelset()
#xaringanExtra::use_clipboard()
#xaringanExtra::use_search(show_icon = TRUE)
```

# *Week 13 &ndash; <br> Reproducible workflows with Snakemake*

----

# I: Workflows and workflow tools

<br> <br> <br> <br> <br>

### Jelmer Poelstra
### 2021/04/06 (updated: `r Sys.Date()`)

---

## Overview of this week

- I: Workflow tools &ndash; why and what (today)
  
- II: Getting started with Snakemake (today/Thu)

- III: Running Snakemake on a cluster & miscellaneous (Thu)

---

## What do we mean by a "pipeline" or "workflow"?

### TODO - Example workflow from exercises

---

## Why create a pipeline / workflow?

What are the advantages of creating a workflow rather than running scripts
one-by-one as needed?

- **Rerunning** everything is much easier -- you may want to do so after adding
  samples, changing a script, changing settings, and so on.

- **Re-applying** the same set of analyses in a different project is much easier. 

- The workflow is a form of **documentation** of the steps taken.

- Similarly, it makes sure you are **including all necessary steps** and
  are not overlooking a script or a manual step.

- **Reproducibility** in general is improved, e.g. it will be easier for others
  to repeat you analysis.

---

## An example partial workflow script

### TODO - Example workflow from exercises

---

## The need for specialized workflow tools

<p align="center">
<img src=img/nature-feature.png width=100%>
</p>

----


> Typically, researchers codify workflows using general scripting languages such as Python or Bash. But these often lack the necessary flexibility.

>Workflows can involve hundreds to thousands of data files; a pipeline must be able to monitor their progress and exit gracefully if any step fails. And pipelines must be smart enough to work out which tasks need to be re-executed and which do not.

---

## The need for specialized workflow tools?

<p align="center">
<img src=img/nature-feature.png width=100%>
</p>

<br>

> Bioinformatician Davis McCarthy at St Vincentâ€™s Institute of Medical Research in Fitzroy, Australia, says Python and R were more than enough for the relatively simple workflows he used as a PhD student.

---

## Challenges with workflows: <br> You need to rerun parts of it

**Rerunning *part* of the workflow may be necessary**, e.g. after:
  
- Some scripts fail.
- Some scripts fail for some samples only.
- When some steps are per-sample & others all-samples: adding a sample.
- You want to change a script or settings somewhere halfway the workflow.

<br>

--

**Ad-hoc solutions to some of these:**
  
- Comment out part of the workflow.
- Make tempory changes -- e.g. to globbing to only run a subset of samples,
  then change the pipeline afterwards to (again) use all samples.
  
--

**Labor-intensive solution to some of these:**

- Lots of command-line options and `if`-statements to enable flexible running of
  part of the pipeline, and to change settings.

---

## Challenges with pipelines: <br> managing cluster jobs

For any pipeline that has some steps operating on each sample separately,
and other steps on all samples together (most do!):

**How do you let the steps properly wait for each other?**

<br>

- **Easy way out** &ndash; but too time-consuming in many cases:  
  
  The whole pipeline is run as a single job, and per-sample steps are simply
  run 1-by-1 in a loop.

<br>

- **Possible solution** &ndash; but cumbersome and hard too manage for more
  complex pipelines:
  
  Use SLURM job dependencies. (We won't discuss those here.)

---

## Challenges with pipelines: <br> monitor and manage script failure

**How do you monitor job failure and automatically stop the pipeline upon job
failure?**

<br>

- **Easy way out** &ndash; but too time-consuming in many cases:   
  Again, if your entire pipeline is run linearly in a single job,
  you can use proper Bash settings and it will work. 

<br>

- **Possible solution** &ndash; but cumbersome and hard too manage for more
  complex pipelines:
  
  - Lots of testing for proper input for each script.

  - Test the exit status of each script. 

---

## Workflow management systems

Workflow tools, often called "workflow management systems",
provide ways to formally describe and execute workflows.

<br>

Some of the most commonly used command-line based options for working with
biological sequence data are:

- Snakemake
- Nextflow
- Common Worklow Language (CWL) 
- Workflow Description Language (WDL).

<br>

--

While there is a bit of a learning curve for any of these,
they solve the challenges discussed above, and offer many more benefits.

For all but the simplest workflows, they will eventually be huge time-savers,
both in terms of time spent composing the workflow and in running time of the
workflow.

---

## Advantages of formal workflow management &ndash; <br> all the buzzwords...

- **Reproducibility and transparency**
  - Document the workflow
  - Document dependencies among data and among code
  - Readability
  - Allows analysis of workflow graph
  - Integration with software management

- **Automation**

  - Detect changes in input files
  - Rerun failed steps

- **Portability**
  
  - Workflow can be easily rerun on a laptop, an HPC system,
    in the cloud, with or without containers.
    
---

## Advantages of formal workflow management (cont.)

- **Scalability**

  - Efficient execution because all dependencies are known,  
    and cluster submission is automated & optimized.
  - No need to loop over samples yourself

- **Flexibility** due to a separation of:
  
  - Generic nuts-and-bolts of the pipeline
  - Run-specific configuration &ndash; samples, directories, options
  - Things specific to the runtime environment (laptop vs cluster vs cloud)

---

<figure>
<p align="center">
<img src=img/giaa140fig1.jpeg width="90%">
<figcaption>From <a href="https://academic.oup.com/gigascience/article/10/1/giaa140/6092773/">Reiter et al. 2021: Streamlining data-intensive biology with workflow systems</a></figcaption>
</p>
</figure>

---
class: center middle inverse

# Questions?

-----

<br> <br> <br> <br>

---

## Advantages of formal workflow management

> The essence of what we're trying to do:
>
> - Automate a workflow
> - Document the workflow
> - Document the dependencies among data files, code
> - Re-run only the necessary code, based on what has changed
>
> &mdash; [Karl Broman](http://kbroman.org/Tools4RR/assets/lectures/01_intro.pdf)
