{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "author": [],
      "contents": "\nThis is the Github website for the course Practical Computing Skills for Biologists (a section of PP8300 – Current Topics in Plant Pathology), a 2-credit online-only course at Ohio State University during the Spring semester of 2021. The course is taught by Jelmer Poelstra from the MCIC.\nOnly slide decks, code-along materials, and exercises are hosted on this website. If you are a student in this course, your starting point should always be the CarmenCanvas site for this course. (Note that you can always find the link to the CarmenCanvas site in the top-right corner of this site by clicking on the graduation cap icon.)\n\n\n\n",
      "last_modified": "2021-03-16T14:32:59-04:00"
    },
    {
      "path": "final_project.html",
      "title": "General information about your final project",
      "author": [],
      "contents": "\n\nContents\nDevising a project\nGraded aspects\nSteps (checkpoints)\n\nDevising a project\nThe goal for your final project is to apply some of the things you have learned during this course, and allow you to get you more practice. While some aspects are required in order to get a good grade (see Graded aspects below), you have a fair amount of freedom. I recommend that you take advantage of that to make your final project as useful as possible for your own research and/or personal development.\nGraded aspects for the project focus on documentation, reproducibility, and automation. Accordingly, there are no real requirements for the level of complexity or sophistication, the number of scripts, the real-world usefulness, and so on. I would in fact recommend you take care not to be too ambitious in this regard: start small and then expand if you are able to.\nSome examples of possible types of projects:\nYou may want to work with your own data or a publicly available dataset that is similar to what you are expecting to work with (though subsetting the data will be useful if you have a large genomic dataset!).\nYou may decide that just focusing on the coding and on creating a reproducible workflow is what is most useful for you, and that this may be easier with a trivial and maybe not-so-relevant dataset so you don’t get bogged down in other details.\nYou may think of trying to automate something “boring” and/or repetitive that you did manually until now.\nIf you would like to work with publicly available fungal genomic data, you should consider using your TA Zach’s MycoTools. You can find some general information in his README and more details usage information in his usage guide. If this looks interesting to you, contact Zach to get more information and to develop a project idea.\nIf you need help with selecting a dataset or a project topic, don’t hesitate to contact me (Jelmer). I don’t currently have any ready-to-go project ideas with accompanying data sets, but I could make one or more, if needed.\n\nGraded aspects\nGraded aspects of your project focus on appropriate usage of many of the tools and principles we covered during the course.\nTo receive a high grade, your project should:\nBe well-organized: contained in a single parent directory with a clear and sensible structure of subdirectories, descriptive file and directory names, no files floating around with unclear purpose or source, and so on.\nBe well-documented, with at least one README in Markdown format in the root directory of your project, and preferably with additional READMEs elsewhere as appropriate.\nBe version-controlled with Git throughout: with regular, meaningful commits. You will also need to push to GitHub for the proposal, draft, and final submission checkpoint.\nContain scripts in Bash and/or Python that do data processing and/or analysis (see also the last point). Try not to do any manual work such as editing a data file in a text editor or Excel to fix column names, since this hinders reproducibility.\nHaving some components in other languages is fine, in case you know how to do things there that we didn’t learn in the course (for example, doing some plotting in R).\nSimilarly, it is fine to call external, command-line programs (e.g. some of the bioinformatics programs we have run, or any program that may be useful for your research).\nIf your project’s heavy lifting consists mostly or almost entirely of calling such programs, though, it becomes especially important that you are creating a reproducible and easily rerunnable pipeline (see below).\n\nRun one or more scripts as SLURM jobs at OSC.\n(A locally run project could be okay if your research does not and will not require the use of supercomputers like those at OSC – check in with me if this is the case and you would therefore prefer not to use OSC.)\nBe easily re-runnable using a Snakefile or a “master” / “controller” script that glues the entire project together.\nIt is not enough to include a README.md that explains what you did or that provides instructions how to rerun the project, even if in detail. Such information is very useful, but the goal here is to provide a way to rerun the project with a single command – a command that I could also run.\nPreferably, this is done using single Snakemake control script (“Snakefile”), but a shell or Python script may also work.\n(This is a challenging aspect, and your grade won’t plummet if you don’t succeed, but it is important to try!)\n\nSteps (checkpoints)\nInformation about expectations for each of the steps (checkpoints) for the project will be provided in the content for individual weeks. Below is an overview which will be updated with the links:\nProposal – due March 23\nDraft – due April 6\nIn-class presentations – April 13 and 15\nFinal submission – due April 23\n\n\n\n",
      "last_modified": "2021-03-16T14:32:59-04:00"
    },
    {
      "path": "finalproject_proposal.html",
      "title": "Final project: proposal",
      "author": [],
      "contents": "\nWrite a concise, informal summary of what you plan to do for your final project (due Tuesday, Mar 23). [10 points]\n\nFor general information about devising a project and expectations for your project, please see this page.\nIf you want to primarily use Python in your project, and are unsure about the possibilities because we are still in the middle of the section on Python, you can contact me to push back your due date for this proposal.\n\nSome pointers:\nCreate a directory for your project and start a Git repository. This will be a repository for the project as a whole, not just for this proposal. [1]\nWrite the proposal in Markdown and include it in your repository. Like we did with the graded assignments, when you’re done, push your repository to GitHub and tag me (@jelmerp) in an “Issue” on GitHub (please make sure to tag me in the text body for the issue, it won’t be parsed as a tag if you do this in the title of the issue). [1]\nIn the proposal, start with a general description of what your project will be about, without going into detail about coding languages and approaches. Describe what data you will work with and what kind of output your project will produce. (This can be pretty minimal if your project is “code-first” with a trivial data set.) [2]\nNext, summarize how you envision the more technical aspects: in which language(s) do you intend to code, what do you think you will need separate scripts for, and how will you structure results? Will you mostly be coding your data processing/analyses from scratch, or are you primarily running external programs? [2]\nCheck the list of graded aspects on the page with general information about the project. If you are not sure you will be able to fulfill one of these, or intend to skip something (e.g. OSC SLURM jobs), mention that here.\nBriefly mention which aspects of your project you are uncertain about, for instance because you intend to include some topics that we have yet to cover in the course, or because something will depend on how other things go. [1]\nFinally, briefly describe why you chose to pick this project. Because it will be useful for your research? Because it gives you practice with coding topics you like or that you want to / need to get better with? All of the above and/or something else? [1]\n\n\n\n",
      "last_modified": "2021-03-16T14:33:00-04:00"
    },
    {
      "path": "index.html",
      "title": "",
      "author": [],
      "contents": "\n\n\nPractical Computing Skills for Biologists  Spring 2021  A section of Current Topics in Plant Pathology (PLNTPTH 8300)\n\n\n\n\n",
      "last_modified": "2021-03-16T14:33:00-04:00"
    },
    {
      "path": "overviews_git.html",
      "title": "Topic Overview: Git",
      "author": [],
      "contents": "\n\nContents\nKey commands\nMiscellaneous\nTips and tricks\nExplanatory figures\nAdvanced (bonus)\nUndoing changes and viewing the past\nMerge conflicts\nForking and Pull Requests\nMiscellaneous\n\n\nKey commands\nCommand\nExplanation\nExamples\nadd\nStage files to be committed, including previously untracked files.\nTherefore, git add does two things for untracked files at once: start tracking them and stage them. Files that are already tracked are merely staged by git add.\nStage a single file: git add file.txt Stages all eligible files, tracked and untracked: git add --all Stage shell scripts anywhere in project: git add *.sh\ncommit\nCommit all currently staged changes to create a snapshot for the repository (that you could always return to later on).\ngit commit -m \"Started the book\" git commit -am \"Fix infinite loop bug\"      -m follow with commit message      -a Also stage all changes (but won’t add untracked files)\nstatus\nGet the status of your repository: which files have changed since the last commit, which untracked files are present, etc.\n\nlog\nSee the repository’s commit history.\ngit log 1-line summary for each commit: git log --oneline Show all branches in “graph” form (useful with >1 branch): git log --oneline --graph --all\ndiff\nBy default, show changes between the working dir and:\nThe stage (index) if something has been staged.\nThe last commit if nothing has been staged.\n\nLast commit vs second-to-last commit - full repo: git diff HEAD HEAD^ Last commit vs a specified commit - specific file: git diff HEAD d715c54 todo.txt Show changed between stage and last commit: git diff --staged\nmv\nMove files that are tracked by Git: better to use git mv than regular mv so Git will immediately register what happened properly.\ngit mv old.txt new.txt\nrm\nDelete files that are tracked by Git: better to use git rm than regular rm so Git will immediately register what happened properly.\ngit rm tmp.txt\nbranch\nVarious functionality for Git branches, such as creating, removing, and listing branches. (To switch between branches, use git checkout.)\nList existing branches: git branch Create a new branch named “my-new-branch”: git branch my-new-branch Rename the current branch to “main”: git branch -M main Remove a branch that is no longer needed: git branch -d fastercode\ncheckout\ngit checkout has multiple functions:\ngit checkout <branch-name> will switch between branches.\ngit checkout -- <file> will revert the specified file back to its last committed state.\ngit checkout <commit-id> will move you to the specified commit for looking around.\n\nMove to the master branch: git checkout master\nmerge\nMerge a specified branch into the current branch. When doing a merge, make sure that you are on the branch that you want to keep, and from there, merge the branch whose changes you want to incorporate.\ngit merge fastercode -m \"my msg\"\nremote\nInteract with “remote” counterparts of repositories: in our case, GitHub repositories.\nList the repo’s remote connections: git remote -v  Add a remote (syntax: git remote add <nickname> <URL>): git remote add origin git@github.com:me/my.git\npush\nUpload (changes to) your local repository to an online counterpart, in our case a GitHub repository.\nPush currently active branch to default remote connection: git push Whenever we push a branch for the first time, we need to use the -u option to set up an “upstream” counterpart: git push -u origin main\npull\nDownload (changes from) an online counterpart to your local repository (in our case a GitHub repository) and merge (as in git merge) the changes into your local repo. May result in a merge conflict if you are collaborating with others on the remote repo.\nPull from currently active branch in remote repo: git pull\nclone\nDownload an only repository and create a local copy.\ngit clone https://github.com/CSB-book/CSB.git\ntag\nAdd a “tag” to a commit, for instance to mark the version of the scripts used in the analysis for your paper. ddddddddddddddddddddddddddddddddddddd\ngit tag -a v1.2.0 -m \"Publish version\" Tags need to be explicitly pushed like so: git push --follow-tags\n\nMiscellaneous\nCode / concept\nExplanation\nExamples\nmodule load git\nLoad the OSC Git module. If you forget to do this, Git will still work but you’ll be using a very old version.\n\n.gitignore\nA file that tells Git what (kinds of) files to ignore, i.e. not to list among “Untracked files”.\n.gitignore is usually (recommended!) in the top-level repo dir.\nDon’t forget to add and commit the .gitignore file.\nNote that files won’t be “retroactively ignored”: files that are already in the repo need to be explicitly removed.\n\nExample entries: Ignore everything in the dir “data”: data/ Ignore all files ending in ~: *~ Ignore gzipped FASTQ files in the entire repo: *.fastq.gz\nHEAD\nA pointer to the most recent (last) commit on the current branch. Note that as a pointer, HEAD can move and will do so e.g. if you switch between branches.\n\nHEAD^, HEAD^^\nPointer to the second-to-last and third-to-last (and so on) commit.\n\nHEAD~1, HEAD~2\nPointer to the second-to-last and third-to-last (and so on) commit: easier than ^ as you get further back, e.g. HEAD~8. ddddddddddddddddddddddddddddddddddddd\nddddddddddddddddddddddddddddddddddddd\n\nTips and tricks\nUse informative commit messages! Think about yourself looking for some specific changes that you made in the repository a year ago: what would help you to find them?\nCommit often, using small commits. This will also help to keep commit messages informative.\nDon’t include unrelated sets of edits in a single commit. If you have worked on two disparate things in your project since the last commit, git add + git commit them separately.\nWhen collaborating: pull often. This will reduce the chances of merge conflicts.\nDon’t commit unnecessary files: include a .gitignore file to ignore these.\nDon’t have nested repositories. For example, in the dir structure work/projects/gwas/, you would have a Git repository for in the work dir and in the gwas dir.\nBe very careful with destructive commands like git reset at the commit level, or simply deleting your .git directory. As you get started with Git, best to make backup copies before you try these.\nFile and repo size limitations:\nBinary files cannot be tracked as effectively as plain text files: Git will just save a new version whenever there has been a change.\nRepository size: Best to keep individual repositories under a total size of 1 GB.\nFile size: GitHub will not allow files over 100 MB.\n\nExplanatory figures\n\n\nReferring to past commits.\n\n\nGit’s three areas (“trees”), staging, and committing.\n\n\nOverview of ways to undo changes that have not been committed.\n\nAdvanced (bonus)\nUndoing changes and viewing the past\nI need to\nUse this\nExamples\nUndo unstaged changes to a file\ngit checkout -- <file>\nNote: for the most recent Git versions (not yet on OSC), the recommended method is the new command git restore.\nAfter accidentally deleting README.md or overwriting it instead of appending to it: git checkout -- README.md\nUnstage a file\ngit reset HEAD <filename>\nNote: for the most recent Git versions (not yet on OSC), the recommended method is the new command git restore --staged.\nAfter accidentally staging README.md which was not supposed to be part of the next commit: git reset HEAD README.md\nUndo staged changes to a file\ngit checkout HEAD -- <filename>.\nNote: this irrevocably discards the non-committed changes (your data is only safe with Git once it has been committed!).\nAfter accidentally deleting README.md or overwriting it instead of appending to it, AND staging these changes: git checkout HEAD -- README.md\nUndo ALL unstaged changes (all files)\ngit checkout -- .\nAfter making edits to several files that you all want to get rid of, to revert back to the last commit: git checkout -- .\nUndo ALL staged and unstaged changes (all files)\ngit reset --hard HEAD\nThis will take you back to the state of your repository right after the last commit.\n\nView the state of my repo for a past commit\ngit checkout <commit-id>.\nThis is only for “looking around”. If you decide you want to go back to your repo as it was in an earlier commit, use git revert or git reset (see below).\ngit checkout 4dce25f git checkout HEAD^^ To go back, assuming you’re on branch master: git checkout master\nUndo one or more commits\nTo undo one or more commits, i.e. to roll the state of your repository back to how it was before the commit you want to undo, there are two main commands:\ngit revert: Undo the changes made by commits by reverting them in a new commit. (Safe – does not change history.)\ngit reset: Delete commits as if they were never made. (Unsafe – changes history.)\n(git reset HEAD^ undoes changes made by the last commit as it resets to the second-to-last commit.)\nUndo changes by most recent commit: git revert HEAD Undo changed by second-to-last commit: git revert HEAD^ Undo changes by any arbitrary commit: git revert e1c5739 Undo last commit and completely discard all changes made by it: git reset --hard HEAD^ Undo last commit and put all changes made by that commit in the working dir: git reset HEAD^ Undo last commit and stage all changes made by it (“peel off commit”): git reset --soft HEAD^\nRetrieve a file version from a past commit dddddddddddddddddddd\ngit checkout --  or git show ddddddddddddddddddddddddddddddddddddd\nRetrieve from second-to-last commit: git checkout HEAD^^ -- README.md Retrieve from arbitrary commit: git checkout e1c5739 -- README.md To not change after all and go back to the current version: git checkout HEAD -- README.md Look at the version from the last commit: git show HEAD:README.md Revert to arbitrary earlier version using redirection: git show ad4ca74:README.md > README.md ddddddddddddddddddddddddddddddddddddd\n\nMerge conflicts\nA merge conflict can occur when all three of the following conditions are met:\nYou try to merge two branches (including when pulling from remote: recall that a pull includes a merge).\nOne or more files have been changed (via commits) on both of these branches since their divergence.\nSome of these changes were made in the same part(s) of file(s).\nWhen this occurs, Git has no way of knowing which changes to keep, and will report a merge conflict. When it does so, your merge process has basically been paused, and Git wants you to make changes to resolve the conflict, after which a git commit will complete the merge. To resolve a merge conflict:\nUse git status to find the conflicting files.\nOpen and edit those files manually to a version that fixes the conflict. Note that Git has added <<<<<<<, =======, and >>>>>>> to help you see the conflict and that you will want to remove.\nUse git add to tell Git you’ve resolved the conflict in a particular file.\nUse git status to check that all changes are staged, at which point Git should tell you “All conflicts fixed but you are still merging. (use \"git commit\" to conclude merge)”.\nUse git commit -m \"My merge message\" to conclude the merge.\n\nForking and Pull Requests\nForking is a GitHub concept that will create a personal GitHub repo that remains linked to the source repository. For instance, you could keep your fork up-to-date with the source, and you could issue a Pull Request from your fork. See these slides for details and screenshots.\n\nMiscellaneous\nCode / concept\nExplanation\nExamples\nstash\ngit stash temporarily saves files. It can be useful when you need to pull from remote, but are prevented from doing so because you have changes in your working dir (and those are not appropriate for a separate commit / new branch). In such a case, stash the changes, pull, and get your stashed changes back (see example on right).\nStash changes to tracked files:git stash (Add -u to include untracked files) Pull from the remote repository: git pull Apply stashed changes: git stash apply\ncommit --amend\nAdd something to a commit that has already been made, for instance if you notice a type or forgot a file. Note: this “changes history” and as such is not recommended by everyone – at any rate, do not amend commits that have been pushed online!\ngit commit --amend --no-edit (No edit will keep the same commit message)\ncheckout -b\nCreate a new branch and move to it immediately.\ngit checkout -b my-new-branch\ngit log <filename>\nShow commits in which a specific file was changed (!). ddddddddddddddddddddddddddddddddddddd\ngit log my-failing-script.sh\n\n\n\n\n",
      "last_modified": "2021-03-16T14:33:01-04:00"
    },
    {
      "path": "overviews_markdown.html",
      "title": "Topic Overview: Markdown",
      "author": [],
      "contents": "\n\nContents\nMarkdown syntax\nWhitespace\nTables\nHTML\nRendering\n\n\n\n\nMarkdown syntax\nMentioned extended syntax is not supported by all interpreters, but it is by GitHub-flavored Markdown among others.\nSyntax\nResult\n*italic*\nitalic (alternative: single underscore _)\n**bold**\nbold (alternative: double underscore __)\n<https://website.com>\nClickable link: https://website.com\n[link text](website.com)\nLink with custom text: link text\n![](path/to/figure.png)\nFigure\n# My Title\nHeader level 1 (largest)\n## My Section\nHeader level 2\n### My Subsection\nHeader level 3 – and so forth\n- List item - Another item - Third item\nBulleted (unordered) list\n1. List item 1. Another item 1. Third item\nNumbered (ordered) list  (numbering will be automatic)\n`inline code`\ninline code formatting\n``` or 4 leading spaces\nStart/end of generic code block (``` is extended syntax)\n```bash\nStart of bash-formatted code block (end with ```)\n---\nHorizontal rule (line)\n> Text\nBlockquote (think quoted text in email)\nblank line\nNew paragraph (white space between lines)\ntwo spaces at end of line\nForce a line break. ddddddddddddddddddddddddddddddddddddddddddddd\n~~strikethrough~~\nstrikethrough (extended syntax)\nFootnote ref[^1]\nFootnote ref1(extended syntax)\n[^1]: Text\nThe actual footnote (extended syntax)\nWhitespace\nMarkdown generally ignores single line breaks: use two spaces at the end of a line to force a line break, or when appropriate, leave a blank line to start a new paragraph.\nMultiple consecutive blank lines (or spaces) will be treated the same as a single blank line (or space). For extra vertical whitespace, use the HTML tag <br>, each of which forces a new line break.\nTables\nNote that the vertical bars | don’t have to be aligned.\n| blabla | blabla |  |———–|———–|\n| blabla | blabla |\n| blabla | blabla |\nblabla\nblabla\nblabla\nblabla\nblabla\nblabla\nHTML\nMost Markdown interpreters accept HTML syntax. Some simple examples:\nFigures – a centered figure using 50% of the page width:\n<p align=\"center\">\n<img src=my.png width=50%>\n<\/p>\nText colors:\ninline <span style=\"color:red\">colored<\/span> text\ninline colored text\nOther:\nSyntax\nResult\nsuperscript<sup>2<\/sup>\nsuperscript2\nsuperscript<sub>2<\/sub>\nsubscript2\n<br>\nlinebreak / empty line\n<!-- text -->\ncomment\nRendering\nUse Pandoc:\npandoc README.md > README.html\npandoc -o README.pdf README.md\nGitHub will automatically show rendered versions of Markdown files.\nIn VS Code, when editing a Markdown file, click *Open Preview to the Side* to see a preview.\n\n\n\n",
      "last_modified": "2021-03-16T14:33:01-04:00"
    },
    {
      "path": "overviews_shell.html",
      "title": "Topic Overview: Shell",
      "author": [],
      "contents": "\n\nContents\nBasic commands\nData tools\nMiscellaneae\nShell wildcards\nRegular expressions\nDetails for selected commands\nless\nsed\nsed flags:\nawk\n\nKeyboard shortcuts\n\n\n\n\nBasic commands\nCommand\nDescription\nExamples / options\npwd\nPrint current working dir.\npwd\nls\nList files in working dir (default) or elsewhere.\nls data/      -l long format      -h human-readable file sizes      -a show hidden files\ncd\nChange working dir.\ncd /fs/ess/PAS1855 cd ../.. (Two levels up) cd - (To previous dir)\ncp\nCopy files or dirs/recursively (with -r).  If target is a dir, file will keep same name; otherwise a new name can be provided.\ncp *.fq data/ (All .fq files into dir data) cp my.fq data/new.fq (With new name) cp -r data/ ~ (Copy dir and contents) \nmv\nMove/rename files or dirs (-r not needed).  If target is a dir, file will keep same name; otherwise a new name can be provided.\nmv my.fq data/ (Keep same name) mv my.fq my.fastq (Simple rename) mv file1 file2 mydir/ (Last arg is destination)\nrm\nRemove files or dirs/recursively (with -r).  With -f (force), any write-protections that you have set will be overridden.\nrm *fq (Remove all matching files) rm -r mydir/ (Remove dir & contents)      -i Prompt for confirmation      -f Force remove\nmkdir\nCreate a new dir.  Use -p to create multiple levels at once and to avoid an error if the dir exists.\nmkdir my_new_dir mkdir -p new1/new2/new3\ntouch\nIf file does not exist: create empty file.  If file exists: change last-modified date.\ntouch newfile.txt\ncat\nPrint file contents to standard out (screen).\ncat my.txt cat *.fa > concat.fq (Concatenate files)\nhead\nPrint the first 10 lines of a file or specify number with -n <n> or shorthand -<n>.\nhead -n 40 my.fq (print 40 lines) head -40 my.fq (equivalent)\ntail\nLike head but print the last lines.\ntail -n +2 my.csv (skip first line) tail -f slurm.out (“follow” file)\nless\nView a file in a file pager; type q to exit. See below for more details.\nless myfile      -S disable line-wrapping\ncolumn -t\nView a tabular file with columns nicely lined up in the shell.\nNice viewing of a CSV file: column -s \",\" -t my.csv\nhistory\nPrint previously issued commands.\nhistory | grep \"cut\" (Find previous cut usage)\nchmod\nChange file permissions for file owner (user, u), “group” (g), others (o) or everyone (all; a). Permissions can be set for reading (r), writing (w), and executing (x). ddddddddddddddddddddddddddddddddddddd\nchmod u+x script.sh (Make script executable) chmod a=r data/raw/* (Make data read-only)      -R recursive ddddddddddddddddddddddddddddddddddddddddddddd\n\nData tools\nCommand\nDescription\nExamples and options\nwc -l\nCount the number of lines in a file.\nwc -l my.fq\ncut\nSelect one or more columns from a file.\nSelect columns 1-4: cut -f 1-4 my.csv      -d \",\" comma as delimiter\nsort\nSort lines.\nThe -V option will successfully sort chr10 after chr2. etc.\nSort column 1 alphabetically, column 2 reverse numerically: sort -k1,1 -k2,2nr my.bed      -k 1,1 by column 1 only      -n numerical sorting      -r reverse order      -V recognize number with string\nuniq\nRemove consecutive duplicate lines (often from single-column selection): removes all duplicates if input is sorted.\nUnique values for column 2: cut -f2 my.tsv | sort | uniq\nuniq -c\nIf input is sorted, create a count table for occurrences of each line (often from single-column selection).\nCount table for column 3: cut -f3 my.tsv | sort | uniq -c\ntr\nSubstitute (translate) characters or character classes (like A-Z for uppercase letters). Does not take files as argument; piping or redirection needed.\nTo “squeeze” (-s) is to remove consecutive duplicates (akin to uniq).\nTSV to CSV: cat my.csv | tr \"\\t\" \",\" Uppercase to lowercase: tr A-Z a-z < in.txt > out.txt      -d delete      -s squeeze\ngrep\nSearch files for a pattern and print matching lines (or only the matching string with -o).\nDefault regex is basic (GNU BRE): use -E for extended regex (GNU ERE) and -P for Perl-like regex.\nTo print lines surrounding a match, use -A n (n lines after match) or -B n (n lines after match) or -C n (n lines before and after match).\nddddddddddddddddddddddddddddddddddddd\nMatch AAC or AGC: grep \"A[AG]C\" my.fa Omit comment lines: grep -v \"^# my.gff      -c count      -i ignore case      -r recursive      -v invert      -o print match only\n\nMiscellaneae\nSymbol\nMeaning\nexample\n/\nRoot directory.\ncd /\n.\nCurrent working directory.\ncp data/file.txt . (Copy to working dir) Use ./ to execute script if not in $PATH: ./myscript.sh\n..\nOne directory level up.\ncd ../.. (Move 2 levels up)\n~ or $HOME\nHome directory.\ncp myfile.txt ~ (Copy to home)\n$USER\nUser name.\nmkdir $USER\n>\nRedirect standard out to a file.\necho \"My 1st line\" > myfile.txt\n>>\nAppend standard out to a file.\necho \"My 2nd line\" >> myfile.txt\n2>\nRedirect standard error to a file.\nmyscript.sh >log.txt 2> err.txt\n&>\nRedirect standard out and standard error to a file.\nmyscript.sh &> log.txt\n|\nPipe standard out (output) of one command into standard in (input) of a second command\nsort myfile.txt | head\n{}\nBrace expansion. Use .. to indicate numeric or character ranges (1..4 => 1, 2, 3, 4) and , to separate items.\nmkdir Jan{01..31} (Jan01, Jan02, …, Jan31) touch fig1{A..F} (fig1A, fig1B, …, fig1F) mkdir fig1{A,D,H} (fig1A, fig1D, fig1D)\n$()\nCommand substitution. Allows for flexible usage of the output of any command: e.g., use command output in an echo statement or assign it to a variable.\nReport number of FASTQ files: echo \"I see $(ls *fastq | wc -l) files\" Substitute with date in YYYY-MM-DD format: mkdir results_$(date +%F) nlines=$(wc -l < $infile)\n$PATH\nContains colon-separated list of directories with executables: these will be searched when trying to execute a program by name. ddddddddddddddddddddddddddddddddddddd\nAdd dir to path: PATH=$PATH:/new/dir (But for lasting changes, edit the Bash configuration file ~./bashrc.) dddddddddddddddddddddddddddddddd\n\nShell wildcards\nWildcard\nMatches\n\n*\nAny number of any character, including nothing\nls data/*fastq.gz ls *R1*\n?\nAny single character\nls sample1_?.fastq.gz\n[] and [^]\nOne or none (^) of the “character set” within the brackets\nls fig1[A-G] ls fig[0-9] ls fig[^4]*\n\nRegular expressions\nNote: ERE = GNU “Extended Regular Expressions”. If “yes” in ERE column, symbol needs -E flag for grep and sed (note that awk uses ERE by default) to turn on ERE. (When using the default in grep and sed, Basic Regular Expressions (BRE), symbol would need to be preceded by a backslash to work.)\nSymbol\nERE\nMatches\nExample\n.\n\nAny single character\nCapture anything following “Olfr”: grep -o \"Olfr.*\"\n*\n\nQuantifier: matches preceding character any number of times\nSee previous example.\n+\nyes\nQuantifier: matches preceding character at least once\nAt least two consecutive digits: grep -E [0-9]+\n?\nyes\nQuantifier: matches preceding character at most once\nOnly a single digit: grep -E [0-9]?\n{m} / {m,} / {m,n}\nyes\nQuantifier: match preceding character m times / at least m times / m to n times\nBetween 50 and 100 consecutive Gs: grep -E \"G{50,100}\"\n^ / $\n\nAnchors: match beginning / end of line\nExclude empty lines: grep -v \"^$\" Exclude lines beginning with a “#”: grep -v \"^#\"\n\\t\n\nTab (To match in grep, needs -P flag for Perl-like regex)\necho -e \"column1 \\t column2\"\n\\n\n\nNewline (Not straightforward to match since Unix tools are line-based.)\necho -e \"Line1 \\n Line2\"\n\\w\n(yes)\n“Word” character: any alphanumeric character or “_”. Needs -E (ERE) in grep but not in sed (…).\ngrep -E -o 'gene_id \"\\w+\"' sed s/\\w/X/ (change any word character to X)\n|\nyes\nAlternation / logical or: match either the string before or after the |\nFind lines with either “intron” or “exon”: grep -E \"intron|exon\"\n()\nyes\nGrouping\nFind “AAG” repeated 10x: grep (AAG){10}\n\\1, \\2, etc.\nyes\nBackreferences to groups captured with (): first group is \\1, second group is \\2, etc. ddddddddddddddddddddddddddddddddddddd\nInvert order of two words: sed -E 's/(\\w+) (\\w+)/\\2 \\1/'\n\nDetails for selected commands\nless\nKey\nFunction\nq\nExit less\nspace / b\nGo down / up a page. (pgup / pgdn usually also work.)\nd / u\nGo down / up half a page.\ng / G\nGo to the first / last line (home / end also work).\n/<pattern> or ?<pattern>\nSearch for <pattern> forwards / backwards: type your search after / or ?.\nn / N\nWhen searching, go to next / previous search match.dddddddddddddddddddddddddddddddddddddddddddddddddddd\nsed\nsed flags:\nFlag\nMeaning\n-E\nUse extended regular expressions\n-e\nWhen using multiple expressions, precede each with -e\n-i\nEdit a file in place\n-n\nDon’t print lines unless specified with p modifier\nsed examples\n# Replace \"chrom\" by \"chr\" in every line,\n# with \"i\": case insensitive, and \"g\": global (>1 replacements per line)\nsed 's/chrom/chr/ig' chroms.txt\n\n# Only print lines matching \"abc\":\nsed -n '/abc/p' my.txt\n\n# Print lines 20-50:\nsed -n '20,50p'\n\n# Change the genomic coordinates format chr1:431-874 (\"chrom:start-end\")\n# ...to one that has a tab (\"\\t\") between each field:\necho \"chr1:431-874\" | sed -e 's/:/\\t/' -e 's/-/\\t/'\n#> chr1    431     874\n\n# Invert the order of two words:\necho \"inverted words\" | sed -E 's/(\\w+) (\\w+)/\\2 \\1/'\n#> words inverted\n\n# Capture transcript IDs from a GTF file (format 'transcript_id \"ID_I_WANT\"'):\n# (Needs \"-n\" and \"p\" so lines with no transcript_id are not printed.) \ngrep -v \"^#\" my.gtf | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p'\n\n# When a pattern contains a `/`, use a different expression delimiter:\necho \"data/fastq/sampleA.fastq\" | sed 's#data/fastq/##'\n#> sampleA.fastq\nawk\nRecords and fields: by default, each line is a record (assigned to $0). Each column is a field (assigned to $1, $2, etc).\nPatterns and actions: A pattern is a condition to be tested, and an action is something to do when the pattern evaluates to true.\nOmit the pattern: action applies to every record.\nawk '{ print $0 }' my.txt     # Print entire file\nawk '{ print $3,$2 }' my.txt  # Print columns 3 and 2 for each line\nOmit the action: print full records that match the pattern.\n# Print all lines for which:\nawk '$3 < 10' my.bed          # Column 3 is less than 10\nawk '$1 == \"chr1\"' my.bed     # Column 1 is \"chr1\"\nawk '/chr1/' my.bed           # Regex pattern \"chr1\" matches\nawk '$1 ~ /chr1/' my.bed      # Column 1 _matches_ \"chr1\"\n\nawk examples\n# Count columns in a GTF file after excluding the header\n# (lines starting with \"#\"):\nawk -F \"\\t\" '!/^#/ {print NF; exit}' my.gtf\n\n# Print all lines for which column 1 matches \"chr1\" and the difference\n# ...between columns 3 and 2 (feature length) is less than 10:\nawk '$1 ~ /chr1/ && $3 - $2 > 10' my.bed\n\n# Select lines with \"chr2\" or \"chr3\", print all columns and add a column \n# ...with the difference between column 3 and 2 (feature length):\nawk '$1 ~ /chr2|chr3/ { print $0 \"\\t\" $3 - $2 }' my.bed\n\n# Caclulate the mean value for a column:\nawk 'BEGIN{ sum = 0 };            \n     { sum += ($3 - $2) };             \n     END{ print \"mean: \" sum/NR };' my.bed\nawk comparison and logical operators\nComparison\nDescription\na == b\na is equal to b\na != b\na is not equal to b\na < b\na is less than b\na > b\na is greater than b\na <= b\na is less than or equal to b\na >= b\na is greater than or equal to b\na ~ /b/\na matches regular expression pattern b\na !~ /b/\na does not match regular expression pattern b\na && b\nlogical and: a and b\na || b\nlogical or: a or b [note typo in Buffalo]\n!a\nnot a (logical negation)\nawk special variables and keywords\nkeyword/variable\nmeaning\nBEGIN\nUsed as a pattern that matches the start of the file\nEND\nUsed as a pattern that matches the end of the file\nNR\nNumber of Records (running count; in END: total nr. of lines)\nNF\nNumber of Fields (for each record)\n$0\nContains entire record (usually a line)\n$1 - $n\nContains one column each\nFS\nInput Field Separator (default: any whitespace)\nOFS\nOutput Field Separator (default: single space)\nRS\nInput Record Separator (default: newline)\nORS\nOutput Record Separator (default: newline)\nawk functions\nFunction\nMeaning\nlength(<string>)\nReturn number of characters\ntolower(<string>)\nConvert to lowercase\ntoupper(<string>)\nConvert to uppercase\nsubstr(<string>, <start>, <end>)\nReturn substring\nsplit(<string>, <array>, <delimiter>)\nSplit into chunks in an array\nsub(<from>, <to>, <string>)\nSubstitute (replace) regex\ngsub(<from>, <to> <string>)\n>1 substitution per line\nprint\nPrint, e.g. column: print $1\nexit\nBreak out of record-processing loop;  e.g. to stop when match is found\nnext\nDon’t process later fields: to next iteration\n\n\n\nKeyboard shortcuts\nShortcut\nFunction\nTab\nTab completion\n⇧ / ⇩\nCycle through previously issued commands\nCtrl+Shift+C\nCopy selected text\nCtrl+Shift+V\nPaste text from clipboard\nCtrl+A / Ctrl+E\nGo to beginning/end of line\nCtrl+U / Ctrl+K\nCut from cursor to beginning / end of line1\nCtrl+W\nCut word before before cursor2\nCtrl+Y\nPaste (“yank”)\nAlt+.\nLast argument of previous command (very useful!)\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\nCtrl+C\nKill (stop) currently active command\nCtrl+D\nExit (a program or the shell depending on the context)\nCtrl+Z\nSuspend (pause) a process: then use bg to move to background.\nCtrl+K doesn’t work by default in VS Code, but can be set there.↩︎\nDoesn’t work by default in VS Code, but can be set there.\n\n↩︎\n",
      "last_modified": "2021-03-16T14:33:02-04:00"
    },
    {
      "path": "overviews_shellscript.html",
      "title": "Topic Overview: Shell scripting",
      "author": [],
      "contents": "\n\nContents\nBash script essentials\nInside the script\nExecuting scripts\n\nScripting-related code\nOverview\nfor loops\nif statements\nFile test operators\nComparison operators\n\n\n\n\n\nBash script essentials\nIn VS Code, don’t forget to install the shellcheck extension!\nInside the script\nCode\nExplanation\nExample\n#!/bin/bash\n“Shebang” line: points the computer to the Bash interpreter at /bin/bash.\n\nset -u -e -o pipefail\nBash strict settings: exit script with error if an unset variable is referenced (-u), a general error occurs (but with exceptions; -e), an error occurs in a shell pipeline (-o pipefail).\n\n$0\nName of the script (does not work for scripts submitted as SLURM jobs).\n\n$1, $2, etc\n“Positional parameters”: first, second, etc, arguments passed to the script from the shell.\n./script.sh my_arg1 my_arg2 $1 will be “my_arg1” and $2 will be “my_arg2”.\n$#\nNumber of arguments passed to the script.\n./script.sh my_arg1 my_arg2 $# will be 2.\n>&2\nRedirect standard out to standard error: e.g., “manually” designate an echo statement to represent standard error.\necho \"Error: Invalid line number\" >&2\nexit 1\nExit the script with exit code 1 (= failure). ddddddddddddddddddddddddddddddddddddd\necho \"Error: need at least 3 args\" && exit 1\nExecuting scripts\nCommand\nExplanation\nbash myscript.sh\nScripts that are not executable (execute permission not set) and/or have no shebang line can be run by explicitly calling bash.\n./myscript.sh scripts/myscript.sh\nScripts that are executable and have a shebang line can be called directly. But if they are in the current working dir, preface with ./ (otherwise the script will be looked for only in $PATH).\n./myscript.sh input.txt output.txt\nCall a script with two arguments, which will be available inside the script as $1 and $2.\n\nScripting-related code\nOverview\nCode\nExplanation\nExample\n=\nAssign a variable. No spaces around the = !\nnlines=200 nlines=$(wc -l my.csv)\n$\nRecall/reference a variable with $.\nPreferably quote variables too, especially in scripts, to prevent unwanted shell expansion in case of spaces and other special characters in variable values.\nOptionally, put variable names in curly braces {}.\necho $nlines (After assignment e.g. nlines=200) echo \"$nlines\" (Safer: quoted) echo \"${nlines}\" (Optionally: “embraced”)\n[ ]\nTest statement. Spaces required around the brackets (see examples)!\n[ 9 -gt 5 ] (Returns true: 9 is greater than 5) [ $var1 -lt $var2 ] (Returns true if $var1 is less than $var2) [ -d my_dir ] (Returns true if dir exists and is a dir)\n()\nUse to assign an array: a collection of items that can e.g. be looped over.\nsample_names=(zmaysA zmaysB zmaysC) sample_files=($(cut -f 3 samples.txt)) sample_files=($(cat fastq_files.txt))\n${array[@]}\nPrint all values in an array.\necho ${sample_names[@]}\n&&\nChain commands: execute second command only if the first succeeds.\ncd data && ls data git add --all && git commit -m \"Add README\" && git push\n||\nChain commands: execute second command only if the first fails.\ncd \"$outdir\" || echo \"Cannot change directory!\" [ -d \"$outdir\" ] || mkdir \"$outdir\"\nbasename\nStrip any directory names from a path, and optionally a suffix too.\nbasename data/A.fq (Returns A.fq) basename data/A.fq .fq (Returns A)\nexpr\nSimple arithmetic in the shell (but: no decimals, only integers!) ddddddddddddddddddddddddddddddddddddd\nnseqs=$(expr $nlines / 4) (Divide a value by 4)\nfor loops\nBasic example showing the syntax:\nfor i in 1 2 3; do\n    echo \"Now the variable 'i' is: $i\"\ndone\n#> Now the variable 'i' is: 1\n#> Now the variable 'i' is: 2\n#> Now the variable 'i' is: 3\nIn each iteration, one of the items provided after in will be assigned to the variable name provided after for, which can then be used inside the loop.\nPractical examples:\n# Loop over files using globbing - better than using `ls`:\nfor fastq_file in data/raw/*fastq.gz; do\n      echo \"File $fastq_file has $(wc -l < $fastq_file) lines.\"\n      # More processing...\ndone\n  \n# Loop to rename files:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" _001.fastq).fq\n    echo \"Old/new name: $oldname $newname\"\n    mv \"$oldname\" \"$newname\"\ndone\n\n# Loop using an array to submit a script for each sample:\nmy_samples=($(cut -f1 my_metadata.txt))\nfor my_sample in ${my_samples[@]}; do\n    my_script.sh $my_sample\ndone\nif statements\nBasic syntax:\nif <some_test>; then\n    # Commands to run if test evaluated to true\nfi\n  \nif <some_test>; then\n    # Commands to run if test evaluated to true\nelse\n    # Commands to run if test evaluated to false\nfi\nThe test is usually done with the [] syntax for a test, e.g. [ -d my_dir ] which will evaluate to true is my_dir is an existing directory.\nPractical examples:\n# Differential processing based on (e.g.) the number of samples:\nn_samples=$(wc -l < samples.txt)\nif [ \"$n_samples\" -gt 9 ]; then  # If the nr of samples is >9\n    echo \">9 samples: processing files with algorithm A...\"\nelse\n    echo \"<= 9 samples: processing files with algorithm B...\"\nfi\n\n# Test whether the correct number of arguments (here: 2)\n# were provided to the script:\nif [ ! \"$#\" -eq 2 ]; then\n      echo \"Error: wrong number of arguments\"\n      echo \"You provided $# arguments, while 2 are required.\"\n      echo \"Usage: line.sh <line-number> <file>\"\n      exit 1\nfi\n\n# Test whether the input file is a regular file (-f) and can be read (-r):\nif [ ! -f $file ] || [ ! -r $file ]; then\n    echo \"Error: can't open file\"\n    echo \"Second argument should be a readable file\"\n    echo \"You provided: $file\"\n    exit 1\nfi\n\n# Use a command's exit status - for grep, a match is success is true:\nif grep \"AGATCGG\" contimated.fasta > /dev/null; then\n    echo \"OH NO! File is contaminated!\"\n    exit 1\nfi\n\n# Remove all empty files from a directory:\nfor file in *; do\n    if [ ! -s \"$file\" ]; then\n        rm \"$file\"\n    fi\ndone\nFile test operators\nOperator\nReturns true if:\n-f\nFile is a regular file (e.g. not a directory)\n-d\nFile is a directory\n-e\nFile exists\n-s\nFile is not zero size\n-h\nFile is a symbolic link\n-r / -w / -x\nFile has read/write/execute permissions\nComparison operators\nString\nDescription\n-z str\nString str is null (empty)\nstr1 = str2\nStrings str1 and str2 are identical\nstr1 != str2        \nStrings str1 and str2 are different                \nInteger\nDescription\nint1 -eq int2\nIntegers int1 and int2 are equal\nint1 -ne int2\nIntegers int1 and int2 are not equal\nint1 -lt int2\nInteger int1 is less than int2\nint1 -gt int2\nInteger int1 is greater than int2\nint1 -le int2\nInteger int1 is less than or equal to int2\nint1 -ge int2      \nInteger int1 is greater than or equal to int2\n\n\n\n",
      "last_modified": "2021-03-16T14:33:02-04:00"
    },
    {
      "path": "overviews_slurm.html",
      "title": "Topic Overview: OSC -- SLURM jobs and software",
      "author": [],
      "contents": "\n\nContents\nUsing SLURM\nSubmitting batch (non-interactive) jobs\nSubmitting interactive jobs\nMonitoring and managing SLURM jobs\n\nUsing software at OSC\nPre-installed software using modules\nConda\n\n\nUsing SLURM\nSubmitting batch (non-interactive) jobs\nSLURM directives can be provided as arguments to sbatch when submitting the job and/or inside the script on lines starting with #SBATCH. If the same directive is provided in both places, the command-line (sbatch call) value will override the one in the script.\nMost important sbatch options\nFor more options, see the SLURM documentation.\nResource/use\nshort\nlong\ndefault\nexample / remarks\nProject to be billed\n-A\n--account\nN/A\n--account=PAS0471 -A PAS1855\nTime limit\n-t\n--time\n1 hour\n-t 60 (60 min) -t 2:30 (2 min and 30 sec) -t 5:00:00 (5 h) -t 2-12 (2 days and 12 h) --time=60 (60 min)\nNumber of nodes\n-N\n--nodes\n1\n--nodes=2 Only ask >1 node if you have explicit parallelization with e.g. MPI (uncommon in bioinformatics).\nNumber of cores\n-c\n--cpus-per-task\n1\n--cpus-per-task=4 For jobs with multi-threading (common).\nNumber of “tasks” (processes)\n-n\n--ntasks\n1\n--ntasks=2For jobs with multiple processes (not as common).\nNumber of tasks per node\n-\n--ntasks-per-node\n1\n--ntasks-per-node=2For jobs with multiple processes (not as common).\nMemory limit per node\n-\n--mem\n(4G)\n--mem=40G The default unit is MB (MegaBytes) – use “G” for GB.\nLog output file\n-o\n--output\nslurm-%j.out\n--output=slurm-fastqc-%j.out (Useful to include a descriptive name but be sure to also include %j, the job number.)\nError output file\n-e\n--error\nN/A\n--error=slurm-fastqc-%j.err (Note: by default, stderr is included with stdout in -o / --output; use -e/--error to separate.)\nJob name\n-\n--job-name\nN/A\n--job-name=fastqc (Useful to distinguish jobs in the queue.)\nPartition (queue type)\n-\n--partition\nany\n--partition=longserial  See these OSC docs for more info.\nGet email when job starts/ends/fails\n-\n--mail-type\nN/A\n--mail-type=START (When job starts) --mail-type=END (When job ends) --mail-type=FAIL (When job fails) --mail-type=ALL (Any event) \nJob can’t start until specified time\n-\n--begin\nN/A\n--begin=2021-02-01T12:00:00\nJob can’t start until dependency job has finished ddddddddddddddddddddd\n-\n--dependency\nN/A\n--dependency=afterany:123456 dddddddddddddddddddddddddddddddddd\nSubmitting the job with sbatch\nTo submit a script to the queue with sbatch, simply prepend sbatch, optionally with sbatch options, before the call to the script:\nsbatch [sbatch-options] <script> [script-arguments]\nSome examples with and without sbatch options and script arguments:\n# No sbatch options (must be provided in script) and no script arguments:\nsbatch myscript.sh\n\n# No sbatch options, one script argument:\nsbatch myscript.sh sampleA.fastq.gz\n\n# Two sbatch options, no script arguments:\nsbatch -t 60 -A PAS1855 --mem=20G myscript.sh\n\n# Two sbatch options, one script argument:\nsbatch -t 60 -A PAS1855 --mem=20G myscript.sh sampleA.fastq.gz\nExample script header with SLURM directives\nNote: Because SLURM directives are a special type of comments, they need to occur before any lines that are executed in order to be parsed. For instance, they should be placed above the set header below:\n#!/bin/bash\n\n#SBATCH --account=PAS1855\n#SBATCH --time=00:45:00\n#SBATCH --mem=8G\n\nset -e -u -o pipefail\nSLURM environment variables\nInside the script, SLURM environment variables will be available, such as:\nVariable\nCorresponding option\nDescription\n$SLURM_SUBMIT_DIR\nN/A\nPath to dir from which job was submitted.\n$TMPDIR\nN/A\nPath to a dir available during the job (fast I/O).\n$SLURM_JOB_ID\nN/A\nJob ID assigned by SLURM.\n$SLURM_JOB_NAME\n--job-name\nJob name supplied by the user.\n$SLURM_CPUS_ON_NODE\n-c / --cpus-per-task\nNumber of CPUs (~ cores/threads) available on 1 node.\n$SLURM_MEM_PER_NODE\n--mem\nMemory per node,\n$SLURMD_NODENAME\nN/A\nName of the node running the job.\nAs an example of how using these environment variables can be convenient, below we use $SLURM_CPUS_ON_NODE (when calling the program STAR inside our script. This way, we only have to modify the number of threads in one place, and we don’t risk having a mismatch between resources requested and resources (attempting to be) used.\nSTAR --runThreadN \"$SLURM_CPUS_ON_NODE\" --genomeDir ...\nSubmitting interactive jobs\nCommand\nExplanation\nExample\nsinteractive\nOSC convenience wrapper around srun / salloc. Only accepts short options (e.g. -A, not --account). Default time is 30 minutes and maximum time is 60 minutes.\nsinteractive -A PAS1855 -t 60\nsrun\nStart an interactive job with any set of options; needs --pty /bin/bash to enter a Bash shell on the reserved node. ddddddddddddddddddddddddddddddddddddd\nsrun -A PAS1855 -t 60 --pty /bin/bash\nMonitoring and managing SLURM jobs\nCommand\nExplanation\nExample\nsqueue\nCheck the SLURM job queue: will only see queued and running jobs, no finished jobs. (Use -u or you will see everyone’s jobs!)\nsqueue -u $USER (show all my jobs) squeue -u $USER -l (long format, more info)\nscancel\nCancel one or more jobs.\nscancel 2526085 (Cancel job 2526085) scancel -u $USER (Cancel all my jobs)\nscontrol\nInformation about any job, mostly a summary of the resources available to the job and other options that were implicitly or explicitly set.\nscontrol show job 2526085 (Show stats for job 2526085) scontrol show job $SLURM_JOB_ID (for usage INSIDE a script)\nsstat\nInformation about running jobs, including memory usage  (=> can be included in script).\nsstat -j $SLURM_JOB_ID --format=jobid,avecpu,averss,maxrss,ntasks\nsacct\nInformation about finished jobs, including memory usage. ddddddddddddddddddddddddddddddddddddd\nsacct -j 2978487\nNotes:\nIf you need to check CPU and memory usage of your jobs, see also the XDMoD tool on OSC Ondemand.\nIn sstat and sacct, MaxRSS is the maximum amount of memory used by the job.\n\nUsing software at OSC\nPre-installed software using modules\nCommand\nExplanation\nExample\nmodule spider\nSee all modules (software available through the module system).\nmodule spider (See all modules) module spider python (All modules matching “Python”)\nmodule avail\nSee modules that can be loaded given the current software environment.\nmodule avail (See all modules) module avail python (All modules matching “Python”)\nmodule load\nLoad a specific module. After loading the module, the software will be available in your $PATH and can thus be called directly.\nmodule load python (Load default version) module load python/3.7-2019.10 (Load a specific version)\nmodule list\nList currently loaded modules\nmodule list\nmodule unload\nUnload a currently loaded module. ddddddddddddddddddddddddddddddddddddd\nmodule unload python dddddddddddddddddddddddddddddddddddddddddd\nConda\nTo use Conda at OSC, first load the module python/3.6-conda5.2 using module load python/3.6-conda5.2.\nConda commands and options:\nCommand\nExplanation\nExample\ncreate\nCreate a new Conda environment.\nconda create -n my-env conda create cutadapt-env cutadapt\nenv create\nCreate a new Conda environment using a YAML file describing the environment (see conda export below).\nconda env create --file environment.yml\n-y\nDon’t prompt for confirmation when installing/removing things.\nconda install -y cutadapt\nactivate\nActivate a specific Conda environment, so you can use the software installed in that environment.\nsource activate cutadapt-env source activate cutadapt-env Activate -“stack”- a second environment: conda activate --stack <second-env-name>\ninstall\nInstall software into the currently active environment.\nconda install python=3.7 Specify channel for installation: conda install -c bioconda cutadapt\nconfig\nConfigure Conda (see below).\nconda config --add channels bioconda\nexport\nExport a YAML file that describes the environment\nExport the active environment: conda env export > environment.yml Export any environment by name: conda env export -n multiqc-env > multiqc-env.yml\nenv list\nList all your environments.\nconda env list\nlist\nList all packages (software) in an environment.\nconda list -n multiqc-env\ndeactivate\nDeactivate the currently active environment.\nconda deactivate\nremove\nRemove an environment entirely.\nconda env remove -n multiqc-env\nsearch\nSearch for a software package. ddddddddddddddddddddddddddddddddddddd\nconda search 'bwa*'\nConda channels\nConda “channels” are like repositories, each of which carry overlapping sets of software. A one-time setup step to set the channel priorities in the order that is generally desired – run these lines in the following order:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Highest priority\nChannels can also be specified for individual installations, in order to override these defaults:\nconda install -c bioconda cutadapt\nConda setup to enable conda activate\nTo enable conda activate to work (in addition to source activate), add the following lines to your Bash configuration file at ~/.bashrc (which you can open with VS Code or Nano and edit):\nif [ -f /apps/python/3.6-conda5.2/etc/profile.d/conda.sh ]; then\n      source /apps/python/3.6-conda5.2/etc/profile.d/conda.sh\nelif [ -f /usr/local/python/3.6-conda5.2/etc/profile.d/conda.sh ]; then\n      source /usr/local/python/3.6-conda5.2/etc/profile.d/conda.sh\nfi\nFor immediate effects, you’ll need to run the ~/.bashrc file:\nsource ~/.bashrc\n\n\n\n",
      "last_modified": "2021-03-16T14:33:03-04:00"
    },
    {
      "path": "w01_exercises.html",
      "title": "Week 1 Exercises",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nGetting set up\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\nSolutions\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\n\nThe following exercises were copied from Chapter 1 of the CSB book, with hints and solutions modified from those provided in the book’s Github repo.\n(The exercises marked as “Advanced” are omitted since they require topics not actually covered in the chapter, which we will cover in later modules.)\n\nMain exercises\nGetting set up\nYou should already have the book’s Github repository with exercise files.\nIf not, go to /fs/ess/PAS1855/users/$USER, and run git clone https://github.com/CSB-book/CSB.git.\nNow, you should have the CSB directory referred to in the first step of the first exercise.\nIntermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\nShow hints\nRecall that .. is one level up in the dir tree, and that you can combine multiple .. in a single statement.\n\n(d) Use an absolute path to go to the sandbox directory within python.\n(e) Return to the data directory within the python directory.\nShow hints\nRecall that there is a shortcut to return to your most recent working dir.\n\nIntermezzo 1.2\nTo familiarize yourself with these basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\nShow hints\nThe touch command creates an empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt.\nIntermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? Which is the last?\nShow hints\nYou can either first select the 5th column using cut, and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter.\nTo view just the first or the last line, pipe to head or tail.\n\n(b) How many families are represented in the database?\nShow hints\nStart by selecting the relevant column.\nUse a tail trick shown in the chapter to exclude the first line.\nRemember to sort before using uniq.\n1.10.1 Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (>).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\nShow hints\nTo show the size of a file you can use the -l option of the command ls, and to display human-readable file sizes, the -h option.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\nShow hints\nRecall that with the cp command, it is also possible to give the copy a new name at once.\n\n4. How many contigs are classified as isogroup00036?\nShow hints\nIs there a grep option that counts the number of occurrences?\nAlternatively, you can pass the output of grep to wc -l.\n\n5. Replace the original “two-spaces” delimiter with a comma.\nShow hints\nIn the file, the information on each contig is separated by two spaces:\n>contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n>contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\nIn Linux, we can’t write output to a file that also serves as input (see here), so the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" > myfile.txt # Don't do this! \nIn this case, we will have to save the output in a temporary file and on a separate line, overwrite the original file using mv.\n\n6. How many unique isogroups are in the file?\nShow hints\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nShow hints\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically.\n\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nHow many times were the levels of individuals 3 and 27 recorded?\nShow hints\nYou can use cut to extract just the maleID from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”.\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nShow hints\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written.\nIn the script, you can use the first two so-called positional parameters, $1 and $2, to represent the file name and the maleID, respectively.\n$1 and $2 represent the first and the second argument passed to a script like so: bash myscript.sh arg1 arg2.\n1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\n$ bash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\nShow hints\nTo build the script, you need to combine several commands:\nTo find the number of rows, you can use wc.\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n, and count characters.\n1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nFor example, running the script with:\n$ bash explore.sh ../data/Buzzard2015_data.csv 7\nshould return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\nShow hints\nYou can select a given column from a csv file using the command cut. Then:\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the positional parameters $1 and $2 for the file name and column number, respectively.\n\n\nSolutions\nIntermezzo 1.1\nSolution\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER.\n$ cd /fs/ess/PAS1855/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS1855/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\ncd -\n\nIntermezzo 1.2\nSolution\n(a) Go to the data directory within CSB/unix.\n$ cd /fs/ess/PAS1855/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory         without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\nIntermezzo 1.3\nSolution\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? And which is the last?\ncd ∼/CSB/unix/data/\n\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\n\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\n\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t\";\" -k 5 Pacifici2013_data.csv | head -n 1\n\n\nFollowing the output that you wanted, you may have gotten errors like this:\nsort: write failed: 'standard output': Broken pipe\nsort: write error\nThis may seem disconcerting, but is nothing to worry about, and has to do with the way data streams through a pipe: after head/tail exits because it has done what was asked (print one line), sort or cut may still try to pass on data.\n\n\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n\n1.10.1 Next-Generation Sequencing Data\n1. Change directory to CSB/unix/sandbox.\ncd /fs/ess/PAS1855/users/$USER/CSB/unix/sandbox\n\n2. What is the size of the file Marra2014_data.fasta?\nls -lh ../data/Marra2014_data.fasta\n# Among other output, this should show a file size of 553K\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\ncp ../data/Marra2014_data.fasta my_file.fasta\nTo make sure the copy went well, list the files in the sandbox:\nls\n\n4. How many contigs are classified as isogroup00036?\nTo count the occurrences of a given string, use grep with the option -c\ngrep -c isogroup00036 my_file.fasta \n# 16\nYou can also use a pipe and wc -l to count:\ngrep isogroup00036 my_file.fasta | wc -l\n\n5. Replace the original “two-spaces” delimiter with a comma.\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,:\ncat my_file.fasta | tr -s ' ' ',' > my_file.tmp\nmv my_file.tmp my_file.fasta\n\n6. How many unique isogroups are in the file?\nFirst, searching for > with grep will extract all lines with contig information:\ngrep '>' my_file.fasta | head -n 2\n# >contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n# >contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, use cut to extract the 4th column\ngrep '>' my_file.fasta | cut -d ',' -f 4 | head -n 2\n#gene=isogroup00001\n#gene=isogroup00001\nFinally, use sort -> uniq -> wc -l to count the number of unique occurrences:\ngrep '>' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n# 43\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nWe need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n# >contig00001,numreads=2\n# >contig00002,numreads=8\n# >contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t ‘=’ to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n# >contig00089,numreads=1\n# >contig00176,numreads=1\n# >contig00210,numreads=1\n# >contig00001,numreads=2\n# >contig00003,numreads=2\nUsing the flag -r we can sort in reverse order:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n# >contig00302,numreads=3330\nFinding that contig 00302 has the highest coverage, with 3330 reads.\n\n\n1.10.2 Hormone Levels in Baboons\n1. How many times were the levels of individuals 3 and 27 recorded?\nFirst, let’s see the structure of the file:\nhead -n 3 ../data/Gesquiere2011_data.csv \n# maleID        GC      T\n# 1     66.9    64.57\n# 1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 ../data/Gesquiere2011_data.csv | head -n 3\n# maleID\n# 1\n# 1\nThen we can pipe the results to grep -c to count the number of occurrences (note the flag -w as we want to match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 3\n# 61\n\n# For maleID 27\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 27\n# 5\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nIn the script, we just need to incorporate the arguments given when calling the script, using $1 for the file name and $2 for the individual ID, into the commands that we used above:\ncut -f 1 $1 | grep -c -w $2\nA slightly more verbose and readable example:\n#!/bin/bash\n\nfilename=$1\nind_ID=$2\necho \"Counting the nr of occurrences of individual ${ind_ID} in file ${filename}:\" \ncut -f 1 ${filename} | grep -c -w ${ind_ID}\n\n\nVariables are assigned using name=value (no dollar sign!), and recalled using $name or ${name}. It is good practice to put curly braces around the variable name. We will talk more about bash variables in the next few weeks.\n\n\nTo run the script, assuming it is named count_baboons.sh:\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n1.10.3 Plant–Pollinator Networks\nSolution\nCounting rows:\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\nTo avoid printing the file name, we can either use cat or input redirection:\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l < ../data/Saavedra2013/n10.txt \nCounting rows:\nCounting the number of columns is more work. First, we need only the first line:\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\nNow we can remove all spaces and the line terminator using tr:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\nFinally, we can use wc -c to count the number of characters in the string:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nFinal script:\n#!/bin/bash\n\nfilename=$1\n\necho \"Filename:\"\necho ${filename}\necho \"Number of rows:\"\ncat ${filename} | wc -l\necho \"Number of columns:\"\nhead -n 1 ${filename} | tr -d ' ' | tr -d '\\n' | wc -c\nTo run the script, assuming it is named counter.sh:\nbash counter.sh ../data/Saavedra2013/n10.txt\n# 5\n\n\nWe’ll learn about a quicker and more general way to count columns in a few weeks.\n\n\n\n1.10.4 Data Explorer\nSolution\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\nHere is an example of what the script could look like:\n#!/bin/bash\n\nfilename=$1\ncolumn_nr=$2\n\necho \"Column name\"\ncut -d ',' -f ${column_nr} ${filename} | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n",
      "last_modified": "2021-03-16T14:33:04-04:00"
    },
    {
      "path": "w02_exercises.html",
      "title": "Exercises: Week 2",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nExercise 1: Course notes in Markdown\nExercise 2\n\nBonus exercises\nExercise 3\nBuffalo Chapter 3 code-along\n\nSolutions\nExercise 2\n\n\nMain exercises\nExercise 1: Course notes in Markdown\nCreate a Markdown document with course notes. I recommend writing this document in VS Code.\nMake notes of this week’s material in some detail. If you have notes from last week in another format, include those too. (And try to keep using this document throughout the course!)\nSome pointers:\nUse several header levels and use them consistently: e.g. a level 1 header (#) for the document’s title, level 2 headers (##) for each week, and so on.\nThough this should foremost be a functional document for notes, try to incorporate any appropriate formatting option: e.g. bold text, italic text, inline code, code blocks, ordered/unordered lists, hyperlinks, and perhaps a figure.\nMake sure you understand and try out how Markdown deals with whitespace, e.g. starting a new paragraph and how to force a newline.\nExercise 2\nWhile doing this exercise, save the commands you use in a text document – either write in a text document in VS Code and send the commands to the terminal, or copy them into a text document later.\nGetting set up\nCreate a directory for this exercise, and change your working dir to go there. You can do this either in your $HOME dir (e.g. ~/pracs-sp21/w02/ex2/) or your personal dir in the course’s project dir (/fs/ess/PAS1855/users/$USER/w02/ex2/).\nCreate a disorganized mock project\nUsing the touch command and brace expansions, create a mock project by creating 100s of empty files, either in a single directory or a disorganized directory structure.\nIf you want, you can create file types according to what you typically have in your project – otherwise, a suggestion is to create files with:\nRaw data (e.g. .fastq.gz)\nReference data (e.g. .fasta),\nMetadata (e.g. .txt or .csv)\nProcessed data and results (e.g. .bam, .out)\nScripts (e.g. .sh, .py or .R)\nFigures (e.g. .png or .eps)\nNotes (.txt and/or .md)\nPerhaps some other file type you usually have in your projects.\n\nOrganize the mock project\nOrganize the mock project according to some of the principles we discussed this week.\nEven while adhering to these principles, there is plenty of wiggle room and no single perfect dir structure: what is optimal will depend on what works for you and on the project size and structure. Therefore, think about what makes sense to you, and what makes sense given the files you find yourself with.\nTry to use as few commands as possible to move the files – use wildcards!\nChange file permissions\nMake sure no-one has write permissions for the raw data files. You can also change other permissions to what you think is reasonable or necessary precaution for your fictional project.\nHints\nUse the chmod command to change file permissions and recall that you can use wildcard expansion to operate on many files at once.\nSee the slides starting from here for an overview of file permissions and the chmod command.\nAlternatively, chmod also has an -R argument to act recursively: that is, to act on dirs and all of their contents (including other dirs and their contents).\n\nCreate mock alignment files\nCreate a directory alignment inside an appropriate dir in your project (e.g. analysis, results, or a dir for processed data), and create files for all combinations of 30 samples (01-30), 5 treatments (A-E), and 2 dates (08-14-2020 and 09-16-2020), like so: sample01_A_08-14-2020.sam - sample50_H_09-16-2020.sam.\nThese 300 files can be created with a single touch command. (If you already had an alignment dir, first delete its contents or rename it.)\nHints\nUse brace expansion three times: to expand sample IDs, treatments, and dates.\nNote that {01..20} will successfully zero-pad single-digit numbers.\n\nRename files in a batch\nWoops! We stored the alignment files that we created in the previous step as SAM files (.sam), but this was a mistake – the files are actually the binary counterparts of SAM files: BAM files (.bam).\nMove into the dir with your misnamed BAM files, and use a for loop to rename them: change the extension from .sam to .bam.\nHints\nLoop over the files using globbing (wildcard expansion) directly; there is no need to call ls.\nUse the basename command, or alternatively, cut, to strip the extension.\nStore the output of the basename (or cut) call using command substitution ($(command) syntax).\nThe new extension can simply be pasted behind the file name, like newname=\"$filename_no_extension\"bam or newname=$(basename ...)bam.\nCopy files with wildcards\nStill in the dir with your SAM files, create a new dir called subset. Then, using a single cp command, copy files that satisfy the following conditions into the subset dir:\nThe sample ID/number should be 01-19;\nThe treatment should be A, B, or C.\nCreate a README.md in the dir that explains what you did.\nHints\nJust like you used multiple consecutive brace expansions above, you can use two consecutive wildcard character sets ([]) here.\n\nBonus: a trickier wildcard selection\nStill in the dir with your SAM files, create a new dir subset2. Then, copy all files except the one for “sample28” into this dir.\nDo so using a single cp command, though you’ll need two separate wildcard expansion or brace expansion arguments (as in cp wildcard-selection1 wildcard-selection2 destination/).\nHints\nWhen using brace expansion ({}), simply use two numeric ranges: one for IDs smaller than and one for IDs larger than the focal ID.\nWhen trying to do this with wildcard character sets ([]), you’ll run into one of its limits: you can’t combine conditions with a logical or. Therefore, to exclude only sample 28, you have to separately select IDs that (1) do not start with a 2 and (2) start with a 2 but do not end with an 8.\nBonus: a trickier renaming loop\nYou now realize that your date format is suboptimal (MM-DD-YYYY; which gave 08-14-2020 and 09-16-2020) and that you should use the YYYY-MM-DD format. Use a for loop to rename the files.\nHints\nUse cut to extract the three elements of the date (day, month, and year) on three separate lines.\nStore the output of these lines in variables using commands substitution, like: day=$(commands).\nFinally, paste your new file name together like: newname=\"$part1\"_\"$year\" etc.\nWhen first writing your commands, it’s helpful to be able to experiment easily: start by echo-ing a single example file name, as in: echo sample23_C_09-16-2020.sam | cut ....\nCreate a README\nInclude a README.md that described what you did; again, try to get familiar with Markdown syntax by using formatting liberally.\nBonus exercises\nExercise 3\nIf you feel like it would be good to reorganize one of your own, real projects, you can do so using what you’ve learned this week. Make sure you create a backup copy of the entire project first!\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3.\n\nSolutions\nExercise 2\n1. Getting set up\nmkdir ~/pracs-sp21/w02/ex2/ # or similar, whatever dir you chose\ncd !$                       # !$ is a shortcut to recall the last argument from the last commands\n\n2. Create a disorganized mock project\nAn example:\ntouch sample{001..150}_{F,R}.fastq.gz\ntouch ref.fasta ref.fai\ntouch sample_info.csv sequence_barcodes.txt\ntouch sample{001..150}{.bam,.bam.bai,_fastqc.zip,_fastqc.html} gene-counts.tsv DE-results.txt GO-out.txt\ntouch fastqc.sh multiqc.sh align.sh sort_bam.sh count1.py count2.py DE.R GO.R KEGG.R\ntouch Fig{01..05}.png all_qc_plots.eps weird-sample.png\ntouch dontforget.txt README.md README_DE.md tmp5.txt\ntouch slurm-84789570.out slurm-84789571.out slurm-84789572.out\n\n3. Organize the mock project\nAn example:\nCreate directories:\nmkdir -p data/{raw,meta,ref}\nmkdir -p results/{alignment,counts,DE,enrichment,logfiles,qc/figures}\nmkdir -p scripts\nmkdir -p figures/{ms,sandbox}\nmkdir -p doc/misc\nMove files:\nmv *fastq.gz data/raw/\nmv ref.fa* data/ref/\nmv sample_info.csv sequence_barcodes.txt data/meta/\nmv *.bam *.bam.bai results/alignment/\nmv *fastqc* results/qc/\nmv gene-counts.tsv results/counts/\nmv DE-results.txt results/DE/\nmv GO-out.txt results/enrichment/\nmv *.sh *.R *.py scripts/\nmv README_DE.md results/DE/\nmv Fig[0-9][0-9]* figures/ms\nmv weird-sample.png figures/sandbox\nmv all_qc_plots.eps results/qc/figures/\nmv dontforget.txt tmp5.txt doc/misc/\nmv slurm* results/logfiles/\n\n4. Change file permissions\nTo ensure that no-one has write permission for the raw data, you could, for example, use:\nchmod a=r data/raw/*   # set permissions for \"a\" (all) to \"r\" (read)\n\nchmod a-w data/raw/*   # take away \"w\" (write) permissions for \"a\" (all)\n\n5. Create mock alignment files\n$ mkdir -p results/alignment\n$ # rm results/alignment/* # In the example above, we already had such a dir with files\n$ cd results/alignment \n\n# Create the files:\n$ touch sample{01..30}_{A..E}_{08-14-2020,09-16-2020}.sam\n\n# Check if we have 300 files:\n$ ls | wc -l\n# 300\n\n6. Rename files in a batch\n# cd results/alignment  # If you weren't already there\n\n# Use *globbing* to match the files to loop over (rather than `ls`):\nfor oldname in *.sam\ndo\n   # Remove the `sam` suffix using `basename $oldname sam`,\n   # use command substitution (`$()` syntax) to catch the output of the\n   # `basename` command, and paste `bam` at the end:\n   newname=$(basename $oldname sam)bam\n   \n   # Report what we have:\n   # (Using `-e` with echo we can print an extra newline with '\\n`,\n   # to separate files by an empty line)\n   echo \"Old name: $oldname\"\n   echo -e \"New name: $newname \\n\"\n   \n   # Execute the move:\n   mv \"$oldname\" \"$newname\"\ndone\nA couple of take-aways:\nNote that we don’t need a special construction to paste strings together. we simply type bam after what will be the extension-less file name from the basename command.\nWe print the old and new names to screen; this is not necessary, of course, but good practice. Moreover, this way we can test whether our loop works before adding the mv command.\nWe use informative variable names (oldname and newname), not cryptic ones like i and o.\n7. Copy files with wildcards\nCreate the new dir:\nmkdir subset\nCopy the files using four consecutive wildcard selections:\nThe first digit should be a 0 or a 1 [0-1],\nThe second can be any number [0-9] (? would work, too),\nThe third, after an underscore, should be A, B, or C [A-C],\nWe don’t care about what comes after that, but do need to account for the additional characters, so will use a * to match any character:\n\ncp sample[0-1][0-9]_[A-C]* subset/\nReport what we did, including a command substitution to insert the current date:\necho \"On $(date), created a dir \"subset\" and copied only files for samples 1-29 \\\nand treatments A-D into this dir\" > subset/README.md\n\n# See if it worked:\ncat subset/README.md\n\n8. Bonus: a trickier wildcard selection\nCreate the new dir:\nmkdir subset2\nThe most straightforward way in this case is using two brace expansion selections, one for sample numbers smaller than 28, and one for sample numbers larger than 28:\ncp sample{01..27}* sample{29..30}* subset2/\nHowever, we may not always be able to use ranges like that, and being a little creative with wildcard expansion also works — first, we select all samples not starting with a 2, and then among samples that do start with a 2, we exclude 28:\ncp sample[^2]* sample2[^8]* subset2/\n\n9. Bonus: a trickier renaming loop\nfor oldname in *.bam\ndo\n   # Use `cut` to extract month, day, year, and a \"prefix\" that contains\n   # the sample number and the treatment, and save these using command substitution:\n   month=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 1)\n   day=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 2)\n   year=$(basename \"$oldname\" .bam | cut -d \"_\" -f 3 | cut -d \"-\" -f 3)\n   prefix=$(echo \"$oldname\" | cut -d \"_\" -f 1-2)\n   \n   # Paste together the new name:\n   # (This will fail without quotes around prefix, because the underscore\n   # is then interpreted as being part of the variable name.)\n   newname=\"$prefix\"_\"$year\"-\"$month\"-\"$day\".bam\n   \n   # Report what we have:\n   echo \"Old name: $oldname\"\n   echo -e \"New name: $newname \\n\"\n   \n   # Execute the move:\n   mv \"$oldname\" \"$newname\"\ndone\n\n\nThis renaming task can be done more succinctly using regular expressions and the sed command – we’ll learn about both of these topics later in the course.\n\n\n\n\n\n\n",
      "last_modified": "2021-03-16T14:33:04-04:00"
    },
    {
      "path": "w02_UA_github-signup.html",
      "author": [],
      "contents": "\nWhat?\nCreate a Github account and let me know you’ve done so.\nWhy?\nGithub is a website that hosts git repositories, i.e. version-controlled projects. In Module 3 of this course, we will be learning how to use git together with Github. Also, all graded assignments for this course will be submitted through Github. I will need to know your Github user names in advance to set up the infrastructure for this.\nHow?\nIf you already have a Github account, log in and start at step 6.\nGo to https://github.com. Click “Sign Up” in the top right. Fill out the form: When choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use Github to share your code, for instance when publishing a paper.) You can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.) You probably want to uncheck the box under Email Preferences. When you’re done, click “Create account”. You can answer the questions Github will now ask you, but you should also be able to just skip them. Check your email and click the link to verify your email address. In the far top-right of the page back on Github, click your randomly assigned avatar, and in the dropdown menu, click “Settings”. In the Emails tab (left-hand menu), deselect the box “Keep my email addresses private”. In the Profile tab, enter your Name. Still in the Profile tab, upload an avatar. This can be a picture of yourself but if you prefer, you can use something else. Here at this CarmenCanvas assignment, submit the link to your Github profile, which will be “https://github.com/”.\n\n\n\n",
      "last_modified": "2021-03-16T14:33:05-04:00"
    },
    {
      "path": "w03_GA_git.html",
      "title": "Graded Assignment I: Shell, Markdown, and Git",
      "author": [],
      "contents": "\n\nContents\nIntroduction\nPart I – Create a new Git repo\nPart II – Add some Markdown content\nPart III – Add some “data” files\nPart IV – Renaming files on a new branch\nPart V – Create and sync an online version of the repo\n\n\nIntroduction\nIn this assignment, you will primarily be practicing your Git skills. You will also show that you can write in Markdown and can write a fo loop to rename files.\nThe instructions below will take you through it step-by-step. All of the the steps involve things that we have gone through in class and/or that you have practiced with in last week’s exercises.\nGrading information:\nThe number of points (if any) that you can earn for each step are denoted in cursive between square brackets (e.g. [0.5]). In total, you can earn 10 points with this assignment, which is 10% of your grade for the class.\nIf you make a mistake like an unnecessary commit, or something that requires you to subsequently make an extra commit: this is no problem. Points will not be subtracted for extra commits as long as you indicate what each commit is for & I’m still able to figure out which commits are the ones that I requested.\nPoints are not subtracted for minor things like typos in commit messages, either.\nBasically, you should not feel like you have to restart the assignment unless things have become a total mess…\nSome general hints:\nDon’t forget to constantly check the status of your repository (repo) with git status: before and after nearly all other Git commands that you issue. This will help prevent mistakes, and will also help you understand what’s going on as you get up and running with Git.\nIt’s also a good idea to regularly check the log with git log; recall that git log --oneline will provide a quick overview with one line per commit.\n\nPart I – Create a new Git repo\nCreate a new directory at OSC.\nA good place for this directory is in /fs/ess/PAS1855/users/$USER/week03/, but you are free to create it elsewhere (I will only be checking the online version of your repo).\nI suggest the name pracs-sp21-GA1 for the dir, (“GA1” for “Graded Assignment 1”), but you are free to pick a name that makes sense to you.\nLoad the OSC git module. Don’t forget to do this, or you will be working with a much older version of Git.\nInitialize a local Git repository inside your new directory. [0.5]\nCreate a README file in Markdown format. [0.5]\nThe file should be named README.md, and for now, just contain a header saying that this is a repository for your assignment.\nStage and then commit the README file. [0.5]\nInclude an appropriate commit message.\nPart II – Add some Markdown content\nCreate a second Markdown file with some more contents. [1.5]\nThis file can have any name you want, and you can also choose what you want to write about.\nUse of a good variety of Markdown syntax, as discussed and practiced in week 2: headers, lists, hyperlinks, and so on. (You’ll have to include some inline code and a code block later on, so you may or may not choose to use those now.) Also, make sure to read the next step before you finish writing.\nAs long as you are not writing minimalistic dummy text without any meaning (like “Item 1, Item 2”), you will not be graded for what you are writing about, so feel free to pick something you like – and don’t worry about the details.\n(If you’re not feeling inspired, here are some suggestions: lecture and reading notes for this week; a table of Unix and/or Git commands that we’ve covered; things that you so far find challenging or interesting about Git; how computational skills may help you with your research.)\nHints In VS Code, open the Markdown preview on the side, so you can experiment and see whether your formatting is working the way you intend.\n\nCreate at least two commits while you work on the Markdown file. [0.5]\nTry to break your progress up into logical units that can be summarized with a descriptive commit message.\nHints\n\nBad commits/commit messages:\n“First part of the file” along with “Second part of the file”.\nGood commits and commit messages:\n“Summarized key concepts in the Git workflow” along with “Made a table of common Git commands”.\n\nUpdate the README.md file. [0.5]\nBriefly describe the current contents of your repo now that you actually have some contents.\nStage and commit the updated README file. [0.5]\nPart III – Add some “data” files\nCreate a directory with dummy data files. [0.5]\nCreate a directory called data and inside this directory, create 100 empty files with a single touch command and at least one brace expansion (e.g. for samples numbered 1-100, or 20 samples for 5 treatments). Give the files the extension .txt.\nStage and commit the data directory and its contents. [0.5]\nAs always, include a descriptive message with your commit.\nPart IV – Renaming files on a new branch\nAs it turns out, there was a miscommunication, and and all the “data” files will have to be renamed. You realize that this is risky business, and you don’t want to lose any data.\nIt occurs to you that one good way of going about this is creating a new Git branch and performing the renaming task on that branch. Then, if anything goes wrong, it will be easy to go back: you can just switch back to the master branch.\n(Note that there are many other ways of undoing and going back in Git, but this one is perhaps conceptually the easiest, and safe.)\nGo to a new branch. [0.5]\nCreate a new branch called rename-data-files or something similar, and move to the new branch. (Also, check whether this worked.)\nWrite a for loop to rename the files. [1.5]\nYou can keep it as simple as switching the file extension to .csv, or do something more elaborate if you want.\nBecause these files are being tracked by Git, recall that there is a Git-friendly modification to the command to rename files: use that in the loop.\nHints\nThese slides from last week have a similar renaming loop example.\nTo replace the last part of the filename, like the extension, recall that you can strip suffices using the basename command.\nBefore you go ahead and actually rename the files, use echo in your loop to print the (old and) new filenames: if it looks good, then add the renaming command to the loop.\n\nIf you just used mv and forgot to use git mv, don’t fret. Have a look at the status of the repo, then do git add --all and check the status again: Git figured out the rename.\nIf things go wrong, for instance you renamed incorrectly, or even deleted the files, you can start over in a few ways:\nYou can delete any files in data/, commit the deletion, and recreate the files. (The extra commits will not affect your grade, but use your commit messages to clarify what you’re doing.)\nA solution that would always work, including in case these files were actually valuable and you couldn’t just recreate them, is as follows:\nLike above, delete any files in data/ and commit the deletion.\nSwitch back to the master branch. Your data/ files should be back.\nCreate a new renaming branch and move there.\nOptionally, delete the old/failed renaming branch.\n\nCommit the changes made by the renaming operation.\nMerge into the master branch. [0.5]\nWith the files successfully renamed, go back to the master branch.\nThen, merge the rename-data-files branch into the master branch.\n(Optionally, remove the rename-data-files branch, since you will no longer need it.)\nUpdate the README file.\nIn the README file, describe what you did, including some inline code formatting, and put the code for the for loop in a code block.\n(No, the repo doesn’t really make sense as a cohesive unit anymore, but that’s okay while we’re practicing. :) )\nStage and commit the updated README file. [0.5]\nPart V – Create and sync an online version of the repo\nPhew, those were a lot of commits! Let’s share all of this work with the world.\nCreate a Github repository. [0.5]\nGo to https://github.com, sign in, and create a new repository.\nIt’s a good idea to give it the same name as your local repo, but these names don’t have to match.\nYou want to create an empty GitHub repository, because you will upload all the contents from your local repo: therefore, don’t check any of the boxes to create files like a README.\n\nPush your local repo online. [0.5]\nWith your repo created, follow the instructions that Github now gives you, under the heading “…or push an existing repository from the command line”.\nThese instructions will comprise three commands: git remote add to add the “remote” (online) connection, git branch to rename the default branch from master to main, and git push to actually “push” (upload) your local repo.\nWhen you’re done, click the Code button, and admire the nicely rendered README on the front page of your Github repo.\nHints\n\nAssuming that you’re using SSH authentication, which you should have set up in this week’s ungraded assignment, make sure you use the SSH link type: starting with git@github.com rather than HTTPS.\n\n\nCreate an issue to mark that you’re done! [0.5]\nFind the “Issues” tab for your repo on Github:\n\n\n\nIn the Issues tab, click the green button New Issue to open a new issue for you repo.\nGive it a title like “I finished my assignment”, and in the issue text, tag @jelmerp. You can say, for instance, “Hey @jelmerp, can you please take a look at my repo?”.\n\n",
      "last_modified": "2021-03-16T14:33:05-04:00"
    },
    {
      "path": "w03_UA_git-setup.html",
      "title": "Week 3 -- Ungraded Assignment : <br> Git setup and GitHub authentication",
      "author": [],
      "contents": "\nPart 1: Git setup\nThese instructions are for setting up Git at OSC, but from Step 3 onwards, you can also follow them to set up Git for your local computer.\nOpen up a terminal at OSC.\nYou can do this after logging in at https://ondemand.osc.edu in one of two ways:\nDirect shell access: Clusters (top blue bar) > Pitzer Shell Access.\nIn VS Code: Interactive Apps > Code Server > then open a terminal using Ctrl+backtick > break out of the Singularity shell by typing bash.\n\nLoad the OSC Git module.\n(Note: Git is available in any OSC shell without loading any modules, but that is a rather old version – so we load a newer one.)\nmodule load git/2.18.0\nUse git config to make your name known to Git.\nSomewhat confusingly, mote that this should be your actual name, and not your GitHub username:\ngit config --global user.name 'John Doe'\nUse git config to make your email address known to Git.Make sure that you use the same email address you used to sign up for GitHub.\ngit config --global user.email 'doe.391@osu.edu'\nUse git config to set a default text editor for Git.\nOccasionally, when you need to provide Git with a “commit message” to Git and you haven’t entered one on the command line, Git will open up a text editor for you. Even though we’ll be mostly be working in VS Code during this course, in this case, it is better to select a text editor that can be run within the terminal, like nano (or vim, if you are familiar with it). To specify Nano as your default text editor for Git:\ngit config --global core.editor \"nano -w\"\n\n# You could set VS Code as your editor on your local computer,\n# if you use it there:\n# git config --global core.editor \"code --wait\"\nCheck whether you successfully changed the settings:\ngit config --global --list\n\n# user.name=John Doe\n# user.email=doe.39@osu.edu\n# colour.ui=true\n# core.editor=nano -w\nSet colors if needed.\nMake sure you see colour.ui=true in the list (like above), so Git output in the terminal will use colors. If you don’t see this line, set it using:\ngit config --global color.ui true\n\nPart 2: GitHub authentication\nGitHub authentication: Background\nTo be able to link local Git repositories to their online counterparts on GitHub, we need to set up GitHub authentication.\nRegular password access (over HTTP/HTTPS) is now “deprecated” by GitHub, and two better options are to set up SSH access with an SSH key, or HTTPS access with a Public Access Token.\nWe’ll use SSH, as it is easier – though still a bit of drag – and because learning this procedure will also be useful for when you’ll be setting up SSH access to the Ohio Supercomputer Center. (But note that GitHub now labels HTTPS access as the “preferred” method.)\nFor everything on GitHub, there are separate SSH and HTTPS URLs, and GitHub will always show you both types of URLs. When using SSH, we need to use URLs with the following format:\ngit@github.com:<USERNAME>/<REPOSITORY>.git\n(And when using HTTPS, you would use URLS like https://github.com/<USERNAME>/<REPOSITORY>.git)\nSetting up GitHub SSH authentication\nThese instructions are for setting up authentication at OSC, but you can repeat the same steps to set up authentication for your local computer.\nIn a terminal at OSC, use the ssh-keygen command to generate a public-private SSH key pair like so:\nssh-keygen -t rsa\nYou’ll be asked three questions, and for all three, you can accept the default by just pressing Enter:\n# Enter file in which to save the key (<default path>):\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again: \n\n\n\nNow, you have a file called id_rsa.pub in your ~/.ssh folder, which is your public key. To enable authentication, we will put this public key on GitHub – our public key interacts with our private key, which we do not share.\nPrint the public key to screen using cat:\ncat ~/.ssh/id_rsa.pub\nCopy the public key, i.e. the contents of the public key file, to your clipboard. Make sure you get all of it, including the “ssh-rsa” part (but beware that your new prompt may start on the same line as the end the key):\n\n\n\nIn your browser, go to https://github.com and log in.\nGo to your personal Settings. (Click on your avatar in the far top-right of the page, and select Settings in the drop-down menu.)\nClick on SSH and GPG keys in the sidebar.\nClick the green New SSH key button.\nGive the key an arbitrary, informative name, e.g. “OSC” to indicate that you are using this key at OSC.\nPaste the public key, which you copied to your clipboard earlier, into the box.\n\n\nClick the green Add SSH key button. Done!\n\n\n\n\n",
      "last_modified": "2021-03-16T14:33:06-04:00"
    },
    {
      "path": "w04_exercises.html",
      "title": "Exercises: Week 4",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nExercise 1: Pseudogenes with grep, uniq -c, and others\nExercise 2: Calculations with awk\nExercise 3: Replacements with sed\n\nBonus exercises\nExercise 4: FASTA\nExercise 5: Exons\nExercise 6: Miscellaneous\n\nSolutions\nExercise 1: Pseudogenes with grep, uniq -c, and others\nExercise 2: Calculations with awk\nExercise 3: Replacements with sed\nExercise 4: FASTA\nExercise 5: Exons\nExercise 6: Miscellaneous\n\n\nMain exercises\nThese exercises will use the same files that we’ve been working with in class: those in chapter-07-unix-data-tools in the bds-files repository for the Buffalo book.\nYou can either work directly in that directory, or create a “sandbox” directory with the Buffalo files copied.\nFor many of the exercises, there are multiple ways of solving them. The hints and solutions provide one or a few ways, but you may well end up using a different approach. If you struggle, try to follow the approach suggested by the hints, so you can later cross-check with solutions that use this same approach.\nFor most questions, the code required is quite similar to the examples in Buffalo chapter 7 (and the slides). If the hints are too cryptic for you, I recommend you first go back to the section on the relevant command in the chapter.\nExercise 1: Pseudogenes with grep, uniq -c, and others\nIn this exercise, we will start using the file Mus_musculus.GRCm38.75_chr1.gtf that we’ve seen in class.\nLet’s say that you need to extract annotation information for all pseudogenes on mouse chromosome 1 from the GTF, and write the results to a new GTF file. We’ll do that in this exercise with a detour along the way.\nCount the number of lines with the string “pseudogene”.\nHints Use grep -c to count the matches.\n\n\nDoes this mean there are over a 1,000 pseudogenes on this chromosome? No, because there are “nested” genomic features in the file: for each gene, several “subfeatures” are listed, such as individual exons. This genomic feature type information is listed in column 3.\nFor lines that match “pseudogene” as above, which different feature types are listed in column 3, and what are their counts (i.e., create a count table for column 3)?\nAre there any coding sequences (“CDS”), start codons (“start_codon”) and stop codons (“stop_codon”)?\nHints\n\nTo answer this question, compute count tables using sort | uniq -c after grepping for “pseudogene”.\n\n\nIf you are more restrictive in your grep matching to only match “pseudogene” as a full “word”, do you still see CDS or start/stop codons?\nHints Use grep -w to only match full “words”: consecutive alphanumeric characters and the underscore.\nThis would avoid matching longer words that contain “pseudogene”, such as “transcribed_pseudogene”.\n\n\nWe have “exon”, “transcript” and “gene” features in column 3, but we want just the gene-level coordinates of the pseudogenes: after grepping for “pseudogene” like in the previous question, select only rows for which the feature type in column 3 is gene.\nPipe your output into head or less; we don’t want to write to a new file yet.\nHints\n\nThe “gene” you want to match is in its own column. Matching restrictively using the -w flag to only match whole “words” will work in this case.\nEven better would be to explicitly match the tabs surrounding “gene”. This can be done using the -P flag to grep (grep -P) and the \\t symbol for tabs.\nYet another approach that is even more explicit uses awk to make the match specifically for the 3rd column (recall that $3 is the third column in awk, so you can use $3 == \"gene\"). If you try with awk, you’ll have to make sure to ignore the header lines, which are not tabular.\n\n\nComprehensively check that your output only contains “gene” in column 3.\nHints\n\nPipe your previous command into cut and then sort | uniq -c to quickly check which values are present in column 3.\n\n\nNow, you are almost ready to write the results to a new GTF file, called pseudogenes.gtf, with the features you selected above: gene-level pseudogene features.\nOne challenge is that you also want the header, which you won’t get when redirecting the output from the previous command.\nA straightforward solution would be to first write the header to a new file, and then append the lines with the selected features.\nAlternatively, you can use subshells to do this in a single go. If you want to try this, make sure you have read the Buffalo section on subshells (p. 169-170) first.\nHints\n\nStructure the subshell solution as follows:(<select-header> ; <select-from-rest-of-file>) > pseudogenes.gtf\n\n\nExercise 2: Calculations with awk\nIn this exercise, you will continue working with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nHow many elements are at least 1,000 base pairs long?\nHints\nRemove the header from consideration using grep -v.\nFilter by feature length using awk, and get the length by subtracting column 4 from column 5.\n\nHow many genes (“gene” in column 3) on the + strand (“+” in column 7) are at least 1,000 base pairs long?\nHints\nAdjust your command for the previous question as follows:\nMatch columns 3 (feature type) and 7 (strand) with awk, using == to only select matching rows.\nUse the && operator in awk to combine conditions.\n\nWhat’s the combined size of all genes?\n(On this chromosome, that is: this GTF only contains a single chromosome.)\nHints\nFirst, remove the header from consideration using grep -v.\nNext, select “gene”-level features only, which you can do with awk as above, with grep -w, or with grep -P with \\t for tab.\nNext, use a variable in awk:\nInitialize the variable in a BEGIN statement;\nAdd the length of the gene (column 5 - column 4) for each row;\nPrint the result with an END statement.\n\n\nWhat is the mean UTR length?\nHints\nThe code is very similar to that for the previous question; just grep for “UTR” instead, and calculate the mean by dividing the final value of awk variable by the number of rows (NR) in the END statement.\n\n\nExercise 3: Replacements with sed\nThe genotypes.txt contains diploid genotypes for 4 loci for 9 individuals.\nRemove all underscores from genotypes.txt and write the output to a new file.\nHints\nUse the sed 's/<pattern>/<replacement>/<modifiers>' <file> construct.\nYou’ll need the modifier to do global (i.e., >1 per line) replacements.\n\n\nReplace the / delimiter for genotypes (G/G) in genotypes.txt by a colon (G:G) and write the output to a new file.\nHints\nIn the sed 's/<pattern>/<replacement>/<modifiers>' <file> construct, forward slashes / will not work here, because the pattern we are looking for is a forward slash. Use a different symbol instead, e.g. a #:sed 's#<pattern>#<replacement>#<modifiers>' <file>\n\n\nBonus exercises\nExercise 4: FASTA\nFirst, convert the FASTQ file contaminated.fastq to a FASTA file using the following awk command, and try to understand what it does, by and large:\nawk '{ if(NR%4 == 1) { printf(\">%s\\n\", substr($0,2)); } else if(NR%4 == 2) print; }' \\\n    contaminated.fastq > contaminated.fasta\nSelect FASTA entries (sequence and header) from contaminated.fasta with the sequence AGATCGGAAGA and write them to a new file, contaminated_sel.fasta. (Your new file should have 6 sequences, so 12 lines.)\nHints\n\nMatch the sequence with grep and use the -B option to also retrieve the preceding line, which is the sequence header/identifier. In addition, use --no-group-separator to prevent grep from printing -- between matches (alternatively, you could use another call to grep with -v to filter out -- lines).\n\n\nCheck how many sequences, if any, have one ore more undetermined bases (N).\nHints\n\nMake sure you first exclude the header lines, since you only want to match N in the sequences.\n\n\nApparently, there may have been a problem with cycle 50 in the Illumina run.\nGet the frequency of each base for position 50 in the FASTA file.\nHints\n\nUse cut -c <n> to extract the 50th position.\n\n\n\nExercise 5: Exons\nIn this exercise, you will once again work with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nNow, let’s explore some statistics on exons in the GTF file.\nCreate a tab-separated, two-column file called exons.txt that lists each unique “exon_id” along with its “gene_id”, sorted by gene ID. (The “exon_id” and “gene_id” key-value pairs are in the huge final column.)\nThe first couple of lines of the file should look as follows:\n  ENSMUSG00000000544      ENSMUSE00000160546\n  ENSMUSG00000000544      ENSMUSE00000160549\n  ENSMUSG00000000544      ENSMUSE00000226264\n  ENSMUSG00000000544      ENSMUSE00000395516\nHints\n\nFirst grep for lines that contain “gene_id” followed by “exon_id”.\nUse a sed command with two backreferences (capture the backreferences with parentheses, (<match>), and recall them with \\1 and \\2) to extract just the gene ID and the exon ID.\nFinally, only keep unique lines and sort.\n\nUse the exons.txt file you created to answer the following questions.\nOn average, how many exons does each gene have?\nHints\n\nThe number of genes is simply the number of lines in exons.txt.\nCount the number of exons using uniq and then wc -l.\nThe division can be done with expr $n_genes / $n_exons although you will not get any decimals (arithmetic in bash is very limited!). The above exmplae assumes you assigned the results to variables using command substitution, e.g.: ngenes=$(wc -l ...).\n(You could also, e.g., call Python inline, but we’re not there yet.)\n\nWhat is the highest number of exons for one gene?\nHints\n\nUse uniq -c followed by a reverse numeric sort.\n\n\nCreate a count table for the number of exons per gene. It should tell you how many genes have one exon, how many have two exons, etc.\nHints\n\nYou’ll need to process the uniq -c output of exon counts to get rid of the leading spaces. You can do this with sed; use ^ * as the search pattern, or ^ + with sed -E (the + symbol to match one ore more of the preceding character is in the extended regex set).\nThen, you’ll need to create a count table from these counts, so another round of count | uniq -c.\n\nExercise 6: Miscellaneous\nIn this exercise, you will once again work with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nCount the total number of features on each strand (+ or -, column 7).\nCount the number of features from each of the three values for “gene_source” (“ensembl”, “havana”, and “ensembl_havana”).\nHints\n\nYou can either use sed with a backreference to capture the “gene_source” type, or grep -o to capture a larger match followed by cut and sed to extract just the “gene_source” type.\n\n\nHow many genes have a UTR (untranslated region)? Create a list of “gene_id”s for genes with a UTR.\nHints\n\nFirst grep for UTR, then for the “gene_id” while also extracting the match with grep -o. Once again, the extraction of a match can alternatively be done with a backreference in sed.\n\n\nSolutions\nExercise 1: Pseudogenes with grep, uniq -c, and others\nTo start with, I will assign the name of the GTF file to the variable $gtf, for quick reference in the solutions for the and the next exercises:\ngtf=Mus_musculus.GRCm38.75_chr1.gtf\n1. Count the number of lines with the string “pseudogene”.\n$ grep -c \"pseudogene\" $gtf\n\n#> 1041\nThe -c option to grep will count the number of matching lines.\n\n2. Count feature types.\n$ grep -v \"^#\" $gtf | grep \"pseudogene\" | cut -f 3 | sort | uniq -c\n\n#>     15 CDS\n#>    524 exon\n#>    223 gene\n#>      4 start_codon\n#>      4 stop_codon\n#>    264 transcript\n#>      7 UTR\nThe first line computes a count table for pseudogenes, and the second line computes an equivalent count table for all genes.\nWe use grep -v \"^#\" to exclude the header (the command would work without this line, but it’s better not to assume the header won’t match, and to explicitly remove it first). Note that the caret ^ is a regex symbol for the beginning of the line. We use the -v option to grep to invert the match.\nWe use cut -f 3 to select the third column.\nWe use the sort | uniq -c combination to create a count table.\n3. Count feature types after more restrictive matching.\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | cut -f 3 | sort | uniq -c\n\n#>    499 exon\n#>    221 gene\n#>    258 transcript\nThis is very similar to the solution for the previous exercise, but now we use the -w option to grep to only select full words (consecutive alphanumeric characters and the underscore).\n\n4. Select “pseudogene” matches for which the feature type in column 3 is “gene”.\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | grep -w \"gene\" | head\n\n# Alternative 1: More explicit match by matching tabs:\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | grep -P \"\\tgene\\t\" | head\n\n# Alternative 2: More explicit match by matching column 3 only:\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | awk '$3 == \"gene\"' | head\nTo match tabs like in the second solution, we need to use greps -P option, for \"Perl-like regular expressions. (Yes, it confusing and unfortunate that there are three types of regular expressions in grep!)\nIn the third solution we explicitly and exactly match column 3 with awk – this is the most robust solution.\n\n5. Check that your output only contains “gene” in column 3.\n$ grep -v \"^#\" $gtf | grep \"pseudogene\" | grep -w \"gene\" | \\\n      cut -f 3 | sort | uniq -c\n\n#> 223 gene\nWe select the third column and use the sort | uniq -c combination to create a count table, which will quickly tell us whether there are multiple values in the column.\n\n6. Write the pseudogene selection to a new GTF file.\nSolution when writing the header and the rest of the file separately (don’t forget to use >> for appending the output of the second line, if you use this method!):\n$ grep \"^#\" $gtf > pseudogenes.gtf\n$ grep -v \"^#\" $gtf | \\\n      awk '$2 == \"pseudogene\" && $3 == \"gene\"' \\\n      >> pseudogenes.gtf\nSolution with subshells:\n$ (grep \"^#\" $gtf; grep -v \"^#\" $gtf | \\\n      awk '$2 == \"pseudogene\" && $3 == \"gene\"') \\\n      > pseudogenes.gtf\nNote: rather than with grep -v \"^#\", we can also exclude the header using !/^#/ in awk:\n$ (grep \"^#\" $gtf; \\\n      awk '!/^#/ && $2 == \"pseudogene\" && $3 == \"gene\"' $gtf) \\\n      > pseudogenes.gtf\n\nExercise 2: Calculations with awk\n1. How many features are at least 1,000 bp long?\n$ grep -v \"^#\" $gtf | \\\n      awk '$5 - $4 > 999' | \\\n      wc -l\n\n#> 8773\nFirst, we again exclude the header like in exercise 1 (this can also be done using awk: see below).\nNext we subtract column 4 ($4) from column 5 ($5) and use as a condition that this difference should be larger than 999.\n# Alternative that excludes the header in awk:\n$ awk '!/^#/ && $5 - $4 > 999' $gtf | \\\n      wc -l\n\n#> 8773\n\n2. How many genes on the + strand are at least 1,000 bp long?\n$ grep -v \"^#\" $gtf | \\\n      awk '$3 == \"gene\" && $7 == \"+\" && $5 - $4 > 999' | \\\n      wc -l\n\n#> 725\nWe chain together different conditions using &&.\nThe second line uses awk to match + in column 7, and to select lines where the difference between column 5 and 4 (i.e., the feature length) is larger than 999 bp.\nThe third line simply counts the number of lines we have left.\n2. What’s the combined size of all genes?\n$ grep -v \"^#\" $gtf | \\\n      grep -w \"gene\" | \\\n      awk 'BEGIN { s=0 }; { s += $5 - $4 }; END { print s }'\n  \n#> 78935729    # 78,935,729 => ~79 Mbp\nThe first two lines are the same as for the previous question (see the solution there for further details).\nIn the third line, we use a variable in awk to compute a sum of the feature lengths (i.e., column 5 minus column 4). With the BEGIN statement, we initialize the variable s before we start processing lines. Then, for each line, we add the feature length to the value using the += operator (shorthand for s = s +). Finally, with the END statement, we report the final value of the variable s after we have processed all lines.\n3. What is the mean UTR length?\n$ grep -v \"^#\" $gtf | \\\n      grep -w \"UTR\" | \\\n      awk 'BEGIN { s=0 }; { s += $5 - $4 }; END { print s/NR }'\n      \n#> 472.665      \nThis code is very similar to that for the previous question (see that solution for more details), but now we divide the sum (variable s) by NR, an automatic variable in awk that holds the current record (line) number. Since the END statement is executed after all lines have been processed, the record number will be the total number of lines in the file.\n\nExercise 3: Replacements with sed\n1. In genotypes.txt, remove all underscores.\n$ sed 's/_//g' genotypes.txt > genotypes2.txt\n\n$ cat genotypes2.txt\n#> id      indA    indB    indC    indD\n#> S000    G/G     A/G     A/A     A/G\n#> S001    A/T     A/T     T/T     T/T\n#> S002    C/T     T/T     C/C     C/T\n#> S003    C/T     C/T     C/T     C/C\n#> S004    C/G     G/G     C/C     C/G\n#> S005    A/T     A/T     A/T     T/T\n#> S006    C/G     C/C     C/G     C/G\n#> S007    A/G     G/G     A/A     G/G\n#> S008    G/T     G/T     T/T     G/T\n#> S009    C/C     C/C     A/A     A/C\nSince there are multiple underscores on line 1, you need the g modifier.\n\n2. In genotypes.txt, replace the / delimiter by a colon (:).\n$ sed 's#/#:#g' genotypes.txt > genotypes3.txt\n\n$ cat genotypes3.txt\n#> id      ind_A   ind_B   ind_C   ind_D\n#> S_000   G:G     A:G     A:A     A:G\n#> S_001   A:T     A:T     T:T     T:T\n#> S_002   C:T     T:T     C:C     C:T\n#> S_003   C:T     C:T     C:T     C:C\n#> S_004   C:G     G:G     C:C     C:G\n#> S_005   A:T     A:T     A:T     T:T\n#> S_006   C:G     C:C     C:G     C:G\n#> S_007   A:G     G:G     A:A     G:G\n#> S_008   G:T     G:T     T:T     G:T\n#> S_009   C:C     C:C     A:A     A:C\nBecause the pattern we want to match is a /, we use a different symbol in the sed syntax: here, I chose a #.\n\nExercise 4: FASTA\n2. Write sequences with AGATCGGAAGA into a new file.\n$ grep -B 1 --no-group-separator \"AGATCGGAAGA\" contaminated.fasta \\\n      > contaminated_sel.fasta\n\n$ wc -l < contaminated_sel.fasta\n\n#> 12\n\n# Alternative to `--no-group-separator`;\n# remove the lines with `--` afterwards:\n$ grep -B 1 \"AGATCGGAAGA\" contaminated.fasta | \\\n      grep -v \"--\" > contaminated_sel.fasta\nUse B 1 to get the sequence ID line for each match.\nUse --no-group-separator (top solution) or pipe into grep -v \"--\" to avoid having a group separator, which grep inserts for multiline output like this, in the output.\n3. Check how many sequences have one ore more undetermined bases (N).\nMake sure you first exclude the header lines, since you only want to match “N” in the sequences themselves:\n$ grep -v \"^>\" contaminated.fasta | grep -c \"N\"\n\n#> 0\nThere are no Ns in the sequence.\n\n4. Get the frequency of each base for position 50 in the fasta file.\n$ grep -v \"^>\" contaminated.fasta | cut -c 50 | sort | uniq -c \n\n#> 2 C\n#> 3 G\n#> 3 T\n\nExercise 5: Exons\n1. Create a tab-delimited file with “exon_id” and “gene_id”\n$ grep \"gene_id.*exon_id\" $gtf | \\\n      sed -n -E 's/.*gene_id \"(\\w+)\".*exon_id \"(\\w+).*/\\1\\t\\2/p' | \\\n      sort | uniq > exons.txt\nIn the first line of code, we select only lines that contain both “gene_id” and “exon_id”.\nIn the second line, we capture the values for “gene_id” and “exon_id” using backreferences in sed.\nFinally, we sort (since we want to sort by the first column, just sort will work) and then take only unique rows using uniq, and redirect to a new file.\n2. On average, how many exons does each gene have?\n$ n_genes=$(wc -l < exons.txt)\n$ n_exons=$(cut -f 1 exons.txt | uniq | wc -l)\n\n# Divide using `expr`:\n$ expr $(wc -l < exons.txt) / $(cut -f 1 exons.txt | uniq | wc -l)\n\n#> 11\n\n# Alternatively, do the division with Python:\n$ python -c \"print($n_genes / $n_exons)\"\n\n#> 11.627\nFirst, we count the number of genes, then the number of exons, and we assign each value to a variable.\nThen, we divide using expr (or Python to get decimals).\n3. What is the highest number of exons for one gene?\n$ cut -f 1 exons.txt | uniq -c | sort -rn | head\n\n#>    134 ENSMUSG00000026131\n#>    116 ENSMUSG00000066842\n#>    112 ENSMUSG00000026207\n#>    108 ENSMUSG00000026609\n#>    102 ENSMUSG00000006005\n#>     97 ENSMUSG00000037470\n#>     93 ENSMUSG00000026141\n#>     92 ENSMUSG00000026490\n#>     91 ENSMUSG00000048000\n#>     82 ENSMUSG00000073664\n“ENSMUSG00000026131” has as many as 134 exons!!\n\n3. Create a count table for the number of exons per gene.\n$ cut -f 1 exons.txt | uniq -c | \\\n      sed 's/^ *//' | cut -d \" \" -f 1 | \\\n      sort -rn | uniq -c | \\\n      sort -rn > exon_count_table.txt\n\n$ head exon_count_table.txt     \n#>    646 1   # 646 genes with 1 exon\n#>    186 2   # 186 genes with 2 exons\n#>     94 3   # etc...\n#>     71 5\n#>     71 4\n#>     53 8\n#>     51 7\n#>     51 6\n#>     44 11\n#>     41 10\nIn the first line, we get the number of exons for each gene.\nIn the second line, we clean up the uniq -c output.\nIn the third line, we create the count table of the number of exons.\nFinally we sort this and redirect the output to a file.\nExercise 6: Miscellaneous\n1. Count the number of elements on each strand.\n$ grep -v \"^#\" $gtf | cut -f 7 | sort | uniq -c\n\n#> 40574 +\n#> 40652 -\n\n2. Count the number of elements for each of the three values for “gene_source”.\n# Capture the gene_source with sed:\n$ sed -n -E 's/.*gene_source \"(\\w+)\".*/\\1/p' $gtf | \\\n      sort | uniq -c\n\n#> 18209 ensembl\n#> 61089 ensembl_havana\n#> 1928 havana\n\n# Alternatively, capture the gene_source with `grep -o`:\n$ grep -E -o 'gene_source \"\\w+\"' $gtf | \\\n      cut -f2 -d\" \" | sed 's/\"//g' | \\\n      sort | uniq -c\n      \n#> 18209 ensembl\n#> 61089 ensembl_havana\n#> 1928 havana\n\n3. How many genes have a UTR? Create a list of gene_ids for genes with a UTR.\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n   grep -E -o 'gene_id \"\\w+\"' | sort | uniq | wc -l\n\n#> 1179\n\n# Create the list of genes:\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n      grep -E -o 'gene_id \"\\w+\"' | \\\n      cut -f2 -d\" \" | sed 's/\"//g' | \\\n      sort | uniq > genes_with_UTR.txt\n\n# Alternative with sed:\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n      sed -n -E 's/.*gene_id \"(\\w+)\".*/\\1/p' | \\\n      sort | uniq | wc -l > genes_with_UTR.txt\n\n\n\n\n",
      "last_modified": "2021-03-16T14:33:07-04:00"
    },
    {
      "path": "w05_exercises.html",
      "title": "Week 5 Exercises",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nBackground\nExercise 1: Getting set up\nExercise 2: Create a script to compute stats for a FASTQ file\nExercise 3: Modify the looping script\nExercise 4: Bells and whistles\n\nBonus exercises\nExercise 5: Find the longest file\nExercise 6: Plant-pollinator networks\nExercise 7: Data explorer\n\nSolutions\nExercise 1\nExercise 2\nExercise 3\nExercise 4\nExercise 5\nExercise 6\nExercise 7\n\n\nThe main exercises will work with some FASTQ files. If you don’t care much for DNA sequence files, and perhaps start to get lost in the technicalities, make sure to carry on to the three bonus exercises.\nMain exercises\nBackground\nThese exercises will work with 6 FASTQ files with sequences from the V4 region of 16S rRNA, generated in a metabarcoding experiment.\nThe FASTQ files come in pairs: for every sample, there is a FASTQ file with forward reads (or “read 1” reads) that contains _R1_ in its filename, and a FASTQ file with corresponding reverse reads (or “read 2” reads) that contain _R2_ in its filename. So, our 6 FASTQ files consist of 3 pairs of files with forward and reverse reads for 3 different biological samples.\nThe sequences were generated by first amplifying environmental samples with a pair of universal 16S primers, and these primer sequences are expected to be present in the FASTQ sequences. You will search for these primer sequences below, and there are two things to be aware of:\nA primer can also be present in the FASTQ sequence as its reverse complement, so we will search for reverse complements too.\nThe primers contain a few variable sites, which are indicated using ambiguity codes. For instance, an R means that the site can be either an A or a G, and an N means that the site can be any of the four bases. See here for a complete overview of these ambiguity codes.\nHere are the primer sequences and their reverse complements1:\nForward primer (“515F”): GAGTGYCAGCMGCCGCGGTAA / TTACCGCGGCKGCTGRCACTC.\nReverse primer (“806R”): ACGGACTACNVGGGTWTCTAAT / ATTAGAWACCCBNGTAGTCCGT.\nExercise 1: Getting set up\nCreate a directory for this week’s exercises and move into it.\nIn these exercises, you will be working with and modifying one of the scripts in Buffalo’s Chapter 12, which is printed below. Save this script as fastq_stat_loop.sh.\n#!/bin/bash\nset -e -u -o pipefail\n\n# Specify the input samples file (3rd column = path to FASTQ file):\nsample_info=samples.txt\n\n# Create a Bash array from the third column of \"$sample_info\":\nsample_files=($(cut -f 3 \"$sample_info\"))\n\n# Loop through the array:\nfor fastq_file in ${sample_files[@]}; do\n\n  # Strip .fastq from each FASTQ file, and add suffix:\n  results_file=\"$(basename $fastq_file .fastq)-stats.txt\"\n\n  # Run \"fastq_stat\" on a file:\n  fastq_stat \"$fastq_file\" > stats/$results_file\n\ndone\nThe FASTQ files you’ll work with are inside the directory /fs/ess/PAS1855/data/week05/fastq. Copy these files into an appropriate directory inside your own directory for these exercises, like data/.\n\nExercise 2: Create a script to compute stats for a FASTQ file\nUnfortunately, the fastq_stat program referenced in Buffalo’s script is an imaginary program… So, let’s create a script fastqc_stat.sh that actually produces a few descriptive statistics for a a FASTQC file.\nSet up a script skeleton with the header lines we’ve been discussing: a shebang line and the various set settings for robust scripts.\nThe script should process one command-line argument, the input file. Assign the automatic placeholder variable for the first argument to a variable with a descriptive name, like fastq_file.\nHints\n\n\nThe automatic placeholder variable for the first argument is $1.\n\n\n\nIn the sections below, you can print all your output to standard out, i.e. simply use echo with no redirection.\nAlso, just for the purpose of testing the commands while developing them below, it will be convenient to assign one of the FASTQ files to a variable fastq_file.\n\n\nHave the script report its own name as well as the name of the FASTQ file that is being processed.\nHints\n\n\nRecall that the name of the script is automatically stored in $0.\n\n\nCompute and report the number of sequences in the FASTQ file.\nHints\n\nThe number of sequences can be assumed to be the total number of lines divided by 4 (or alternatively, the number of lines consisting only of a +). Recall that FASTQ files have 4 lines per sequence: a header, the sequence, a + divider, and the quality scores.\nTo make the division, you can use syntax like expr 12 / 4 – and you can use command substitution, $(wc -l ...), to insert the number of lines in the division.\n\n\nAs mentioned in the Background section, the primer sequences should be present in the FASTQ files. Prior to sequence analyses, these are usually removed with a specialized program like cutadapt, but we can use our grep skills to quickly search for them.\nWe will search for the forward primers only in the forwards reads, and for the reverse primers only in the reverse reads. Therefore, start with an if statement that tests whether the file name contains forward (_R1_) or reverse (_R2_) reads.If it contains forward reads, you should be counting occurrences of the forward primer or its reverse complement. Similarly, if it contains reverse reads you should be counting occurrences of the reverse primer or its reverse complement.\nYou’ll also have to replace the ambiguity codes, like R, with character classes that enumerate the possible alternative bases.\nHints\n\n\nThe test in the if statement should be a grep command that examines whether the filename contains _R1_ (pipe echo output into grep). The grep standard output should be redirected to /dev/null, since we’re only interested in the exit status: If grep finds a match, the commands following then will be executed; if it doesn’t, the commands following else will be executed.\nYou can assume that matches will only be made in the sequences themselves (and not the headers or quality scores), so you can grep the file as is: you don’t need to first select the lines with sequences.To replace ambiguity codes with character classes in the grep regular expression: an R in the primers becomes [AG] in the grep expression, e.g. the partial sequence ATRG would become AT[AG]G in your regular expression.\nYou’ll need both the primer and its reverse complement in a single grep regular expression: separate them with an logical “or” (|) and to enable this, use grep -E for extended regular expressions.\n\n\nBonus: Print a count table of sequence lengths.\nHints\n\n\nYou’ll have to select only the lines with the actual DNA sequences, and the best way of doing that is using awk like so (see the Solution for an explanation of why this works):\nawk '{ if(NR%4 == 2) }'\nNext, you need to count the number of characters in each line, which is best done in the same awk command using print length($0), which will print the number of characters in the line.\nAfter that, it’s the familiar sort | uniq -c idiom to create a count table.\n\n\nMake the script executable.\nHints\n\n\nUse the chmod command.\n\n\nRun the script for a single FASTQ file by calling it from the command line.\n\nExercise 3: Modify the looping script\nNow, let’s modify Buffalo’s script fastq_stat_loop.sh.\nCurrently, the metadata file that contains the list of files to process is hard-coded as samples.txt. Also, the file names have to be extracted from a specific column from the metadata file. This is not a very flexible setup, and is sensitive to minor changes in a file like samples.txt.\nChange the script to let it accept a list of FASTQ file names as an argument.\nHints\n\n\nRecall that the placeholder variable for the first argument that is passed to a script on the command line is $1.\nInside the command substitution ($()) that populates the array, you can simply use cat instead of cut on the file that contains the list of file names, since this file will no longer have multiple columns.\n\n\nCurrently, the output directory is also hard-coded, as stats – let’s instead add the output directory as a second argument to the script. Moreover, add code that creates this output directory if it doesn’t already exist.\nHints\n\n\nYou can write an explicit test to see if the output dir exists first, but simply using mkdir -p will also work: with the -p option, mkdir doesn’t complain when a dir already exists (and can also make multiple levels of directories at once).\n\n\nChange the line that runs the imaginary program fastq_stat to let it run your fastq_stat.sh script instead. Make sure the path to your script and the path to the output file is correct.\nIn each iteration of the loop, let the script report which FASTQ file will be analyzed.\nNow that the script takes arguments, we need another file or script to create the list of filenames and to submit the script with the appropriate arguments. Specifically, in this file, we need:\nA line that creates a new file just containing the FASTQ file names (akin to column 3 from samples.txt – but with the paths to our actual FASTQ files).\nA line that runs the fastq_stat_loop.sh script. The file that contains the list of FASTQ files should be passed to the script as the first argument, and the output directory as the second argument.\nAs for the actual path to the output dir, you can use whatever (relative!) path makes sense to you.\nCreate the file containing the code outlined above. You can save this file either as a .sh script (e.g. fastqc_runner.sh), or put these lines in a Markdown file inside a code block. Either option is reasonable because these lines would likely be run interactively, as opposed to the fastq_stat_loop.sh script which will be run non-interactively. It’s still important to save these interactive commands in a file, so you know what you did and can easily reproduce it.\nMake fastq_stat_loop.sh executable and run it.\nThe loop script should run fastq_stat.sh on all your FASTQ files. Check the output, which should be in one file per FASTQ file in the output dir you designated. You should be seeing that the vast majority of reads contain the primer sequences. Be proud – with a quick script that only runs for a couple of seconds, and no specialized software, you have queried hundreds of thousands of sequences!\n\nExercise 4: Bells and whistles\nIn this exercise, you will touch up your fastqc_stat.sh script to include tests for robustness, and to report what the script is doing.\nFor the tests, check whether they work (…)! For instance, to check the test for the number of arguments, try running the script with no arguments, and also with two arguments, and see if the script produces the error messages you wrote.\nWrite an if statement to check whether the FASTQ file exists / is a regular file and whether it can be read. If not, give an error and exit the script with exit code 1.\nHints\n\n\nGo back to this week’s slides for an example of testing whether a file is a regular file (-f) and whether it can be read (-r).\nTo exit with exit code 1, simply use: exit 1. This should be done after printing any error messages you, or those won’t actually be printed.\n\n\nCheck whether exactly 1 argument was passed to the script on the command line. If not, return an error, succinctly report how to use the script (“usage: …”), and exit with exit code 1.\nHints\n\n\nGo back to this week’s slides for an example of a very similar test.\nThe number of arguments that were passed to a script are automatically available in $#.\nYou can test for equality using <integer> -eq <integer> (e.g. 10 -eq 10 which will evaluate to true) and you can negate a test (\"number of argument is not equal to 1) using a ! before the comparison expression.\n\n\nAdd date commands at the start and the end of the script, so you’ll be able to tell how long it took the script to complete.\n\nBonus exercises\nExercise 5: Find the longest file2\nWrite a shell script called longest.sh that takes two arguments: the name of a directory and a file extension (like txt). The script should print the name of the file that has the most lines among all files with with that extension in that directory.\nMake sure the script has the shebang and set headers, and make the script executable.\nThen, run your script to learn which FASTQ file has the most lines (and sequences):\n$ ./longest.sh data/fastq fastq\n… should print the name of the .fastq file in data/fastq with the highest number of lines and therefore sequences.\nHints\n\nYou can count lines for many files at once using wc -l: simply provide it with a globbing pattern.\n\nExercise 6: Plant-pollinator networks\nThis exercise is slightly modified after 1.10.3 from the CSB book. The Saavedra2013 directory can be found inside the CSB repository at CSB/unix/data/Saavedra2013, and the following code assumes you are in the directory CSB/unix/sandbox. (If you no longer have the repository, download it again using git clone https://github.com/CSB-book/CSB.git.)\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files as an argument, and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces. Don’t forget to make your script executable and to add the standard header lines. Your script should return:\n$ ./netsize.sh ../data/Saavedra2013/n1.txt\n\n#> Filename: ../data/Saavedra2013/n1.txt\n#> Number of rows: 97\n#> Number of columns: 80\nWrite a script that prints the numbers of rows and columns for each network, taking the directory containing all the files as an argument:\n$ ./netsize_all.sh ../data/Saavedra2013\n\n#> ../data/Saavedra2013/n10.txt     14      20\n#> ../data/Saavedra2013/n11.txt     270     91\n#> ../data/Saavedra2013/n12.txt     7       72\n#> ../data/Saavedra2013/n13.txt     61      17\n#> …\nHints\n\nTo find the number of columns, use awk and recall awk’s NF (number of fields => number of columns) keyword.\nTo combine them in a script, use command substitution to assign the result of a command to a variable. (For example: mytxtfiles=$(ls *.txt) stores the list of .txt files in the variable $mytxtfiles.)\nNext, you need to write a for loop.\nYou can now use the script you’ve just written in combination with sort to answer the questions (remember the option -k to choose a column and -r to reverse the sorting order).\nYou can use echo -e to print tabs using \\t: echo -e \"column1 \\t column2\".\n\nExercise 7: Data explorer\nThis is slightly modified after exercise 1.10.4 from the CSB book. The Buzzard2015_data.csv file can be found inside the CSB repository at CSB/unix/data/Buzzard2015_data.csv.\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\n1. Write a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nDon’t forget to make your script executable and add the standard header lines.\nFor example, running the script with:\n$ ./explore.sh ../data/Buzzard2015_data.csv 7\n…should return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\nHints\n\nYou can select a given column from a csv file using the command cut. Then,\nThe column name is going to be in the first line (header); access it with head.\nFor the next few commands, you’ll need to remove the header line – the tail trick to do so is tail -n +2.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail).\nRename the placeholders $1 and $2 for the command-line arguments to named variables for the file name and column number, respectively.\n\nSolutions\nExercise 1\n1. Create a directory for these exercises.\n\nmkdir /fs/ess/PAS1855/users/$USER/week05/exercises/\n\n3. Copy the FASTQ files into your own dir.\n\nmkdir -p data/fastq/\ncp /fs/ess/PAS1855/data/week05/fastq/* data/fastq/\n\nExercise 2\n1. Create a new script with header lines.\n\nThe first two lines of the script:\n#!/bin/bash\nset -u -e -o pipefail\nSave the file as fastq_stat.sh.\n\n2. Assign the first argument to a named variable.\n\nAdd a line like this to your fastq_stat.sh script:\nfastq_file=$1\n\nFor testing the code below, we first assign one of the FASTQ filenames to the variable $fastq_file:\nfastq_file=201-S4-V4-V5_S53_L001_R1_001.fastq\n3. Let the script report its name and the name of the FASTQ file.\n\nAdd lines like these to your fastq_stat.sh script:\necho \"$0: A script to compute basic summary stats for a FASTQ file.\"\necho \"FASTQ file to be analyzed: $fastq_file\"\n\n4. Compute and report the number of sequences in the FASTQ file.\n\nAdd lines like these to your fastq_stat.sh script:\n# We save the output of our commands using command substitution, $().\n# In the wc -l command, use input redirection so the filename is not in the output.\nn_lines=$(wc -l < \"$fastq_file\")\nn_seqs=$(expr \"$n_lines\" / 4)     # Use expr for arithmetics\n\necho \"Number of sequences: $n_seqs\"\nAlternatively, you can use the (( )) syntax for arithmetics – just take care that in this case, there can be no spaces between the mathematical operator and the numbers:\nn_seqs=$((\"$n_lines\"/4))\nTo get the number of sequences, you can also count the number of lines that only have a + symbol:\nn_seqs=$(grep -c \"^+$\" $fastq_file)\nRecall that + is also a regular expression symbol, but only so in the extended regex set. Therefore, without the -E flag to grep, we are matching a literal + when we use one in our expression.\n5. Search for adapter sequences.\n\nAdd lines like these to your fastq_stat.sh script:\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelse\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\nTo initiate the if statement:\nWe redirect grep’s output to /dev/null, since we’re only interested in the exit status.\nIf grep finds a match, this evaluates to “true”, and the next code block will be executed. If no match is found, the block after else will be executed.\nInside the if statement:\nWe use the grep’s -E option to search for either of the two primer sequences at once with |.\nWe use character classes like [CT] in place of each ambiguity code.\nWe use grep’s -c option to count the matches.\nOr, to explicitly check the file contains _R2 in its name, rather than assuming this must be the case if it doesn’t contain _R1, you can use elif (short for “else-if”) to add another test:\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelif echo \"$fastq_file\" | grep \"_R2_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\nWhen we test this code by itself, we get:\n#> FASTQ file contains forward (R1) reads, checking for primer 1...\n#> Number of forward primer sequences found: 44687\nThis looks good!\n\n6. Bonus: Print a count table of sequence lengths.\n\nAdd lines like these to your fastq_stat.sh script:\necho \"Count table of sequence lengths:\"\nawk '{ if(NR%4 == 2) print length($0) }' \"$fastq_file\" | sort | uniq -c\nTo select only actual sequences, and not other lines, from the FASTQ file, we use the following trick. We know that in the FASTQ file, every fourth line, starting from line number 2, contains the actual sequence (i.e.: line 2, line 6, line 10, etc). To select these lines we can use the “modulo” operator to select only line numbers (NR in awk) for which, after dividing the line number by 4, we have 2 left: NR%4 == 2.\nNext we print the number of characters on the entire line using awk’s length function: print length($0).\nFinally, we pass the sequence lengths on to the sort | uniq -c idiom, which will give us a count table.\n7. Make the script executable.\n\nAssuming it is in the working dir:\n$ chmod u+x fastq_stat.sh \n\n# Or for all scripts at once:\n$ chmod u+x *sh\n\n8. Run the script for a single FASTQ file by calling it from the command line.\n\n# Assuming you have assigned a FASTQ file to $fastq_file for testing:\n./fastq_stat.sh $fastq_file\n\nExercise 3\nChange the script to let it accept a list of FASTQ file names as an argument.\n\nAdd this line to the script:\nfile_list=\"$1\"\nNow, replace the following line:\n# Old line:\n# sample_files=($(cut -f 3 \"$sample_info\"))\n\n# New line:\nsample_files=($(cat \"$file_list\"))\n\nAdd the output directory as a second argument to the script, and create the output dir if necessary.\n\noutput_dir=\"$2\"\n\n# Create the output dir, if necessary:\nmkdir -p \"$output_dir\"\nmkdir -p will not complain if the directory already exists, and it can make multiple levels of directories at once.\n3. Modify the line in the script that calls fastq_stat to call your script.\n\n# Old line:\n# fastq_stat \"$fastq_file\" > stats/$results_file\n\n# New line:\nscripts/fastq_stat.sh \"$fastq_file\" > \"$output_dir\"/\"$results_file\"\nThe results_file line can remain the same:\nresults_file=\"$(basename $fastq_file .fastq)-stats.txt\"\n\n4. In each iteration of the loop, let the script report which FASTQ will be analyzed.\n\nAdd the following line inside the loop:\necho \"Running fastq_stat for FASTQ file $fastq_file\"\n\n5. Create a second file/script to create a list of FASTQ files and to run the loop script.\n\nTo create a list of FASTQ files:\nfile_list=fastq_file_list.txt\n\nls data/*fastq >\"$file_list\"\nTo run the loop script:\noutput_dir=results/fastq_stats\n\n./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\n6. Make fastq_stat_loop.sh executable and run it.\n\nRun these lines:\n$ chmod u+x ./fastq_stat_loop.sh\n\n$ ./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\nThe final fastq_stat_loop.sh script.\n\n#!/bin/bash\nset -e -u -o pipefail\n\nfile_list=\"$1\"\noutput_dir=\"$2\"\n\n# Create the output dir, if necessary:\nmkdir -p \"$output_dir\"\n\n# Create an array with FASTQ files\nfastq_files=($(cat \"$file_list\"))\n\n# Report:\necho \"Number of fastq files: ${#fastq_files[@]}\"\n\n# Loop through the array:\nfor fastq_file in \"${fastq_files[@]}\"; do\n\n  echo \"Running fastq_stat for FASTQ file $fastq_file\"\n\n  # Strip .fastq from each FASTQ file, and add suffix:\n  results_file=\"$(basename \"$fastq_file\" .fastq)-stats.txt\"\n\n  # Run \"fastq_stat\" on a file:\n  scripts/fastq_stat.sh \"$fastq_file\" >\"$output_dir\"/\"$results_file\"\n\ndone\n\nThe final lines in the runner script / Markdown code block.\n\nfile_list=fastq_file_list.txt\noutput_dir=results/fastq_stats\n\nls data/*fastq >\"$file_list\"\n\n./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\nExercise 4\n1. Check whether the FASTQ file is a regular file than can be read.\n\nAdd these lines to your script:\n! -f will be true if the file is not a regular/existing file\n! -r will be true if the file is not readable\nWe use || to separate the two conditions with a logical or.\nWith exit 1, we terminate the script with an exit code that indicates failure.\n\nif [ ! -f \"$fastq_file\" ] || [ ! -r \"$fastq_file\" ]; then\n  echo \"Error: can't open file\"\n  echo \"Second argument should be a readable file\"\n  echo \"You provided: $fastq_file\"\n  exit 1\nfi\nTo test your test:\n$ ./fastq_stat.sh blabla \n\n2. Check whether only one argument was provided.\n\nAdd these lines to your script:\n$# is the number of command-line arguments passed to the script\n-eq will test whether the numbers to the left and right of it are the same, and will return true if they are.\nWe negate this with !: if the number of arguments is NOT 1, then error out.\nexit 1 will exit with exit code 1, which signifies an error / failure.\n\nif [ ! \"$#\" -eq 1 ]; then   # If the number of args does NOT equal 1, then\n  echo \"Error: wrong number of arguments\"\n  echo \"You provided $# arguments, while 1 is required.\"\n  echo \"Usage: fastq_stat.sh <file-name>\"\n  exit 1\nfi\nTo test your test:\n$ ./fastq_stat.sh                      # No args\n$ ./fastq_stat.sh $fastq_file blabla   # Two args\n\n3. Add date commands.\n\nSimply include two lines with:\ndate\n… in the script, one before file processing, and one after.\n\nThe final fastq_stat.sh script.\n\n#!/bin/bash\nset -u -e -o pipefail\n\necho \"$0: A script to compute basic summary stats for a FASTQ file.\"\ndate\necho\n\n# Test number of args -------------------------------------------------\nif [ ! \"$#\" -eq 1 ]; then\n  echo \"Error: wrong number of arguments\"\n  echo \"You provided $# arguments, while 1 is required.\"\n  echo \"Usage: fastq_stat.sh <file-name>\"\n  exit 1\nfi\n\n# Process command-line args and report ------------------------------------\n\nfastq_file=\"$1\"\n\necho \"FASTQ file to be analyzed: $fastq_file\"\necho\n\nif [ ! -f \"$fastq_file\" ] || [ ! -r \"$fastq_file\" ]; then\n  echo \"Error: can't open file\"\n  echo \"Second argument should be a readable file\"\n  echo \"You provided: $fastq_file\"\n  exit 1\nfi\n\n# Count primer sequences ------------------------------------------------\n\nn_lines=$(wc -l <\"$fastq_file\")\nn_seqs=$(expr \"$n_lines\" / 4)\n\necho \"Number of sequences: $n_seqs\"\n\n# Count primer sequences ------------------------------------------------\n\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelif echo \"$fastq_file\" | grep \"_R2_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\n\n# Bonus: Count table of sequence lengths ---------------------------------\necho \"Count table of sequence lengths:\"\nawk '{ if(NR%4 == 2) print length($0) }' \"$fastq_file\" | sort | uniq -c\n\n# Report ----------------------------------------------------------------\necho\necho \"Done with $0 for $fastq_file.\"\ndate\n\nExercise 5\nSolution\n\nThere are several ways to do this, here’s one example:\n#!/bin/bash\nset -u -e -o pipefail\n\ndir=\"$1\"\nextension=\"$2\"\n\nwc -l \"$dir\"/*.\"$extension\" | sort -rn | sed -n '2p'\nYou have to print the second rather than the the first line: the first line will be a total line number count across all files, which wc automatically computes.\nInstead of using sed, you could also use head -n 2 | tail -n 1 to print the second line:\nwc -l \"$dir\"/*.\"$extension\" | sort -rn | head -n 2 | tail -n 1\n(Note also that there are two files with the same number of lines: R1 and R2 files for the same sample always have the same number of reads.)\n\nExercise 6\n1. Write a script that takes one file and determines the number of rows and columns.\n\n#!/bin/bash\nset -u -e -o pipefail\n\nfile=\"$1\"\n\necho \"Filename:\"\necho \"$file\"\n\n# We can get the number of rows simply by counting the number of lines:\n# To avoid printing the filename we redirect the input like below\n# (Or we could have done: \"cat ../data/Saavedra2013/n10.txt | wc -l\")\necho \"Number of rows:\"\nwc -l < \"$file\"\n\n# To count the number of columns, we use awk - recall that NF is the number\n# of fields (columns), and to print that number only for a single line, we exit:\necho \"Number of columns:\"\nawk '{ print NF; exit }' \"$file\"\n\n# head -n 1 \"$file\" | awk '{ print NF }' # Also works\nWe can save this script as netsize.sh and make it executable using chmod u+x netsize.sh.\n\n2. Write a script that prints the number of rows and columns for each network.\n\n#!/bin/bash\nset -u -e -o pipefail\n\ndir=\"$1\"\n\n# We can loop over the files using globbing:\nfor file in \"$dir\"/*.txt; do\n\n    # Next, we can save the number of rows and columns in variables:\n    n_row=$(wc -l < \"$file\")\n    n_col=$(awk '{ print NF; exit }' \"$file\")\n    \n    # And print them all on one line:\n    echo -e \"$file \\t $n_row \\t $n_col\"\ndone\nWe can save this script as netsize_all.sh and run it as follows:\n./netsize_all.sh ../data/Saavedra2013\n\n3. Which network has the largest number of rows and which the largest number of columns?\n\n# Having written the script netsize_all.sh,\n# you can take its output and order it according to rows or columns.\n\n# Sorting by column 2 gives you the file with the largest number of rows:\n$ ./netsize_all.sh | sort -n -r -k 2 | head -n 1\n#> ../data/Saavedra2013/n58.txt 678 90\n\n# Sorting by column 3 gives you the file with the largest number of columns:\n\n$ ./netsize_all.sh | sort -n -r -k 3 | head -n 1\n#>  ../data/Saavedra2013/n56.txt 110 207\n\nExercise 7\nSolution\n\n#!/bin/bash\n\nset -u -e -o pipefail\n\nfile=$1    # $1 is the file name\ncolumn=$2  # $2 is the column of interest\n\necho \"Column name:\"\ncut -d ',' -f \"$column\" \"$file\" | head -n 1\n\n# In the next lines, we need to skip the header, which we can do using\n# tail -n +2\necho \"Number of distinct values:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort | uniq | wc -l\n\necho \"Minimum value:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort -n | head -n 1\n\necho \"Maximum value:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort -n | tail -n 1\nIf we save the script as explore.sh, and make it executable, we can run it using:\n./explore.sh ../data/Buzzard2015_data.csv 6\n\n# Column name\n# Abund.n\n# Number of distinct values:\n# 46\n# Minimum value:\n# 1\n# Maximum value:\n# 157\n\n# This works well also for alphabetical order:\n\n./explore.sh ../data/Buzzard2015_data.csv 3\n\n# Column name\n# genus\n# Number of distinct values:\n# 85\n# Minimum value:\n# Acacia\n# Maximum value:\n# Zanthoxylum\n\nThere initially was an error in these primer sequences (one sequence repeated twice), which has been corrected on Friday, Feb 12.↩︎\nThis exercise was slightly modified from Software Carpentry’s Shell Novice tutorial.↩︎\n",
      "last_modified": "2021-03-16T14:33:07-04:00"
    },
    {
      "path": "w06_GA_scripts.html",
      "title": "Graded Assignment II: Shell scripts at OSC",
      "author": [],
      "contents": "\n\nContents\nIntroduction\nGeneral instructions\nGrading information\n\nGetting set up\nCutadapt script for one sample\nRunning the script and finishing up\nOptional (ungraded) - Loop over all samples\n\n\nIntroduction\nIf you did last week’s exercises, much of the following introduction will be familiar to you, as we will be working with the same FASTQ files and looking at the same primers. This time around, you will actually remove the primer sequences using the software Cutadapt.\nThis assignment will work with 6 FASTQ files with sequences from the V4 region of 16S rRNA, generated in a metabarcoding experiment.\nThe FASTQ files come in pairs: for every sample, there is a FASTQ file with forward reads (or “read 1” reads) that contains _R1_ in its file name, and a FASTQ file with corresponding reverse reads (or “read 2” reads) that contains _R2_ in its file name. So, our 6 FASTQ files consist of 3 pairs of R1-R2 files for 3 biological samples.\nThe sequences were generated by first amplifying environmental samples with a pair of universal 16S primers, and these primer sequences are expected to be present in the FASTQ sequences. You will remove these primer sequences with the program Cutadapt, and there are two things to be aware of:\nA primer can also be present in the FASTQ sequence as its reverse complement, so we will search for reverse complements too.\nThe primers contain a few variable sites for which ambiguity codes are used. For instance, an R means that the site can be either an A or a G, and an N means that the site can be any of the four bases. See here for a complete overview of these ambiguity codes.\nThese are the primer sequences1:\nForward primer (“515F”): GAGTGYCAGCMGCCGCGGTAA.\nReverse primer (“806R”): ACGGACTACNVGGGTWTCTAAT.\nGeneral instructions\nFor each numbered step below, you should create at least one Git commit.\nAll files should be added to your repository, unless mentioned otherwise.\nGrading information\nThe number of points (if any) that you can earn for each step are denoted between square brackets (e.g. [0.5]). In total, you can earn 10 points with this assignment, which is 10% of your grade for the class.\n\nGetting set up\nCreate a new directory for this assignment, and inside it, initialize a Git repository. Add a very brief README.md describing that this is a repository for such-and-such assignment. (You can also use this README to further document your workflow, but you don’t have to.) [0.5]\nCopy the FASTQ files from /fs/ess/PAS1855/data/week05/fastq into a directory data/fastq/ inside your assignment’s directory. [0.5]\nCreate a .gitignore file and add a line to make Git ignore all .fastq files. [0.5]\nLoad the Conda module at OSC and create a Conda environment for Cutadapt following these instructions (i.e. just the section “Installation with Conda”). [1]\nHints\n\nTo be able to install Cutadapt with the command provided in that link, you first need to do the following general Conda setup steps, which we will (have) do(ne) together in class:\n$ conda config --add channels defaults\n$ conda config --add channels bioconda\n$ conda config --add channels conda-forge \n(Adding the above lines in that order will result in the different channels having the proper priority, with conda-forge having the highest priority.)\nIf you run into installation problems that you can’t seem to solve, you can can also use my Conda environment in Step 7, below, as follows:\n$ source activate users/PAS0471/jelmer/.conda/envs/cutadaptenv\nBut do this only if needed, and if you do, include a description of your errors in your README file.\n\n\nExport the environment description for your Cutadapt environment to a YAML (.yml) file. [0.5]\n\nCutadapt script for one sample\nNow, you will write a script called cutadapt_single.sh that runs Cutadapt for one pair of FASTQ files: a file with forward (R1) reads and a file with reverse (R2) reads for the same sample.\nThe following instructions all refer to what you should write inside the script:\nStart with the shebang line followed by SLURM directives. Specify at least the following SLURM directives [0.5]\nThe class’s OSC project number, PAS1855.\nA 20-minute wall-time limit.\nExplicitly ask for one node, one process (task), and one core (these are three separate directives).\n\nNext, include the familiar set settings for robust bash scripts, load OSC’s Conda module and then activate your own Cutadapt Conda environment. [0.5]\nHints Use source activate and not conda activate to activate the Conda environment, otherwise Conda will complain about your shell not being properly set up.\n\nLet the script take 4 arguments that can be passed to it on the command-line: [1]\nThe path to a FASTQ file with forward reads (whose value, when passed to the script from the shell will e.g. be data/fastq/201-S4-V4-V5_S53_L001_R1_001.fastq).\nThe name of the output directory for trimmed FASTQ files (whose value, when passed to the script from the shell will be whatever you pick, e.g. results/trim).\nThe sequence of the forward primer (whose value when passed to the script from the shell, will be GAGTGYCAGCMGCCGCGGTAA in this case).\nThe sequence of the reverse primer (whose value when passed to the script from the shell, will be TTACCGCGGCKGCTGRCACTC in this case).\nGive each of these variables a descriptive name rather than directly using the placeholder variables ($1, etc) – that will make your life easier when writing the rest of the script.\nCompute the reverse complement for each primer. The hint below in fact has the answer, but please take a moment to think how you might do this with tr and a new command called rev that simply reverses a string.\nHints\n\nAdjust your variable names as necessary!\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBVDH TAGCRYMKVBHD | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBVDH TAGCRYMKVBHD | rev)\nNote that we needed to not only translate ACTG but also all IUPAC ambiguity codes.\n\n\nFrom the file name of the input FASTQ file with forward reads (which is one of the arguments to the script), infer the name of the corresponding FASTQ file with reverse reads, which will have an identical name except that _R1_ is replaced by _R2_. [0.5]\nAssign output file paths (output dir + file name) for the R1 and R2 output file, inserting _trimmed before the .fastq file extension (that is, the output file paths should be along the lines of <output-dir>/<old-file-basename>_trimmed.fastq). [1]\nHints\n\nYou’ll do yourself a favor by having the script echo the file names that you have assigned, so you can easily check if you’re doing this right.\nMoreover, I recommend that you test interactively whether you’re getting all the file names right: assign one of the paths to the actual FASTQ files to the variable name you’re using for that, and ditto with an output dir. Just take care that these assignments don’t end up in your (final) script.\n\n\nCreate the output directory if it doesn’t already exist. [0.5]\nThe actual call to the Cutadapt program should be as follows – just change any variable names as needed:\n$ cutadapt -a \"$primer_f\"...\"$primer_r_revcomp\" \\\n    -A \"$primer_r\"...\"$primer_f_revcomp\" \\\n    --discard-untrimmed --pair-filter=any \\\n    -o \"$R1_out\" -p \"$R2_out\" \"$R1_in\" \"$R2_in\"\nOptional (ungraded): Touch up the scripts with additional echo statements, date commands, and tests such as whether 4 arguments were provided.\n\nRunning the script and finishing up\nSubmit the script as a SLURM job for one pair of FASTQ files – don’t forget to provide it with the appropriate arguments. Check the SLURM log file and the output files. If it didn’t work, troubleshoot until you get it working. [2]\nDo any necessary cleaning up of files, e.g. move your SLURM log file to an appropriate place, and make sure everything is committed to the Git repository. (I’ll need to see the SLURM log file in your repository to see if the script worked.) [0.5]\nCreate a GitHub repository and push your local Git repository to GitHub. Like last time, start an issue and in the issue, tag @jelmerp. [0.5]\n\nOptional (ungraded) - Loop over all samples\nCreating a script like we did above is worth the trouble mostly if we plan to run it for multiple/many samples. Now, you will create a second script cutadapt_submit.sh that loops over all FASTQ files in a specified directory. It doesn’t need to be a “proper” script with a robust header and so on, and shouldn’t contain any SLURM directives: this script merely functions to submit SLURM jobs and can be run interactively.\nLoop over a globbing pattern that accepts all .fastq files with R1 in the name in the input directory (recall: we don’t want to loop over the R2 files explicitly, because they will be automatically included in our previous script).\nInside the loop, the cutadapt_single.sh script should be submitted as a SLURM job, similar to your single submission of the script above.\nRun the loop.\nCheck the SLURM log files and the output directory. If it didn’t work, remove all these files, troubleshoot, and try again until it is working.\n\nInitially, there was an error in the primer sequences provided in last week’s exercises, which has been corrected on Friday, Feb 12.↩︎\n",
      "last_modified": "2021-03-16T14:33:08-04:00"
    },
    {
      "path": "w06_UA_ssh.html",
      "title": "Optional Ungraded Assignment: SSH setup",
      "author": [],
      "contents": "\n\nContents\nIntroduction\n1. Avoid being prompted for password\n2. Use a shortcut for your SSH connection\n3. Set up your local VS Code to SSH tunnel into OSC\n\n\nIntroduction\nYou can connect to OSC with SSH if you have a bash shell on your machine, either natively with a Mac or Linux machine, or via WSL, Git Bash, or another application on Windows.\nThe regular way to do this is typing ssh <username>@pitzer.osc.edu and then providing your password, which can get tedious. There are two setup steps you can take to make this quicker:\nAvoid being prompted for your password.\nSet up a shortcut for your SSH connection name.\nA third thing you can do below, assuming you have VS Code installed locally, is to get it to SSH tunnel to OSC: this will be as if you have VS Code open in OnDemand, but then not in your browser!\n1. Avoid being prompted for password\nThese steps are similar to what you did for your SSH Github authentication.\nOn your own computer, generate a public-private SSH key-pair:\n$ ssh-keygen -t rsa\nWhen you’re prompted for a passphrase, you can just press enter if you are okay with not having a passphrase.\n\nIf you do want a passphrase, you have to take an extra step to not be prompted, after step 3 below:\n$ ssh-add\n# (Then here, don't enter a passphrase in any case!)\n\nFrom your own computer, transfer the public key to the remote computer:\n# Replace <user> by your username, e.g. \"jelmer@owens.osc.edu\"\n$ cat ~/.ssh/id_rsa.pub | ssh <user>@owens.osc.edu 'mkdir -p .ssh && cat >> .ssh/authorized_keys'\nLog in to the remote computer (OSC) and once there, set appropriate permissions:\n$ chmod 700 .ssh; chmod 640 .ssh/authorized_keys\n\nSee also this Tecmint post in case you’re struggling, and Buffalo Chapter 4 page 59-60.\n\n\n2. Use a shortcut for your SSH connection\nThese two steps should both be done on your local machine.\nCreate a file called ~/.ssh/config:\n$ touch ~/.ssh/config\nOpen the file in a text editor and add your alias(es) in the following format:\nHost <arbitrary-alias-name>    \n     HostName <remote-name>\n     User <user-name>\nFor instance, I have something along these lines for Pitzer and Owens:\nHost op\n    HostName pitzer.osc.edu\n    User jelmer\n\nHost oo\n    HostName owens.osc.edu\n    User jelmer\nNow, you just need to use your, preferably very short, alias to log in:\n$ ssh op\nThis shortcut will also work with scp and rsync!\n$ rsync ~/scripts op:/fs/ess/PAS1855/scripts\n\n3. Set up your local VS Code to SSH tunnel into OSC\nIf you want to use VS Code to write code, have a shell, and interact with files at OSC directly, you don’t need to use the VS Code Server from OSC OnDemand. You can also “SSH tunnel” in with your local installation. This is a more convenient way of working because it’s quicker to start up and you are not working inside a browser (which, among other things, takes away screen space and interferes with some keyboard shortcuts).\nThe set up is pretty simple (see also these instructions if you get stuck), just recall to do this in your local installation of VS Code, assuming you have one.\nInstall the VS Code “Remote Development extension pack”, for instance by opening up the Extensions side bar, searching for it there, and clicking “Install”.\nOpen up the Command Palette (F1 or Ctrl+ShiftP) and start typing “Remote SSH” – select Remote-SSH: Connect to Host… and then specify your SSH connection. If I remember correctly, you can use whatever SSH shortcut you just set up above, otherwise, type it in full (e.g. jelmer@pitzer.osc.edu).\nWith the above setup, you shouldn’t be prompted for a password and VS Code will connect to OSC! Now, if you open a shell or try to open a file, you are interacting with OSC and not you local computer.\n\n\n\n\n",
      "last_modified": "2021-03-16T14:33:08-04:00"
    },
    {
      "path": "w07_exercises.html",
      "title": "Exercises: Week 7",
      "author": [],
      "contents": "\n\nContents\nExercises\nExercise 1: Download genome assembly files\nExercise 2: Checking out the downloaded files\nExercise 3: File transfer\n\nSolutions\nExercise 1\nExercise 2\nExercise 3\n\n\nExercises\nYou can do exercises 1 and 2 in a shell either at OSC or on your local computer: whichever you prefer. In exercise 3, you will be transferring files to or from OSC, which should be done from a local shell.\nExercise 1: Download genome assembly files\nIn this exercise, you’ll download a set of files associated with a genome assembly of one organism from NCBI. As an example, I’m using the genome assembly files for Phytophthora nicotianae var. parasitica (also known simply as P. parasitica), one of several species of fungus that causes buckeye rot of tomato.\nNote, if you are more interested in getting the equivalent files for another organism, you should be able to follow the same steps.\nGo to https://www.ncbi.nlm.nih.gov. NCBI has a number of different databases, including “SRA” for short next-generation sequencing reads, and “PubMed” for publications. In the drop-down menu that says All Databases next to the Search box, select Assembly and type Phytophthora parasitica: we want to search for whole-genome assemblies for this species.\nClick the assembly in the prominent box at the top that says Phyt_para_CJ02B3_V1.\nHints\n\nHere is the link to the assembly, in case things look different for you: https://www.ncbi.nlm.nih.gov/assembly/GCA_000509465.1. (But again, you can also download files from a different organism or assembly.)\n\n\nNow, you should be on the assembly page. There is a big blue button Download Assembly, but if you click on it, you can see that you can only download one file at a time. Since we want all 20 or so files associated with the assembly, we will use wget instead. Note that this would scale easily even if we wanted files for multiple assemblies (for instance, there are 9 different assemblies for P. parasitica, as the page reports).\nClick on FTP directory for GenBank assembly in the top list on the right hand side.\nUse a wget command to download all the files you see on the assembly page, using the URL in the address bar.\nIn addition to the option for downloading multiple files at once (and optionally, other options to optimize the download), make sure you use the following options:\n--no-parent — Stop wget from moving up via the Parent Directory link at the top of the page, which would lead it to start downloading files from other assemblies…\n-e robots=off — Ignore robots.txt to enable the download (as specified in NCBI’s download FAQ).\n\nIf you see errors saying that filenames are too long, you’ll have to use the --no-host-directories and --cut-dirs=6 options as further detailed in the Hints, because all the nested directories that are being created may end up creating paths that are too long.\n\nHints\n\nYour command should be structured as follows:\n$ wget --no-parent -e robots=off <other-options> <link-to-page>\nThe link to the page is: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/509/465/GCA_000509465.1_Phyt_para_CJ02B3_V1/\nUse the --recursive option to download more than one file at once.\nBy default, the files will be placed inside several levels of directories:\n# ftp.ncbi.nlm.nih.gov/\n# └── genomes\n#     └── genbank\n#         └── protozoa\n#             └── Phytophthora_parasitica\n#                 └── latest_assembly_versions\n#                     └── GCA_000509525.1_Phyt_para_IAC_01_95_V1\nTo avoid this, you can use the options --no-host-directories and --cut-dirs=6 as suggested in the NCBI’s download FAQ. And if you do so, you’ll also want to specify an arbitrary target dir name with -P, e.g. -P ncbi_Pparasiticus.\nIf you want to play around with different options, I suggest you use the --accept option to only select one or two small files to be downloaded, e.g. --accept \"*assembly_stats*\".\nDon’t hesitate to look at the solution if your first attempts fail. The minutiae of wget commands aren’t terribly important – the next exercise, while easier, is more so.\n\nExercise 2: Checking out the downloaded files\nMove inside the directory that you’ve downloaded where all the assembly files are. You should be seeing over 20 files, including GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_report.txt and GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz.\nOne of the files you’ve downloaded has md5 checksums for all the other files: md5checksums.txt. Use this file in a single md5sum command to verify that all the downloaded files are identical to the source, i.e. that they have the correct checksums.\nHints\n\nUse the -c (check) option to md5sum to cross-check the checksums of the files in your directory with those in the file you specify.\n\n\nCount the number of lines in the genome sequence (GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.fna.gz) that contain CAGCAGCAG, without unzipping the file.\n(Side note in case you are wondering about the fna extension: this explicitly denotes a FASTA file with nucleotide sequences, as opposed to one with amino acid sequences, .faa.)\nHints\n\nzgrep will allow you to search in a gzipped file.\nRecall that (z)grep’s -c option will count matches.\n\nCheck the size of the same zipped genome sequence FASTA file. Then, unzip the file, and check the file size again. Finally, gzip it back up.\nHints\n\nTo unzip, use gunzip <filename>.\nTo zip, use gzip <filename>.\nCheck the file size with ls or du.\n\nDoes the checksum still match for the “rezipped” FASTA file?\nHints\n\nYou can simply use the same command you used in the first step of this exercise.\n\n\nExercise 3: File transfer\nTransfer the entire directory you downloaded above to OSC (in case you have been working locally so far) or to your local computer (in case you have been working at OSC so far).\nYou can use rsync and/or SFTP, whichever you prefer to get some practice with.\nEdit Fri, Feb 26: We did not end up discussing SFTP in class. But if you are interested in getting experience with this (at OSC, it is recommended for larger transfers since the transfer does not use a login node like scp andrsync do), have a look at this tutorial. The hint below will tell you how you can log in to the OSC SFTP server.\nHints\nrsync\nRecall that using a trailing slash in the source dir will make a difference. In this case (and usually) you will most likely not want to include a trailing slash.\nsftp\nLog in using:\nsftp sftp.osc.edu\nLike many commands, the put and get functions need the -r option to work recursively.\nIn sftp, the ~ shortcut for your home dir does not work.\n\nSolutions\nExercise 1\n4. Use a wget command to download all these files at once.\nThe wget command with the fewest options that works for our goal is:\n$ wget --recursive -e robots=off --no-parent \\\n    https://ftp.ncbi.nlm.nih.gov/genomes/genbank/protozoa/Phytophthora_parasitica/latest_assembly_versions/GCA_000509525.1_Phyt_para_IAC_01_95_V1/\nThese options (--recursive -e robots=off --no-parent) were all discussed in the question and the hints.\nIf you wanted to place these files in a dir with a name of your choice, and no unnecessarily nest directories (see the hint above), you could use:\n--no-host-directories in combination with --cut-dirs=6 will remove the 6 levels of nested directories you got earlier (ftp.ncbi.nlm.nih.gov/genomes/genbank/protozoa/Phytophthora_parasitica/latest_assembly_versions).\n-P ncbi_Pparasiticus/ to name your directory.\nFurthermore, since we are not interested in index.html (the actual HTML we are looking at on the web), we can exclude it using --reject \"index.html\".\nAll in all, this would result in the following command:\n$ wget --recursive -e robots=off --no-parent \\\n       --reject \"index.html\" \\\n       --no-host-directories --cut-dirs=6 \\\n       -P ncbi_Pparasiticus/ \\\n       https://ftp.ncbi.nlm.nih.gov/genomes/genbank/protozoa/Phytophthora_parasitica/latest_assembly_versions/GCA_000509525.1_Phyt_para_IAC_01_95_V1/ \nThis should result in the following set of files being downloaded (one is directory, in fact):\n$ ls ncbi_Pparasiticus # Or whatever dir your downloads are in\n#> annotation_hashes.txt\n#> assembly_status.txt\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_report.txt\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_stats.txt\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_feature_count.txt.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_feature_table.txt.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.fna.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic_gaps.txt.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gbff.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gff.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gtf.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_protein.faa.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_protein.gpff.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_rm.out.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_rm.run\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_rna_from_genomic.fna.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_translated_cds.faa.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_wgsmaster.gbff.gz\n#> index.html.tmp\n#> md5checksums.txt\n#> README.txt\n\nExercise 2\n1. Check the file integrity with md5sum.\nThe -c option, when provided with a filename, will check the checksums of the files in your current directory against those in the file:\n$ md5sum -c md5checksums.txt\n#> ./annotation_hashes.txt: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_report.txt: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_stats.txt: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure/Primary_Assembly/component_localID2acc: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure/Primary_Assembly/scaffold_localID2acc: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure/Primary_Assembly/unplaced_scaffolds/AGP/unplaced.scaf.agp.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure/Primary_Assembly/unplaced_scaffolds/FASTA/unplaced.scaf.fna.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_feature_count.txt.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_feature_table.txt.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.fna.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic_gaps.txt.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gbff.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gff.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gtf.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_protein.faa.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_protein.gpff.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_rm.out.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_rm.run: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_rna_from_genomic.fna.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_translated_cds.faa.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_wgsmaster.gbff.gz: OK\nIf all went well, all files should say OK.\nIf there was a mismatch somewhere, you should also see a separate warning at the end of the output, like:\n#> md5sum: WARNING: 1 computed checksum did NOT match\n\n2. Count the number of lines in the genome sequence that contain CAGCAGCAG.\n$ zgrep -c \"CAGCAGCAG\" GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n#> 1426\n\n3. Check file size, unzip and zip.\n$ ls -lh GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n#> -rw-rw-r-- 1 jelmer jelmer 8.8M Dec 21  2017 GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n\n$ gunzip GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n\n$ ls -lh GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna\n#> -rw-rw-r-- 1 jelmer jelmer 38M Dec 21  2017 GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna\n\n$ gzip GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna\nThe zipped file is less than a quarter of the size of the unzipped file.\n\n4. Does the checksum still match for the “rezipped” FASTA file?\n$ md5sum -c md5checksums.txt\n#> ... (showing the pertinent part of the output)\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz: FAILED\n#> ... \n#> md5sum: WARNING: 1 computed checksum did NOT match\nNo, it no longer matched! This is merely due to the fact that the gzipped file contains metadata about when it was zipped.\nHowever, this is another reason not to modify any original files. If you needed an unzipped file, it would be better to unzip it to a separate file using the -c option:\n$ gunzip -c GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz > \\\n      GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna\n\nExercise 3\nTransfer with rsync\nRegardless of the direction of the transfer, these commands should be executed in a local shell.\nFrom local to OSC, assuming that your files were downloaded in a dir called ncbi_Pparasiticus in your home dir, your username is me, and you made a dir week07/exercises/ for these exercises at OSC (note: rsync will not create “missing” directories!):\n$ rsync -avz --progress \\\n      ~/ncbi_Pparasiticus \\\n      me@pitzer.osc.edu:/fs/ess/PAS1855/users/me/week07/exercises/\nThe above will result in you having an OSC dir /fs/ess/PAS1855/users/me/week07/exercises/ncbi_Pparasiticus. If you would have included a trailing slash in ~/ncbi_Pparasiticus above, you would have copied the contents of ncbi_Pparasiticus directly into the exercises dir, which is probably not what you wanted.\nFrom OSC to local, assuming that your files were downloaded in a dir called ncbi_Pparasiticus in /fs/ess/PAS1855/users/me/week07/exercises/ your username is me, and you copy the downloaded dir straight into your home dir:\n$ rsync -avz --progress \\\n      me@pitzer.osc.edu:/fs/ess/PAS1855/users/me/week07/exercises/ncbi_Pparasiticus \\\n      ~\nThe above will result in you having a local dir ~/ncbi_Pparasiticus. If you would have included a trailing slash in ~/ncbi_Pparasiticus above, you would have copied the contents of ncbi_Pparasiticus directly into your home dir, which is probably not what you wanted.\nTransfer with SFTP\nFrom local to OSC:\nAssuming that your local home ($HOME / ~) directory is /home/me, and your OSC username is me:\n$ sftp sftp.osc.edu\nsftp> put -r /home/me/ncbi_Pparasiticus/ /fs/ess/PAS1855/users/me/week07/exercises/\nThe above will result in you having an OSC dir /fs/ess/PAS1855/users/me/week07/exercises/ncbi_Pparasiticus.\nFrom OSC to local\nAssuming that your local home ($HOME / ~) directory is /home/me, and your OSC username is me:\n$ sftp sftp.osc.edu\nsftp> get -r /fs/ess/PAS1855/users/me/week07/exercises/ncbi_Pparasiticus/ /home/me/\nThe above will result in you having a local dir ~/ncbi_Pparasiticus.\n\n\n\n",
      "last_modified": "2021-03-16T14:33:09-04:00"
    },
    {
      "path": "w08_exercises.html",
      "title": "Exercises: Week 8",
      "author": [],
      "contents": "\n\nContents\nSetup\nExercise 1: Variable types and strings\nExercise 2: Lists\n\n\n\n\n\n Edit March 4: I have moved part of the exercises on lists and all those on dictionaries and sets to the page with exercises for week 9. Though you are welcome to work ahead to those already, the exercises that remain here cover the material that we ended up discussing in class this week. \nSetup\nOpen a new file and save it as week08_exercises.py or something along those lines.\nType your commands in the script and send them to the prompt in the Python interactive window by pressing Shift+Enter.\nProblems with the keyboard shortcut?\nIf this doesn’t work, check your keyboard shortcut by right-clicking in the script and looking for “Run Selection/Line In Python Interactive Window”.\nAlso, you can open the Command Palette (Ctrl+Shift+P) and look for that shortcut there, and change it if you want.\n\nBecause these exercises have many small steps, I put the solutions right below the question, so you don’t have to scroll back-and-forth all the time. However, make sure you actually try to do the exercises!\n\nExercise 1: Variable types and strings\nPrint the type of the value 4.88.\nSolution\nWe can use the function type():\n\ntype(4.88)\n<class 'float'>\n\nThe result would be the same if we first assigned it as a variable:\n\nnum = 4.88\ntype(num)\n<class 'float'>\n\n\nAssign the variable n_samples = 658, and then extract the third character from n_samples.\nHints\nYou can’t index a number like n_samples[index], so you’ll first have to convert n_samples to a string. Also, recall that Python starts counting from 0!\nSolution\n\nn_samples = 658\nstr(n_samples)[2]\n'8'\n\n\nAssign the string ‘CTTATGGAAT’ to a variable called adapter. Print the number of characters in adapter.\nSolution\n\nadapter = 'CTTATGGAAT'\nlen(adapter)\n10\n\n\nReplace all As by Ns in adapter and assign the resulting string to a new variable. Print the new variable.\nHints\nUse the string method replace(), and recall that methods are called using the <object_name>.<method_name>() syntax.\nSolution\n\nbad_seq = adapter.replace('A', 'N')\nbad_seq\n'CTTNTGGNNT'\n\n\nFind out what the third argument to the replace() method does by using the built-in help.\nHints\nIf you are typing your commands in a script rather than straight in the console, you will get some more information already when typing the opening parenthesis of the method (briefly pause if necessary).\nTo get more help, you can use a notation with a ?, or help(object.method).\nSolution\n\nhelp(adapter.replace)\n# Or: \"adapter.replace?\"\n# Or: \"?adapter.replace\"\nHelp on built-in function replace:\n\nreplace(old, new, count=-1, /) method of builtins.str instance\n    Return a copy with all occurrences of substring old replaced by new.\n\n      count\n        Maximum number of occurrences to replace.\n        -1 (the default value) means replace all occurrences.\n\n    If the optional argument count is given, only the first count occurrences are\n    replaced.\n\nAs it turns out, the third argument, count, determines how many instances of the substring will be replaced.\n\nUsing what you found out in the previous steps, replace just the first two As in adapter by Ns.\nSolution\nWe specify 2 as the third argument, which is the number of instances of the substring that will be replaced:\n\nadapter.replace('A', 'N', 2)\n'CTTNTGGNAT'\n\n\nConvert the following strings and numbers to a Boolean value to see what the resulting Boolean is (True or False): \"False\" (with quotes), 0, 1, -1, \"\", None, and see if you can make sense of these results.\nSolution\n\nbool(\"False\")\nTrue\n\n\nbool(1)\nTrue\n\n\nbool(0)\nFalse\n\n\nbool(-1)\nTrue\n\nAs it turns out, among numbers and strings, only 0 is interpreted as False, whereas anything else is interpreted as True.\n\nbool(\"\")\nFalse\n\n\nbool()\nFalse\n\n\nbool(None)\nFalse\n\nBut an empty string, nothing at all between parenthesis, and None (Python’s keyword to define a null value or the lack of a value), are also interpreted as False.\nNote that as soon as you quote \"None\", it is a string again and will be interpreted as True:\n\nbool(\"None\")\nTrue\n\n\nHave a look at the names of the methods that appear when you type adapter. (note the .). Can you find a method that will print the last occurrence of a T in adapter?\nHints\nThe method rfind will search from the right-hand side (hence r), and will therefore print the last occurrence of the specified substring.\nSolution\n\nadapter.rfind(\"T\")\n9\n\n\nSplit the sequence by GAGTCCCTNNNAGCAACGTTNNTTCGTCATTAN by Ns.\nHints\nUse the split() method for strings.\nSolution\n\nseq = \"GAGTCCCTNNNAGCAACGTTNNTTCGTCATTAN\"\nsplit_seq = seq.split('N')\nsplit_seq\n['GAGTCCCT', '', '', 'AGCAACGTT', '', 'TTCGTCATTA', '']\n\n\n\nExercise 2: Lists\nAssign a list plant_diseases that contains the items fruit_rot, leaf_blight, leaf_spots, stem_blight, canker, wilt, root_knot and root_rot.\nSolution\n\ndiseases = ['fruit_rot', 'leaf_blight', 'leaf_spots', 'stem_blight',\n            'canker', 'wilt', 'root_knot', 'root_rot']\n\n\nExtract stem_blight from diseases by its index (position).\nSolutionstem_blight is the fourth item and because Python starts counting at 0, this is index number 3.\n\ndiseases[3]\n'stem_blight'\n\n\nExtract the first 5 items from diseases.\nHints\nRecall that when using ranges, Python does not include the item corresponding to the last index.\nSolution\nWhile index 5 is the sixth item, it is not included, so we specify 0:5 or :5 to extract elements up to and including the fifth one:\n\ndiseases[0:5]\n['fruit_rot', 'leaf_blight', 'leaf_spots', 'stem_blight', 'canker']\n\nOr:\n\ndiseases[:5]\n['fruit_rot', 'leaf_blight', 'leaf_spots', 'stem_blight', 'canker']\n\n\nExtract the last item from diseases.\nHints\nRecall that you can use negative numbers to start counting from the end. Also, while 0 is the first index, “-0” (or something along those lines) is not the last index.\nSolution\n\ndiseases[-1]\n'root_rot'\n\n\nExtract the last 3 items from diseases.\nSolution\nNote that you’ll have to omit a number after the colon in this case, because [-3:-1] would not include the last number, and [-3:0] does not work either.\n\ndiseases[-3:]\n['wilt', 'root_knot', 'root_rot']\n\n\n",
      "last_modified": "2021-03-16T14:33:11-04:00"
    },
    {
      "path": "w08_python-resources.html",
      "title": "Python resources",
      "author": [],
      "contents": "\n\nContents\nGeneral resources for learning Python\nAvailable online via the OSU library\nFree online resources\nNot free\n\nOther resources\nBest practices\nMiscellaneous\n\nCoding infrastructure\nVS Code\nJupyter Notebooks / JupyterLab\n\n\nGeneral resources for learning Python\nHere is a list of recommended resources for learning Python. Most are geared towards beginners (in Python and programming alike). Also, the bioinformatics-specific books are all quite practical, describing “applied bioinformatics” – i.e. more how to analyze your data than how to write your own algorithms.\nAvailable online via the OSU library\nBook: Python for the life sciences: a gentle introduction to python for life scientists (Alexandar Lancestar, 2019). This book is very explicitly geared towards biologists with no or little programming experience, and takes a very practical and project-oriented approach. From what I’ve seen of the book, I can highly recommended it!\nBook: Python for Bioinformatics (Sebastian Bassi, 2018). This book also starts with an introduction to Python and then has chapters that each describe practical problems/projects for Python, like “Calculating melting temperature from a set of primers”.\n(Associated GitHub repository.)\nBook: Python programming for biology, bioinformatics, and beyond (Tim Stevens, 2015). This book starts with an introduction to Python and then has chapters on topics like “Pairwise sequence alignments”, “Sequence variation and evolution”, and “High-throughput sequences”.\nBook: Reproducible Bioinformatics with Python (Ken Youens-Clark, 2021). This is a slightly more advanced book that does not start with an introduction to Python, but you should be able to follow the book with what you’ll learn over the next couple of weeks in the course.\nFree online resources\nVideos\nA YouTube playlist of Microsoft videos introducing Python\nVideos from the MIT course “Introduction to Computer Science and Programming in Python”\nCourses\nPython for everybody – Includes course materials and lectures, and is also available at Coursera and edX.\nProgramming for Biology – This is the Cold Spring Harbor course that your TA Zach took, and the materials are available online.\nNot free\nBook: Bioinformatics with Python Cookbook (Tiago Antao, 2018).\n\nOther resources\nBest practices\nPEP 8 — the Style Guide for Python Code\nTen simple rules for writing and sharing computational analyses in Jupyter Notebooks – Rule et al. 2019, PLoS Computational Biology\nMiscellaneous\nStack Overflow: “The Incredible Growth of Python” (2017)\nStatista: “Python Remains Most Popular Programming Language” (2020)\nThe Economist: “Python is becoming the world’s most popular coding language” (2018)\nCoding infrastructure\nVS Code\nVS Code documentation on its “Python Interactive Window”.\nVS Code documentation on its Jupyter Notebook implementation.\nJupyter Notebooks / JupyterLab\nYou can also use the Jupyter Notebooks / JupyterLab as an Interactive App at OSC OnDemand. If you’re interested in using this, I would recommend trying JupyterLab which can run Jupyter Notebooks but also regular Python scripts, a shell, and so forth.\n\nTo do so at OSC OnDemand, click on Interactive Apps (top blue bar) and then Jupyter (Owens and Pitzer) near the bottom, and check the box Use JupyterLab instead of Jupyter Notebook?.\n\n6-minute video: How to use JupyterLab.\nThis JupyterLab documentation provides a nice introduction to JupyterLab features (the link goes to documentation for a version close to the one at OSC).\nFor a general introduction to Jupyter Notebooks, see also How to Use Jupyter Notebook in 2020: A Beginner’s Tutorial.\n\n\n\n",
      "last_modified": "2021-03-16T14:33:12-04:00"
    },
    {
      "path": "w09_exercises.html",
      "title": "Exercises: Week 9",
      "author": [],
      "contents": "\n\nContents\nSetup\nExercise 1: Dictionaries\nExercise 2: Sets\nIntro to CSB exercises\nExercise CSB-1: Measles time series\nBonus: Exercise CSB-2: Red queen in fruit flies\nCSB Solutions\n\n\n\n\n\nSetup\nOpen a new file and save it as week09_exercises.py or something along those lines.\nType your commands in the script and send them to the prompt in the Python interactive window by pressing Shift+Enter.\nProblems with the keyboard shortcut?\nIf this doesn’t work, check your keyboard shortcut by right-clicking in the script and looking for “Run Selection/Line In Python Interactive Window”.\nAlso, you can open the Command Palette (Ctrl+Shift+P) and look for that shortcut there, and change it if you want.\n\nBecause these first two exercises have many small steps, I put the solutions right below the question, so you don’t have to scroll back-and-forth all the time. However, make sure you actually try to do the exercises!\nExercise 1: Dictionaries\nCreate and print a dictionary called yield_current with the following items:\n{\"plotA_1\": 12, \"plotA_2\": 18, \"plotA_3\": 2,\n \"plotB_1\": 33, \"plotB_2\": 28, \"plotB_3\": 57}\nSolution\n\nyield_current = {\"plotA_1\": 12, \"plotA_2\": 18, \"plotA_3\": 2,\n                 \"plotB_1\": 33, \"plotB_2\": 28, \"plotB_3\": 57}\n\nyield_current                 \n{'plotA_1': 12, 'plotA_2': 18, 'plotA_3': 2, 'plotB_1': 33, 'plotB_2': 28, 'plotB_3': 57}\n\n\nPrint just the value for key plotA_3.\nSolution\nWe can get the value for a specific key using the <dict>[<key>] notation:\n\nyield_current[\"plotA_3\"]\n2\n\n\nUpdate the value for key plotB_2 to be 31 and check whether this worked.\nSolution\nWe can simply assign a new value using =:\n\nyield_current[\"plotB_2\"] = 31\nyield_current[\"plotB_2\"]\n31\n\n\nCount the number of items (i.e. entries, key-value pairs) in your dictionary.\nHints\nUse the len() function.\nSolution\n\nlen(yield_current)\n6\n\n\nBonus: Create a dictionary obs_20210305 with keys plotA_3 and plotC_1, and values 18 and 3, respectively. Then, update the yield_current dictionary with the obs_20210305 dictionary, and check whether this worked.\nSolution\n\nobs_20210305 = {\"plotA_3\": 18, \"plotC_1\": 3}\n\nWe use the update() method as follows:\n\nyield_current.update(obs_20210305)\n\nyield_current\n{'plotA_1': 12, 'plotA_2': 18, 'plotA_3': 18, 'plotB_1': 33, 'plotB_2': 31, 'plotB_3': 57, 'plotC_1': 3}\n\nNow, our dictionary has an updated value for key “plotA_3”, and an entirely new item with key “plotC_1”.\n\nBonus: Get and count the number of unique values in your dictionary.\nHints\nExtract the values with the values() method. Next, turn these values into a set to get the unique values. Finally, count the unique values with the len() function.\nSolution\n\nlen(set(yield_current.values()))\n6\n\n\n\nExercise 2: Sets\nAssign a set named dna with 4 items: each of the 4 bases (single-letter abbreviations) in DNA.\nHints\nRecall the use of curly braces to assign a set.\nThe order of the bases doesn’t matter, because sets are unordered.\nSolution\n\ndna = {'A', 'G', 'C', 'T'}\n\n\nAssign a set named rna with 4 items: each of the 4 bases (single-letter abbreviations) in RNA.\nSolution\n\nrna = {'A', 'G', 'C', 'U'}\n\n\nFind the 3 bases that are shared between DNA and in RNA (try both with an operator and a method, if you want).\nSolution\n\ndna & rna\n{'A', 'G', 'C'}\n\nOr:\n\ndna.intersection(rna)\n{'A', 'G', 'C'}\n\n\nFind all 5 bases that are collectively found among DNA and RNA.\nSolution\n\ndna | rna\n{'U', 'T', 'A', 'G', 'C'}\n\nOr:\n\ndna.union(rna)\n{'U', 'T', 'A', 'G', 'C'}\n\n\nFind the base that only occurs in DNA.\nSolution\n\ndna - rna\n{'T'}\n\nOr:\n\ndna.difference(rna)\n{'T'}\n\n\nAssign a set named purines with the two purine bases and a set named pyrimidines with the three pyrimidine bases.\nSolution\n\npurines = {'A', 'G'}\npyrimidines = {'C', 'T', 'U'}\n\n\nFind the pyrimidine that occurs both in RNA and DNA.\nSolution\nYou can combine more than two sets either by chaining methods or adding another operator.\nSolution\n\npyrimidines & dna & rna\n{'C'}\n\nOr:\n\npyrimidines.intersection(dna).intersection(rna)\n{'C'}\n\n\nBonus: Find the pyrimidine that occurs in RNA but not DNA.\nSolution\n\n(rna - dna) & pyrimidines\n{'U'}\n\nOr:\n\nrna - dna & pyrimidines\n{'U'}\n\nOr:\n\nrna.difference(dna).intersection(pyrimidines)\n{'U'}\n\n\n\nIntro to CSB exercises\n Edit March 12: We did not get to the section on reading tabular files with the csv module in class. Please read CSB 3.7.2 (p. 115-116) before attempting to do these exercises. \nFrom the CSB Chapter 3 preface to the exercises:\n\nHere are some practical tips on how to approach the Python exercises (or any programming task):\nThink through the problem before starting to write code: Which data structure would be more convenient to use (e.g., sets, dictionaries, lists)?\nBreak the task down into small steps (e.g., read file input, create and fill data structure, output).\nFor each step, describe in plain English what you are trying to do— leave these notes as comments within your program to document your code.\nWhen working with large files, initially use only a small subset of the data; once you have tested your code thoroughly you can run it on the whole data set.\nConsider using specific modules (e.g., use the csv module to parse each line into a dictionary or a list).\nSkim through appropriate sections above to refresh your memory on data-type-specific methods.\nUse the documentation and help forums.\n\n\nExercise CSB-1: Measles time series\nIn their article, Dalziel et al. (2016) provide a long time series reporting the numbers of cases of measles before mass vaccination, for many US cities. The data consist of cases in a given US city for a given year, and a given “biweek” of the year (i.e., first two weeks, second two weeks, etc.). The time series is contained in the file Dalziel2016_data.csv.\nWrite a program that extracts the names of all the cities in the database (one entry per city).\nHints\nWhile you could try to parse the file from scratch (you have learnt the building blocks to do so), using the DictReader from the csv module, as we did in class, will make this easier.\nThe city name is in the column loc.\nBecause each city is reported multiple times, the main task here is to remove duplicates. Using a set will be the easiest way to do so, since sets cannot contain duplicates.\nYou don’t need to write to a new file here, just print the set after you are done processing the file.\nPseudocode:\nimport csv\ncities = an empty set\nopen data for reading\ncreate dictionary reader\nfor each row in the file\n    add the city to the set\n\nWrite a program that creates a dictionary where the keys are the cities and the values are the number of records (rows) for that city in the data.\nHints\nInitialize an empty dictionary before you start looping over the lines.\nFor every line, extract the city name and add 1 to the value for that city in your dictionary, since you are counting rows.\nYou don’t need to prepopulate the dictionary with all cities: when you provide a default value with the get() method, a key that is not yet present will be added to the dictionary with said default value.\nFor example, we can build up a dictionary using get() like so:\n\ndd = {} # empty dictionary\nmy_list = ['a', 'b', 'a', 'c', 'd', 'b', 'a']\nfor element in my_list:\n    dd[element] = dd.get(element, 0) + 1\n\nprint(dd)\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\nPseudocode:\nimport csv library\ncitycount = an empty dictionary\nopen file for reading\nset up dictionary reader\n  for each line in data\n      my_city = extract the city\n      citycount[my_city] = use get to update value\n\nWrite a program that calculates the mean population for each city, obtained by averaging the values of pop.\nHints\nNote that for some reason, the population sizes have decimal values.\nAgain, use a dictionary that you keep adding to for each row of the data set. This time, though, each value in the dictionary should be a list of two items: the total population, and the number of occurences.\nIn your get() call, you can initialize the values to be a list of two items as follows (here assuming the dictionary is called citypop and the city’s name has been extracted as mycity):\ncitypop[mycity] = citypop.get(mycity, [0, 0])\nThen, you can refer to each item in the dictionary’s values by chaining indices, e.g. citypop[mycity][0].\nPseudocode:\nimport csv\ncitypop = an empty dictionary\nopen data file reading\nset up dictionary reader\nfor each line in data\n  my_city = extract the city\n  my_pop = extract population\n  if this is the first time you see this city, initialize:\n      citypop[my_city] = [0.0, 0]\n  citypop[my_city][0] = what it was before + my_pop\n  citypop[my_city][1] = what it was before + 1\n\nfor each city\n  divide the first element by the second to obtain the mean\n\nWrite a program that calculates the mean population for each city and year.\nHints\nYou can do this in (at least) two ways with a dictionary:\nBy creating a nested dictionary: each city is a dictionary, which itself contains a dictionary for each year.\nBy using a (city, year) tuple as the keys for the dictionary.\nNote that the worked-out solution in the link below uses the first strategy.\n\n\nBonus: Exercise CSB-2: Red queen in fruit flies\nSingh et al. (2015) show that, when infected with a parasite, the four genetic lines of D. melanogaster respond by increasing the production of recombinant offspring (arguably, trying to produce new recombinants able to escape the parasite). They show that the same outcome is not achieved by artificially wounding the flies. The data needed to replicate the main claim (figure 2 of the original article) is contained in the file Singh2015_data.csv.\nOpen the file, and compute the mean RecombinantFraction for each Drosophila genetic line, and InfectionStatus (W for wounded and I for infected).\nPrint the results in the following form:\nLine 45 Average Recombination Rate:\nW : 0.187\nI : 0.191\nHints For each Dropsophila genetic line, you need to keep track of all the recombination rates for W (wounded) and I (infected).\nFor example, you could build a dictionary of dictionaries in which the first (outer) dictionary has a key for each line, and the inner dictionary has a key for each status (W or I) and a list of recombination rates as each value.\nThen, you would calculate averages for each list at the end.\n\nCSB Solutions\nSolutions for exercise CSB-1.\nSolutions for exercise CSB-2.\n\n\n\n",
      "last_modified": "2021-03-16T21:32:52-04:00"
    },
    {
      "path": "w10_exercises.html",
      "title": "Exercises: Week 10",
      "author": [],
      "contents": "\n\nContents\nExercise 1: More lists\nExercise CSB-1: Assortative mating\nBonus: Exercise CSB-2: Human intestinal ecosystems\nCSB Solutions\n\n\n\n\n\nExercise 1: More lists\nStart with a same list as you created in the exercises for week 8:\n\ndiseases = ['fruit_rot', 'leaf_blight', 'leaf_spots', 'stem_blight',\n            'canker', 'wilt', 'root_knot', 'root_rot']\n\nSort diseases in place.\nSolution\nThe sort() method sorts a list in place:\n\ndiseases.sort()\n\ndiseases\n['canker', 'fruit_rot', 'leaf_blight', 'leaf_spots', 'root_knot', 'root_rot', 'stem_blight', 'wilt']\n\n\nInstead of sorting in place with the sort() method like in the previous step, you can also use the sorted() function, which will not sort in place but return a new, sorted list.\nFind out how to use sorted() to sort in reverse order, and apply this to diseases to create a new list diseases_sorted.\nSolution\nWe can use the reverse argument to sorted() to sort in reverse order:\n\ndiseases_sorted = sorted(diseases, reverse=True)\n\n\nIf you would run fewer_diseases = diseases.remove(\"root_rot\"), what would fewer_diseases contain? Think about what the answer should be, and then check if you were right. Does simply running fewer_diseases versus running print(fewer_diseases) make a difference?\nSolution\nBecause remove() operates in place, it doesn’t return anything:\n\nfewer_diseases = diseases.remove(\"root_rot\")\n\nfewer_diseases\n\nWell, it actualy returns None, which you can see by explicitly calling the print() function:\n\nprint(fewer_diseases)\nNone\n\n\nIf you would run:\n\nmore_diseases = diseases\n\nmore_diseases.append(\"crown_galls\")\n\nWould the list diseases also contain crown_galls? Think about what the answer should be, and then check if you were right.\nSolution\nYes, diseases will contain the item crown_galls that was added to more_diseases, because more_diseases in not an independent list but is merely a second pointer to the same list that diseases points to.\n\nmore_diseases = diseases\n\nmore_diseases.append(\"crown_galls\")\n\ndiseases\n['canker', 'fruit_rot', 'leaf_blight', 'leaf_spots', 'root_knot', 'stem_blight', 'wilt', 'crown_galls']\n\n\nCopy diseases to a new list with a name of your choice – the new list should not simply be a pointed to the old one, but a different object in memory. Then, remove all items from the new list. Check if diseases still contains its items – if not, you’ll have to try again!\nHints\nTo create a new list, use the copy() method or the [:] notation.\nSolution\nTo create a copy, use the copy() method:\n\ndiseases_copy = diseases.copy()\n\nOr the [:] notation.\n\ndiseases_copy = diseases[:]\n\nThen, to remove all elements in the copy of the list:\n\ndiseases_copy.clear()\n\ndiseases\n['canker', 'fruit_rot', 'leaf_blight', 'leaf_spots', 'root_knot', 'stem_blight', 'wilt', 'crown_galls']\n\n\nWhat fundamental difference between lists and strings makes it so that newstring = oldstring creates a new string, whereas newlist = oldlist simply creates a new pointer to the same list?\nSolution\nThe fact that strings are immutable, whereas lists are mutable.\n\nBonus: Get all unique characters (not items) present in diseases.\nHints\nRemember how we can turn a list into a string with join()? If you specify \"\" as the separator, it will simply concatenate all the items in the list.\nNext, note that applying set() to a string will extract the unique characters.\nSolution\nFirst, turn the list into a string using \"\".join. Then, call set() on the string to get a list of unique items (= characters).\n\nset(\"\".join(diseases))\n{'m', 'i', 'p', 'a', 'b', 'l', 'k', 't', 'h', 'e', '_', 'o', 'c', 'g', 'w', 'u', 'n', 'f', 's', 'r'}\n\n\n\nExercise CSB-1: Assortative mating\nJiang et al. (2013) studied assortative mating in animals. They compiled a large database, reporting the results of many experiments on mating. In particular, for several taxa they provide the value of correlation among the sizes of the mates. A positive value of r stands for assortative mating (large animals tend to mate with large animals), and a negative value for disassortative mating.\nYou can find the data in good_code/data/Jiang2013_data.csv. Write a function that takes as input the desired Taxon and returns the mean value of r. Then, apply that function to all taxa in the file.\nHints\nHave a look at the file in the shell before you start.\nTo parse the file, DictReader from the csv module is again a good option, but note that you’ll have the specify the delimiter argument since the file is tab-separated.\nThere are several ways of going about here, but the one in the solutions is to first read in the file and create two lists: one with taxa names and one with the corresponding values for r: (These lists will also be re-used in parts 2 and 3 of the exercise, so it is recommended to follow this approach.)\nimport csv \ntaxa = []\nr_values = []\n\nopen the file and set up dictionary reader\nfor each row:\n    append to taxa\n    append to r_values\nThen, the actual function will take these two lists and a taxon name as input.\ndef compute_avg_r(taxa, r_values, target_taxon = \"Fish\"):\navg_taxon = 0.0\nnum_occurrences = 0\ncycle through the values of taxa\n    every time you find the right taxon, add its r value to avg_taxon\n    and increment num_occurrences\nat the end, divide avg_taxon by num_occurrences and return the average\nTo apply the function to all taxa, use a set to get the unique taxa, and loop over the taxa.\n\nYou should have seen that fish have a positive value of r, but that this is also true for other taxa. Is the mean value of r especially high for fish? To test this, compute a p-value by repeatedly sampling 37 values of r (37 experiments on fish are reported in the database) at random, and calculating the probability of observing a higher mean value of r. To get an accurate estimate of the p-value, use 50,000 randomizations.\nHints\nPart 3 of the exercise will be to repeat this procedure, so it will be a good idea to create a function here.\nYour function should take as input the target taxon name, the lists of taxa and of r-values that you created earlier, and the number of repetitions (randomizations).\nIn the function:\nFirst compute the mean r value for the target taxon.\nThen, iterate over the output of range() to repeat the randomizations.\nIn every iteration, shuffle either the list with r values or the taxon names, using the function scipy.random.shuffle() (don’t forget to import scipy).\nThen, you can simply call the same function you used before to calculate the mean value of r.\nIn every iteration, compare the randomized mean with the observed mean, and keep a tally of the number of times the observed mean is higher.\nThe p-value will simply be the proportion (~= probability) of times you got a higher mean value of r among randomized sets of 37.\nPseudocode:\ndef compute_pvalue(taxa, r_values, target_taxon = \"Fish\", num_rep = 1000):\n    observed_r = compute the mean for the observed average r value\n    count_random_is_higher = 0.0\n    for i in range(num_rep):\n        shuffle the r values\n        random_r = compute the mean using the shuffled values\n        if random_r >= observed_r:\n           increment count_random_is_higher\n    now divide count_random_is_higher by num_rep (= the p-value) and return\n\nRepeat the procedure for all taxa.\nHints\nLoop over all taxa, and call the function you created in the previous part of the exercise in every iteration.\n\n\nBonus: Exercise CSB-2: Human intestinal ecosystems\nLahti et al. (2014) studied the microbial communities living in the intestines of 1,000 human individuals. They found that bacterial strains tend to be either absent or abundant, and posit that this would reflect bistability in these bacterial assemblages.\nThe data used in this study are contained in the directory good_code/data/Lahti2014. The directory contains: - The file HITChip.tab containing estimates of microbial abundance for each sample, as obtained by HITChip signal. - The file Metadata.tab, providing metadata about each of the 1,006 human records. - README, a description of the data by the study’s authors.\nWrite a function that takes as input a dictionary of constraints (i.e., selecting a specific group of records) and returns a dictionary tabulating the values for the column BMI_group for all records matching the constraints.\nFor example, calling:\nget_BMI_count({\"Age\": \"28\", \"Sex\": \"female\"})\nshould return:\n{'NA': 3, 'lean': 8, 'overweight': 2, 'underweight': 1}\nHints\nSeveral strategies are again possible, but the solution here will have all code in the function, including reading in the file.\nOnce again, DictReader() from the csv module is a useful way to read in the data.\nLoop over the lines (rows) and for each row, you’ll want to check whether all conditions are satisfied.\nYour output dictionary will basically be a count table of the values found for the BMI_group, column, so for each matching row, add 1 to the value for the key that represents the group (lean, etc).\nPseudocode:\ndef get_BMI_count(dict_constr):\n    open the file and set up the csv reader\n   for each row:\n        add_to_count = True\n        for each constrain in dict_constr:\n              if constraint is not met:\n                  add_to_count = False\n        if add_to_count:\n              all the constraints are respected\n              add to the tally\n   return the result\n\nWrite a function that takes as input the constraints (as above) and a bacterial “genus.” The function returns the average abundance (in logarithm base 10) of the genus for each BMI group in the subpopulation.\nFor example, calling:\nget_abundance_by_BMI({\"Time\": \"0\",\n                      \"Nationality\": \"US\"},\n                      \"Clostridium difficile et rel.\")\nshould return:\n------------------------------------------------\nAbundance of Clostridium difficile et rel.\nIn subpopulation:\n------------------------------------------------\nNationality -> US\nTime -> 0\n------------------------------------------------\n3.08\nNA\n3.31\nunderweight\n3.84\nlean\n2.89\noverweight\n3.31\nobese\n3.45\nsevereobese\n------------------------------------------------\nHints\nTo write the function, you need to:\nOpen the file Metadata.tab, and extract the SampleID corresponding to the constraints specified by the user (you can use a list to keep track of all IDs).\nOpen the file HITChip.tab to extract the abundances matching the genus specified by the user (and for the ID stored in step 1).\nYou can use the log10 function from the scipy module to calculate the log value (though you may get a deprecation warning; this is now supposed to be called from the numpy module, but we haven’t installed that yet.)\nPseudocode:\ndef get_abundance_by_BMI(dict_constraints, genus = 'Aerococcus'):\n    open the file Metadata.tab extract matching IDs using the same \n    approach as in exercise 1\n    these IDs are stored in BMI_IDs\n\n    Now open HITChip.tab, and keep track of the abundance of the genus for each BMI group\n    Calculate means, and print results\n\nRepeat this analysis for all genera, and for the records having Time = 0.\nHints\nThe genera are contained in the header of the file HITChip.tab. Extract them from the file and store them in a list.\nThen, you can call the function get_abundance_by_BMI({'Time': '0'}, g), where g is the genus; cycle through all genera.\n\n\nCSB Solutions\nSolutions for exercise CSB-1.\nSolutions for exercise CSB-2.\n\n\n\n",
      "last_modified": "2021-03-16T21:32:52-04:00"
    },
    {
      "path": "w11_exercises.html",
      "title": "Exercises: Week 11",
      "author": [],
      "contents": "\n\nContents\nCSB Intermezzo 6.2\nCSB Exercise 1: Lord of the Fruit Flies\nCSB Exercise 2: Number of Reviewers and Rejection Rate\nBonus – CSB Exercise 3: The Evolution of Cooperation\n\n\n\n\n\nCSB Intermezzo 6.2\nGächter and Schulz (2016) performed a provocative experiment to study intrinsic honesty in different countries. Groups of students were asked to per- form two rolls of a fair die, and to report the result of the first roll. They were paid an amount of money proportional to the reported number, with the exception that they were given no money when they reported rolling a 6.\nThe subjects knew of the monetary reward, and that their rolls were private—the experimenters could not determine whether they were telling the truth or not. If everybody were to tell the truth, we would expect that each claim (from 0 to 5 monetary units) would be equally represented in the data, with a proportion of 16 = 0.16. Countries where cheaters were more abundant would have a higher proportion of subjects claiming a reward of 5 units and a lower proportion of those claiming 0 units.\nLoad the file ( data/Gachter2016_data.csv ) using pandas . Which country reported the smallest frequency of Claim == 0 (meaning fewest honest players)? Which the highest?\nNow calculate the reported frequency of rolling the number 5 (which would lead to a claim of 5 units) for each country. Which country has the lowest frequency (most honest players)? Which the highest? Notice that the data report cumulative frequencies; to obtain the frequency of rolling a 5, you need to subtract the cumulative frequency of claiming 4 monetary units from 1.0.\nCSB Exercise 1: Lord of the Fruit Flies\nSuppose you need information on how to breed Drosophila virilis in your lab- oratory and you would like to contact an expert. Conduct a PubMed query on who has published most contributions on D. virilis. This person might be a good researcher to contact.\nIdentify how many papers in the PubMed database have the words Drosophila virilis in their title or abstract. Use the usehistory argument so you can refer to this search in the next step.\nRetrieve the PubMed entries that were identified in step (1).\nCount the number of contributions per author.\nIdentify the five authors with the most contributions.\nCSB Exercise 2: Number of Reviewers and Rejection Rate\nFox et al. (2016) studied the effects on the outcome of papers of the genders of the handling editors and reviewers. For the study, they compiled a database including all the submissions to the journal Functional Ecology from 2004 to 2014. Their data are reported in data/Fox2016_data.csv . Besides the effects of gender and bias in journals, the data can be used to investigate whether manuscripts having more reviewers are more likely to be rejected. Note that this hypothesis should be tested for reviewed manuscripts, that is, excluding “desk rejections” without review. 1. Import the data using pandas , and count the number of reviewers (by summing ReviewerAgreed ) for each manuscript (i.e., unique MsID ). The column FinalDecision contains 1 for rejection, and 0 for acceptance. Compile a table measuring the probability of rejection given the number of reviewers. Does having more reviewers increase the probability of being rejected? 2. Write a function to repeat the analysis above for each year represented in the database. For example, you should return\nYear: 2009\nSubmissions: 626\nOverall rejection rate: 0.827\nNumRev    NumMs   rejection rate\n0   306   0.977\n1   2     0.5\n2   228   0.68\n3   86    0.698\n4   4     0.75\nBonus – CSB Exercise 3: The Evolution of Cooperation\nWhy are some animals (including humans) cooperating? What gives rise to complex social organizations and reciprocity? These fascinating questions can be studied using game theory, made popular in evolutionary biology by May- nard Smith (1982). One of the most well-studied problems in game theory is the “prisoner’s dilemma”: two prisoners are suspected of a crime and interro- gated in separate rooms; each prisoner is given the possibility to either betray the other, or remain silent. If both remain silent (i.e., they cooperate), they each get 1 year in prison; if one remains silent (cooperates) and the other betrays (defects), the one who remained silent is sentenced to 3 years, while the other is let free; finally, if each betrays the other (defects), both receive a term of 2 years. Mathematically, one can show that if the game is played only once, defecting is the safest strategy (Nash equilibrium). But what if the game is played over and over? Then the mathematics becomes difficult, as the best choice depends on the choices of other players. Axelrod’s brilliant idea (Axelrod, 1980a) was to invite game theorists, sociologists, psychologists, and mathematicians to submit programs imple- menting different strategies for a game of iterated prisoner’s dilemma. Each program would have access to the history of the moves played so far, and based on this would decide a move. Each submission then competed against itself, as well as against each other program, in a round-robin tournament.\nImplement the following five strategies:\nalways cooperate\nalways defect\nrandom: cooperate with probability 12 , and defect otherwise\ntit for tat: cooperate on the first turn, then do whatever the other player did in the previous turn\ntit for two tat: start by cooperating, and defect only if the other player has defected twice in a row Each strategy should be a function, accepting as input a list storing the previous turns of the game, and returning 1 for cooperate and 0 for defect.\nWrite a function that accepts the names of two strategies and plays them against each other in a game of iterated prisoner’s dilemma for a given number of turns. Who wins between random and always_defect ? And between random and tit_for_tat ?\n[Advanced] Implement a round-robin tournament in which each strat- egy is played against every other (including against itself) for 10 rounds of 1000 turns each. Who is the winner? 20\n\n\n\n",
      "last_modified": "2021-03-17T09:06:11-04:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
