{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "author": [],
      "contents": "\nThis is the Github website for the course Practical Computing Skills for Biologists (a section of PP8300 – Current Topics in Plant Pathology), a 2-credit online-only course at Ohio State University during the Spring semester of 2021. The course is taught by Jelmer Poelstra from the MCIC.\nOnly slide decks, code-along materials, and exercises are hosted on this website. If you are a student in this course, your starting point should always be the CarmenCanvas site for this course. (Note that you can always find the link to the CarmenCanvas site in the top-right corner of this site by clicking on the graduation cap icon.)\n\n\n\n",
      "last_modified": "2021-02-09T15:22:04-05:00"
    },
    {
      "path": "index.html",
      "title": "",
      "author": [],
      "contents": "\n\n\nPractical Computing Skills for Biologists  Spring 2021  A section of Current Topics in Plant Pathology (PLNTPTH 8300)\n\n\n\n\n",
<<<<<<< HEAD
      "last_modified": "2021-02-09T09:47:29-05:00"
=======
      "last_modified": "2021-02-09T15:22:05-05:00"
>>>>>>> main
    },
    {
      "path": "w01_exercises.html",
      "title": "Week 1 Exercises",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nGetting set up\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\nSolutions\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\n\nThe following exercises were copied from Chapter 1 of the CSB book, with hints and solutions modified from those provided in the book’s Github repo.\n(The exercises marked as “Advanced” are omitted since they require topics not actually covered in the chapter, which we will cover in later modules.)\n\nMain exercises\nGetting set up\nYou should already have the book’s Github repository with exercise files.\nIf not, go to /fs/ess/PAS1855/users/$USER, and run git clone https://github.com/CSB-book/CSB.git.\nNow, you should have the CSB directory referred to in the first step of the first exercise.\nIntermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\nShow hints\nRecall that .. is one level up in the dir tree, and that you can combine multiple .. in a single statement.\n\n(d) Use an absolute path to go to the sandbox directory within python.\n(e) Return to the data directory within the python directory.\nShow hints\nRecall that there is a shortcut to return to your most recent working dir.\n\nIntermezzo 1.2\nTo familiarize yourself with these basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\nShow hints\nThe touch command creates an empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt.\nIntermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? Which is the last?\nShow hints\nYou can either first select the 5th column using cut, and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter.\nTo view just the first or the last line, pipe to head or tail.\n\n(b) How many families are represented in the database?\nShow hints\nStart by selecting the relevant column.\nUse a tail trick shown in the chapter to exclude the first line.\nRemember to sort before using uniq.\n1.10.1 Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (>).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\nShow hints\nTo show the size of a file you can use the -l option of the command ls, and to display human-readable file sizes, the -h option.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\nShow hints\nRecall that with the cp command, it is also possible to give the copy a new name at once.\n\n4. How many contigs are classified as isogroup00036?\nShow hints\nIs there a grep option that counts the number of occurrences?\nAlternatively, you can pass the output of grep to wc -l.\n\n5. Replace the original “two-spaces” delimiter with a comma.\nShow hints\nIn the file, the information on each contig is separated by two spaces:\n>contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n>contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\nIn Linux, we can’t write output to a file that also serves as input (see here), so the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" > myfile.txt # Don't do this! \nIn this case, we will have to save the output in a temporary file and on a separate line, overwrite the original file using mv.\n\n6. How many unique isogroups are in the file?\nShow hints\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nShow hints\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically.\n\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nHow many times were the levels of individuals 3 and 27 recorded?\nShow hints\nYou can use cut to extract just the maleID from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”.\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nShow hints\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written.\nIn the script, you can use the first two so-called positional parameters, $1 and $2, to represent the file name and the maleID, respectively.\n$1 and $2 represent the first and the second argument passed to a script like so: bash myscript.sh arg1 arg2.\n1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\n$ bash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\nShow hints\nTo build the script, you need to combine several commands:\nTo find the number of rows, you can use wc.\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n, and count characters.\n1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nFor example, running the script with:\n$ bash explore.sh ../data/Buzzard2015_data.csv 7\nshould return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\nShow hints\nYou can select a given column from a csv file using the command cut. Then:\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the positional parameters $1 and $2 for the file name and column number, respectively.\n\n\nSolutions\nIntermezzo 1.1\nSolution\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER.\n$ cd /fs/ess/PAS1855/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS1855/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\ncd -\n\nIntermezzo 1.2\nSolution\n(a) Go to the data directory within CSB/unix.\n$ cd /fs/ess/PAS1855/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory         without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\nIntermezzo 1.3\nSolution\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? And which is the last?\ncd ∼/CSB/unix/data/\n\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\n\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\n\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t\";\" -k 5 Pacifici2013_data.csv | head -n 1\n\n\nFollowing the output that you wanted, you may have gotten errors like this:\nsort: write failed: 'standard output': Broken pipe\nsort: write error\nThis may seem disconcerting, but is nothing to worry about, and has to do with the way data streams through a pipe: after head/tail exits because it has done what was asked (print one line), sort or cut may still try to pass on data.\n\n\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n\n1.10.1 Next-Generation Sequencing Data\n1. Change directory to CSB/unix/sandbox.\ncd /fs/ess/PAS1855/users/$USER/CSB/unix/sandbox\n\n2. What is the size of the file Marra2014_data.fasta?\nls -lh ../data/Marra2014_data.fasta\n# Among other output, this should show a file size of 553K\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\ncp ../data/Marra2014_data.fasta my_file.fasta\nTo make sure the copy went well, list the files in the sandbox:\nls\n\n4. How many contigs are classified as isogroup00036?\nTo count the occurrences of a given string, use grep with the option -c\ngrep -c isogroup00036 my_file.fasta \n# 16\nYou can also use a pipe and wc -l to count:\ngrep isogroup00036 my_file.fasta | wc -l\n\n5. Replace the original “two-spaces” delimiter with a comma.\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,:\ncat my_file.fasta | tr -s ' ' ',' > my_file.tmp\nmv my_file.tmp my_file.fasta\n\n6. How many unique isogroups are in the file?\nFirst, searching for > with grep will extract all lines with contig information:\ngrep '>' my_file.fasta | head -n 2\n# >contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n# >contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, use cut to extract the 4th column\ngrep '>' my_file.fasta | cut -d ',' -f 4 | head -n 2\n#gene=isogroup00001\n#gene=isogroup00001\nFinally, use sort -> uniq -> wc -l to count the number of unique occurrences:\ngrep '>' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n# 43\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nWe need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n# >contig00001,numreads=2\n# >contig00002,numreads=8\n# >contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t ‘=’ to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n# >contig00089,numreads=1\n# >contig00176,numreads=1\n# >contig00210,numreads=1\n# >contig00001,numreads=2\n# >contig00003,numreads=2\nUsing the flag -r we can sort in reverse order:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n# >contig00302,numreads=3330\nFinding that contig 00302 has the highest coverage, with 3330 reads.\n\n\n1.10.2 Hormone Levels in Baboons\n1. How many times were the levels of individuals 3 and 27 recorded?\nFirst, let’s see the structure of the file:\nhead -n 3 ../data/Gesquiere2011_data.csv \n# maleID        GC      T\n# 1     66.9    64.57\n# 1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 ../data/Gesquiere2011_data.csv | head -n 3\n# maleID\n# 1\n# 1\nThen we can pipe the results to grep -c to count the number of occurrences (note the flag -w as we want to match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 3\n# 61\n\n# For maleID 27\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 27\n# 5\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nIn the script, we just need to incorporate the arguments given when calling the script, using $1 for the file name and $2 for the individual ID, into the commands that we used above:\ncut -f 1 $1 | grep -c -w $2\nA slightly more verbose and readable example:\n#!/bin/bash\n\nfilename=$1\nind_ID=$2\necho \"Counting the nr of occurrences of individual ${ind_ID} in file ${filename}:\" \ncut -f 1 ${filename} | grep -c -w ${ind_ID}\n\n\nVariables are assigned using name=value (no dollar sign!), and recalled using $name or ${name}. It is good practice to put curly braces around the variable name. We will talk more about bash variables in the next few weeks.\n\n\nTo run the script, assuming it is named count_baboons.sh:\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n1.10.3 Plant–Pollinator Networks\nSolution\nCounting rows:\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\nTo avoid printing the file name, we can either use cat or input redirection:\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l < ../data/Saavedra2013/n10.txt \nCounting rows:\nCounting the number of columns is more work. First, we need only the first line:\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\nNow we can remove all spaces and the line terminator using tr:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\nFinally, we can use wc -c to count the number of characters in the string:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nFinal script:\n#!/bin/bash\n\nfilename=$1\n\necho \"Filename:\"\necho ${filename}\necho \"Number of rows:\"\ncat ${filename} | wc -l\necho \"Number of columns:\"\nhead -n 1 ${filename} | tr -d ' ' | tr -d '\\n' | wc -c\nTo run the script, assuming it is named counter.sh:\nbash counter.sh ../data/Saavedra2013/n10.txt\n# 5\n\n\nWe’ll learn about a quicker and more general way to count columns in a few weeks.\n\n\n\n1.10.4 Data Explorer\nSolution\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\nHere is an example of what the script could look like:\n#!/bin/bash\n\nfilename=$1\ncolumn_nr=$2\n\necho \"Column name\"\ncut -d ',' -f ${column_nr} ${filename} | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n",
      "last_modified": "2021-02-09T15:22:05-05:00"
    },
    {
      "path": "w02_exercises.html",
      "title": "Exercises: Week 2",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nExercise 1: Course notes in Markdown\nExercise 2\n\nBonus exercises\nExercise 3\nBuffalo Chapter 3 code-along\n\nSolutions\nExercise 2\n\n\nMain exercises\nExercise 1: Course notes in Markdown\nCreate a Markdown document with course notes. I recommend writing this document in VS Code.\nMake notes of this week’s material in some detail. If you have notes from last week in another format, include those too. (And try to keep using this document throughout the course!)\nSome pointers:\nUse several header levels and use them consistently: e.g. a level 1 header (#) for the document’s title, level 2 headers (##) for each week, and so on.\nThough this should foremost be a functional document for notes, try to incorporate any appropriate formatting option: e.g. bold text, italic text, inline code, code blocks, ordered/unordered lists, hyperlinks, and perhaps a figure.\nMake sure you understand and try out how Markdown deals with whitespace, e.g. starting a new paragraph and how to force a newline.\nExercise 2\nWhile doing this exercise, save the commands you use in a text document – either write in a text document in VS Code and send the commands to the terminal, or copy them into a text document later.\nGetting set up\nCreate a directory for this exercise, and change your working dir to go there. You can do this either in your $HOME dir (e.g. ~/pracs-sp21/w02/ex2/) or your personal dir in the course’s project dir (/fs/ess/PAS1855/users/$USER/w02/ex2/).\nCreate a disorganized mock project\nUsing the touch command and brace expansions, create a mock project by creating 100s of empty files, either in a single directory or a disorganized directory structure.\nIf you want, you can create file types according to what you typically have in your project – otherwise, a suggestion is to create files with:\nRaw data (e.g. .fastq.gz)\nReference data (e.g. .fasta),\nMetadata (e.g. .txt or .csv)\nProcessed data and results (e.g. .bam, .out)\nScripts (e.g. .sh, .py or .R)\nFigures (e.g. .png or .eps)\nNotes (.txt and/or .md)\nPerhaps some other file type you usually have in your projects.\n\nOrganize the mock project\nOrganize the mock project according to some of the principles we discussed this week.\nEven while adhering to these principles, there is plenty of wiggle room and no single perfect dir structure: what is optimal will depend on what works for you and on the project size and structure. Therefore, think about what makes sense to you, and what makes sense given the files you find yourself with.\nTry to use as few commands as possible to move the files – use wildcards!\nChange file permissions\nMake sure no-one has write permissions for the raw data files. You can also change other permissions to what you think is reasonable or necessary precaution for your fictional project.\nHints\nUse the chmod command to change file permissions and recall that you can use wildcard expansion to operate on many files at once.\nSee the slides starting from here for an overview of file permissions and the chmod command.\nAlternatively, chmod also has an -R argument to act recursively: that is, to act on dirs and all of their contents (including other dirs and their contents).\n\nCreate mock alignment files\nCreate a directory alignment inside an appropriate dir in your project (e.g. analysis, results, or a dir for processed data), and create files for all combinations of 30 samples (01-30), 5 treatments (A-E), and 2 dates (08-14-2020 and 09-16-2020), like so: sample01_A_08-14-2020.sam - sample50_H_09-16-2020.sam.\nThese 300 files can be created with a single touch command. (If you already had an alignment dir, first delete its contents or rename it.)\nHints\nUse brace expansion three times: to expand sample IDs, treatments, and dates.\nNote that {01..20} will successfully zero-pad single-digit numbers.\n\nRename files in a batch\nWoops! We stored the alignment files that we created in the previous step as SAM files (.sam), but this was a mistake – the files are actually the binary counterparts of SAM files: BAM files (.bam).\nMove into the dir with your misnamed BAM files, and use a for loop to rename them: change the extension from .sam to .bam.\nHints\nLoop over the files using globbing (wildcard expansion) directly; there is no need to call ls.\nUse the basename command, or alternatively, cut, to strip the extension.\nStore the output of the basename (or cut) call using command substitution ($(command) syntax).\nThe new extension can simply be pasted behind the file name, like newname=\"$filename_no_extension\"bam or newname=$(basename ...)bam.\nCopy files with wildcards\nStill in the dir with your SAM files, create a new dir called subset. Then, using a single cp command, copy files that satisfy the following conditions into the subset dir:\nThe sample ID/number should be 01-19;\nThe treatment should be A, B, or C.\nCreate a README.md in the dir that explains what you did.\nHints\nJust like you used multiple consecutive brace expansions above, you can use two consecutive wildcard character sets ([]) here.\n\nBonus: a trickier wildcard selection\nStill in the dir with your SAM files, create a new dir subset2. Then, copy all files except the one for “sample28” into this dir.\nDo so using a single cp command, though you’ll need two separate wildcard expansion or brace expansion arguments (as in cp wildcard-selection1 wildcard-selection2 destination/).\nHints\nWhen using brace expansion ({}), simply use two numeric ranges: one for IDs smaller than and one for IDs larger than the focal ID.\nWhen trying to do this with wildcard character sets ([]), you’ll run into one of its limits: you can’t combine conditions with a logical or. Therefore, to exclude only sample 28, you have to separately select IDs that (1) do not start with a 2 and (2) start with a 2 but do not end with an 8.\nBonus: a trickier renaming loop\nYou now realize that your date format is suboptimal (MM-DD-YYYY; which gave 08-14-2020 and 09-16-2020) and that you should use the YYYY-MM-DD format. Use a for loop to rename the files.\nHints\nUse cut to extract the three elements of the date (day, month, and year) on three separate lines.\nStore the output of these lines in variables using commands substitution, like: day=$(commands).\nFinally, paste your new file name together like: newname=\"$part1\"_\"$year\" etc.\nWhen first writing your commands, it’s helpful to be able to experiment easily: start by echo-ing a single example file name, as in: echo sample23_C_09-16-2020.sam | cut ....\nCreate a README\nInclude a README.md that described what you did; again, try to get familiar with Markdown syntax by using formatting liberally.\nBonus exercises\nExercise 3\nIf you feel like it would be good to reorganize one of your own, real projects, you can do so using what you’ve learned this week. Make sure you create a backup copy of the entire project first!\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3.\n\nSolutions\nExercise 2\n1. Getting set up\nmkdir ~/pracs-sp21/w02/ex2/ # or similar, whatever dir you chose\ncd !$                       # !$ is a shortcut to recall the last argument from the last commands\n\n2. Create a disorganized mock project\nAn example:\ntouch sample{001..150}_{F,R}.fastq.gz\ntouch ref.fasta ref.fai\ntouch sample_info.csv sequence_barcodes.txt\ntouch sample{001..150}{.bam,.bam.bai,_fastqc.zip,_fastqc.html} gene-counts.tsv DE-results.txt GO-out.txt\ntouch fastqc.sh multiqc.sh align.sh sort_bam.sh count1.py count2.py DE.R GO.R KEGG.R\ntouch Fig{01..05}.png all_qc_plots.eps weird-sample.png\ntouch dontforget.txt README.md README_DE.md tmp5.txt\ntouch slurm-84789570.out slurm-84789571.out slurm-84789572.out\n\n3. Organize the mock project\nAn example:\nCreate directories:\nmkdir -p data/{raw,meta,ref}\nmkdir -p results/{alignment,counts,DE,enrichment,logfiles,qc/figures}\nmkdir -p scripts\nmkdir -p figures/{ms,sandbox}\nmkdir -p doc/misc\nMove files:\nmv *fastq.gz data/raw/\nmv ref.fa* data/ref/\nmv sample_info.csv sequence_barcodes.txt data/meta/\nmv *.bam *.bam.bai results/alignment/\nmv *fastqc* results/qc/\nmv gene-counts.tsv results/counts/\nmv DE-results.txt results/DE/\nmv GO-out.txt results/enrichment/\nmv *.sh *.R *.py scripts/\nmv README_DE.md results/DE/\nmv Fig[0-9][0-9]* figures/ms\nmv weird-sample.png figures/sandbox\nmv all_qc_plots.eps results/qc/figures/\nmv dontforget.txt tmp5.txt doc/misc/\nmv slurm* results/logfiles/\n\n4. Change file permissions\nTo ensure that no-one has write permission for the raw data, you could, for example, use:\nchmod a=r data/raw/*   # set permissions for \"a\" (all) to \"r\" (read)\n\nchmod a-w data/raw/*   # take away \"w\" (write) permissions for \"a\" (all)\n\n5. Create mock alignment files\n$ mkdir -p results/alignment\n$ # rm results/alignment/* # In the example above, we already had such a dir with files\n$ cd results/alignment \n\n# Create the files:\n$ touch sample{01..30}_{A..E}_{08-14-2020,09-16-2020}.sam\n\n# Check if we have 300 files:\n$ ls | wc -l\n# 300\n\n6. Rename files in a batch\n# cd results/alignment  # If you weren't already there\n\n# Use *globbing* to match the files to loop over (rather than `ls`):\nfor oldname in *.sam\ndo\n   # Remove the `sam` suffix using `basename $oldname sam`,\n   # use command substitution (`$()` syntax) to catch the output of the\n   # `basename` command, and paste `bam` at the end:\n   newname=$(basename $oldname sam)bam\n   \n   # Report what we have:\n   # (Using `-e` with echo we can print an extra newline with '\\n`,\n   # to separate files by an empty line)\n   echo \"Old name: $oldname\"\n   echo -e \"New name: $newname \\n\"\n   \n   # Execute the move:\n   mv \"$oldname\" \"$newname\"\ndone\nA couple of take-aways:\nNote that we don’t need a special construction to paste strings together. we simply type bam after what will be the extension-less file name from the basename command.\nWe print the old and new names to screen; this is not necessary, of course, but good practice. Moreover, this way we can test whether our loop works before adding the mv command.\nWe use informative variable names (oldname and newname), not cryptic ones like i and o.\n7. Copy files with wildcards\nCreate the new dir:\nmkdir subset\nCopy the files using four consecutive wildcard selections:\nThe first digit should be a 0 or a 1 [0-1],\nThe second can be any number [0-9] (? would work, too),\nThe third, after an underscore, should be A, B, or C [A-C],\nWe don’t care about what comes after that, but do need to account for the additional characters, so will use a * to match any character:\n\ncp sample[0-1][0-9]_[A-C]* subset/\nReport what we did, including a command substitution to insert the current date:\necho \"On $(date), created a dir \"subset\" and copied only files for samples 1-29 \\\nand treatments A-D into this dir\" > subset/README.md\n\n# See if it worked:\ncat subset/README.md\n\n8. Bonus: a trickier wildcard selection\nCreate the new dir:\nmkdir subset2\nThe most straightforward way in this case is using two brace expansion selections, one for sample numbers smaller than 28, and one for sample numbers larger than 28:\ncp sample{01..27}* sample{29..30}* subset2/\nHowever, we may not always be able to use ranges like that, and being a little creative with wildcard expansion also works — first, we select all samples not starting with a 2, and then among samples that do start with a 2, we exclude 28:\ncp sample[^2]* sample2[^8]* subset2/\n\n9. Bonus: a trickier renaming loop\nfor oldname in *.bam\ndo\n   # Use `cut` to extract month, day, year, and a \"prefix\" that contains\n   # the sample number and the treatment, and save these using command substitution:\n   month=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 1)\n   day=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 2)\n   year=$(basename \"$oldname\" .bam | cut -d \"_\" -f 3 | cut -d \"-\" -f 3)\n   prefix=$(echo \"$oldname\" | cut -d \"_\" -f 1-2)\n   \n   # Paste together the new name:\n   # (This will fail without quotes around prefix, because the underscore\n   # is then interpreted as being part of the variable name.)\n   newname=\"$prefix\"_\"$year\"-\"$month\"-\"$day\".bam\n   \n   # Report what we have:\n   echo \"Old name: $oldname\"\n   echo -e \"New name: $newname \\n\"\n   \n   # Execute the move:\n   mv \"$oldname\" \"$newname\"\ndone\n\n\nThis renaming task can be done more succinctly using regular expressions and the sed command – we’ll learn about both of these topics later in the course.\n\n\n\n\n\n\n",
      "last_modified": "2021-02-09T15:22:06-05:00"
    },
    {
      "path": "w02_UA_github-signup.html",
      "author": [],
      "contents": "\nWhat?\nCreate a Github account and let me know you’ve done so.\nWhy?\nGithub is a website that hosts git repositories, i.e. version-controlled projects. In Module 3 of this course, we will be learning how to use git together with Github. Also, all graded assignments for this course will be submitted through Github. I will need to know your Github user names in advance to set up the infrastructure for this.\nHow?\nIf you already have a Github account, log in and start at step 6.\nGo to https://github.com. Click “Sign Up” in the top right. Fill out the form: When choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use Github to share your code, for instance when publishing a paper.) You can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.) You probably want to uncheck the box under Email Preferences. When you’re done, click “Create account”. You can answer the questions Github will now ask you, but you should also be able to just skip them. Check your email and click the link to verify your email address. In the far top-right of the page back on Github, click your randomly assigned avatar, and in the dropdown menu, click “Settings”. In the Emails tab (left-hand menu), deselect the box “Keep my email addresses private”. In the Profile tab, enter your Name. Still in the Profile tab, upload an avatar. This can be a picture of yourself but if you prefer, you can use something else. Here at this CarmenCanvas assignment, submit the link to your Github profile, which will be “https://github.com/”.\n\n\n\n",
      "last_modified": "2021-02-09T15:22:06-05:00"
    },
    {
      "path": "w03_GA_git.html",
      "title": "Graded Assignment I: Shell, Markdown, and Git",
      "author": [],
      "contents": "\n\nContents\nIntroduction\nPart I – Create a new Git repo\nPart II – Add some Markdown content\nPart III – Add some “data” files\nPart IV – Renaming files on a new branch\nPart V – Create and sync an online version of the repo\n\n\nIntroduction\nIn this assignment, you will primarily be practicing your Git skills. You will also show that you can write in Markdown and can write a fo loop to rename files.\nThe instructions below will take you through it step-by-step. All of the the steps involve things that we have gone through in class and/or that you have practiced with in last week’s exercises.\nGrading information:\nThe number of points (if any) that you can earn for each step are denoted in cursive between square brackets (e.g. [0.5]). In total, you can earn 10 points with this assignment, which is 10% of your grade for the class.\nIf you make a mistake like an unnecessary commit, or something that requires you to subsequently make an extra commit: this is no problem. Points will not be subtracted for extra commits as long as you indicate what each commit is for & I’m still able to figure out which commits are the ones that I requested.\nPoints are not subtracted for minor things like typos in commit messages, either.\nBasically, you should not feel like you have to restart the assignment unless things have become a total mess…\nSome general hints:\nDon’t forget to constantly check the status of your repository (repo) with git status: before and after nearly all other Git commands that you issue. This will help prevent mistakes, and will also help you understand what’s going on as you get up and running with Git.\nIt’s also a good idea to regularly check the log with git log; recall that git log --oneline will provide a quick overview with one line per commit.\n\nPart I – Create a new Git repo\nCreate a new directory at OSC.\nA good place for this directory is in /fs/ess/PAS1855/users/$USER/week03/, but you are free to create it elsewhere (I will only be checking the online version of your repo).\nI suggest the name pracs-sp21-GA1 for the dir, (“GA1” for “Graded Assignment 1”), but you are free to pick a name that makes sense to you.\nLoad the OSC git module. Don’t forget to do this, or you will be working with a much older version of Git.\nInitialize a local Git repository inside your new directory. [0.5]\nCreate a README file in Markdown format. [0.5]\nThe file should be named README.md, and for now, just contain a header saying that this is a repository for your assignment.\nStage and then commit the README file. [0.5]\nInclude an appropriate commit message.\nPart II – Add some Markdown content\nCreate a second Markdown file with some more contents. [1.5]\nThis file can have any name you want, and you can also choose what you want to write about.\nUse of a good variety of Markdown syntax, as discussed and practiced in week 2: headers, lists, hyperlinks, and so on. (You’ll have to include some inline code and a code block later on, so you may or may not choose to use those now.) Also, make sure to read the next step before you finish writing.\nAs long as you are not writing minimalistic dummy text without any meaning (like “Item 1, Item 2”), you will not be graded for what you are writing about, so feel free to pick something you like – and don’t worry about the details.\n(If you’re not feeling inspired, here are some suggestions: lecture and reading notes for this week; a table of Unix and/or Git commands that we’ve covered; things that you so far find challenging or interesting about Git; how computational skills may help you with your research.)\nHints In VS Code, open the Markdown preview on the side, so you can experiment and see whether your formatting is working the way you intend.\n\nCreate at least two commits while you work on the Markdown file. [0.5]\nTry to break your progress up into logical units that can be summarized with a descriptive commit message.\nHints\n\nBad commits/commit messages:\n“First part of the file” along with “Second part of the file”.\nGood commits and commit messages:\n“Summarized key concepts in the Git workflow” along with “Made a table of common Git commands”.\n\nUpdate the README.md file. [0.5]\nBriefly describe the current contents of your repo now that you actually have some contents.\nStage and commit the updated README file. [0.5]\nPart III – Add some “data” files\nCreate a directory with dummy data files. [0.5]\nCreate a directory called data and inside this directory, create 100 empty files with a single touch command and at least one brace expansion (e.g. for samples numbered 1-100, or 20 samples for 5 treatments). Give the files the extension .txt.\nStage and commit the data directory and its contents. [0.5]\nAs always, include a descriptive message with your commit.\nPart IV – Renaming files on a new branch\nAs it turns out, there was a miscommunication, and and all the “data” files will have to be renamed. You realize that this is risky business, and you don’t want to lose any data.\nIt occurs to you that one good way of going about this is creating a new Git branch and performing the renaming task on that branch. Then, if anything goes wrong, it will be easy to go back: you can just switch back to the master branch.\n(Note that there are many other ways of undoing and going back in Git, but this one is perhaps conceptually the easiest, and safe.)\nGo to a new branch. [0.5]\nCreate a new branch called rename-data-files or something similar, and move to the new branch. (Also, check whether this worked.)\nWrite a for loop to rename the files. [1.5]\nYou can keep it as simple as switching the file extension to .csv, or do something more elaborate if you want.\nBecause these files are being tracked by Git, recall that there is a Git-friendly modification to the command to rename files: use that in the loop.\nHints\nThese slides from last week have a similar renaming loop example.\nTo replace the last part of the filename, like the extension, recall that you can strip suffices using the basename command.\nBefore you go ahead and actually rename the files, use echo in your loop to print the (old and) new filenames: if it looks good, then add the renaming command to the loop.\n\nIf you just used mv and forgot to use git mv, don’t fret. Have a look at the status of the repo, then do git add --all and check the status again: Git figured out the rename.\nIf things go wrong, for instance you renamed incorrectly, or even deleted the files, you can start over in a few ways:\nYou can delete any files in data/, commit the deletion, and recreate the files. (The extra commits will not affect your grade, but use your commit messages to clarify what you’re doing.)\nA solution that would always work, including in case these files were actually valuable and you couldn’t just recreate them, is as follows:\nLike above, delete any files in data/ and commit the deletion.\nSwitch back to the master branch. Your data/ files should be back.\nCreate a new renaming branch and move there.\nOptionally, delete the old/failed renaming branch.\n\nCommit the changes made by the renaming operation.\nMerge into the master branch. [0.5]\nWith the files successfully renamed, go back to the master branch.\nThen, merge the rename-data-files branch into the master branch.\n(Optionally, remove the rename-data-files branch, since you will no longer need it.)\nUpdate the README file.\nIn the README file, describe what you did, including some inline code formatting, and put the code for the for loop in a code block.\n(No, the repo doesn’t really make sense as a cohesive unit anymore, but that’s okay while we’re practicing. :) )\nStage and commit the updated README file. [0.5]\nPart V – Create and sync an online version of the repo\nPhew, those were a lot of commits! Let’s share all of this work with the world.\nCreate a Github repository. [0.5]\nGo to https://github.com, sign in, and create a new repository.\nIt’s a good idea to give it the same name as your local repo, but these names don’t have to match.\nYou want to create an empty GitHub repository, because you will upload all the contents from your local repo: therefore, don’t check any of the boxes to create files like a README.\n\nPush your local repo online. [0.5]\nWith your repo created, follow the instructions that Github now gives you, under the heading “…or push an existing repository from the command line”.\nThese instructions will comprise three commands: git remote add to add the “remote” (online) connection, git branch to rename the default branch from master to main, and git push to actually “push” (upload) your local repo.\nWhen you’re done, click the Code button, and admire the nicely rendered README on the front page of your Github repo.\nHints\n\nAssuming that you’re using SSH authentication, which you should have set up in this week’s ungraded assignment, make sure you use the SSH link type: starting with git@github.com rather than HTTPS.\n\n\nCreate an issue to mark that you’re done! [0.5]\nFind the “Issues” tab for your repo on Github:\n\n\n\nIn the Issues tab, click the green button New Issue to open a new issue for you repo.\nGive it a title like “I finished my assignment”, and in the issue text, tag @jelmerp. You can say, for instance, “Hey @jelmerp, can you please take a look at my repo?”.\n\n",
      "last_modified": "2021-02-09T15:22:07-05:00"
    },
    {
      "path": "w03_UA_git-setup.html",
      "title": "Week 3 -- Ungraded Assignment : <br> Git setup and GitHub authentication",
      "author": [],
      "contents": "\nPart 1: Git setup\nThese instructions are for setting up Git at OSC, but from Step 3 onwards, you can also follow them to set up Git for your local computer.\nOpen up a terminal at OSC.\nYou can do this after logging in at https://ondemand.osc.edu in one of two ways:\nDirect shell access: Clusters (top blue bar) > Pitzer Shell Access.\nIn VS Code: Interactive Apps > Code Server > then open a terminal using Ctrl+backtick > break out of the Singularity shell by typing bash.\n\nLoad the OSC Git module.\n(Note: Git is available in any OSC shell without loading any modules, but that is a rather old version – so we load a newer one.)\nmodule load git/2.18.0\nUse git config to make your name known to Git.\nSomewhat confusingly, mote that this should be your actual name, and not your GitHub username:\ngit config --global user.name 'John Doe'\nUse git config to make your email address known to Git.Make sure that you use the same email address you used to sign up for GitHub.\ngit config --global user.email 'doe.391@osu.edu'\nUse git config to set a default text editor for Git.\nOccasionally, when you need to provide Git with a “commit message” to Git and you haven’t entered one on the command line, Git will open up a text editor for you. Even though we’ll be mostly be working in VS Code during this course, in this case, it is better to select a text editor that can be run within the terminal, like nano (or vim, if you are familiar with it). To specify Nano as your default text editor for Git:\ngit config --global core.editor \"nano -w\"\n\n# You could set VS Code as your editor on your local computer,\n# if you use it there:\n# git config --global core.editor \"code --wait\"\nCheck whether you successfully changed the settings:\ngit config --global --list\n\n# user.name=John Doe\n# user.email=doe.39@osu.edu\n# colour.ui=true\n# core.editor=nano -w\nSet colors if needed.\nMake sure you see colour.ui=true in the list (like above), so Git output in the terminal will use colors. If you don’t see this line, set it using:\ngit config --global color.ui true\n\nPart 2: GitHub authentication\nGitHub authentication: Background\nTo be able to link local Git repositories to their online counterparts on GitHub, we need to set up GitHub authentication.\nRegular password access (over HTTP/HTTPS) is now “deprecated” by GitHub, and two better options are to set up SSH access with an SSH key, or HTTPS access with a Public Access Token.\nWe’ll use SSH, as it is easier – though still a bit of drag – and because learning this procedure will also be useful for when you’ll be setting up SSH access to the Ohio Supercomputer Center. (But note that GitHub now labels HTTPS access as the “preferred” method.)\nFor everything on GitHub, there are separate SSH and HTTPS URLs, and GitHub will always show you both types of URLs. When using SSH, we need to use URLs with the following format:\ngit@github.com:<USERNAME>/<REPOSITORY>.git\n(And when using HTTPS, you would use URLS like https://github.com/<USERNAME>/<REPOSITORY>.git)\nSetting up GitHub SSH authentication\nThese instructions are for setting up authentication at OSC, but you can repeat the same steps to set up authentication for your local computer.\nIn a terminal at OSC, use the ssh-keygen command to generate a public-private SSH key pair like so:\nssh-keygen -t rsa\nYou’ll be asked three questions, and for all three, you can accept the default by just pressing Enter:\n# Enter file in which to save the key (<default path>):\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again: \n\n\n\nNow, you have a file called id_rsa.pub in your ~/.ssh folder, which is your public key. To enable authentication, we will put this public key on GitHub – our public key interacts with our private key, which we do not share.\nPrint the public key to screen using cat:\ncat ~/.ssh/id_rsa.pub\nCopy the public key, i.e. the contents of the public key file, to your clipboard. Make sure you get all of it, including the “ssh-rsa” part (but beware that your new prompt may start on the same line as the end the key):\n\n\n\nIn your browser, go to https://github.com and log in.\nGo to your personal Settings. (Click on your avatar in the far top-right of the page, and select Settings in the drop-down menu.)\nClick on SSH and GPG keys in the sidebar.\nClick the green New SSH key button.\nGive the key an arbitrary, informative name, e.g. “OSC” to indicate that you are using this key at OSC.\nPaste the public key, which you copied to your clipboard earlier, into the box.\n\n\nClick the green Add SSH key button. Done!\n\n\n\n\n",
      "last_modified": "2021-02-09T15:22:07-05:00"
    },
    {
      "path": "w04_exercises.html",
      "title": "Exercises: Week 4",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nExercise 1: Pseudogenes with grep, uniq -c, and others\nExercise 2: Calculations with awk\nExercise 3: Replacements with sed\n\nBonus exercises\nExercise 4: FASTA\nExercise 5: Exons\nExercise 6: Miscellaneous\n\nSolutions\nExercise 1: Pseudogenes with grep, uniq -c, and others\nExercise 2: Calculations with awk\nExercise 3: Replacements with sed\nExercise 4: FASTA\nExercise 5: Exons\nExercise 6: Miscellaneous\n\n\nMain exercises\nThese exercises will use the same files that we’ve been working with in class: those in chapter-07-unix-data-tools in the bds-files repository for the Buffalo book.\nYou can either work directly in that directory, or create a “sandbox” directory with the Buffalo files copied.\nFor many of the exercises, there are multiple ways of solving them. The hints and solutions provide one or a few ways, but you may well end up using a different approach. If you struggle, try to follow the approach suggested by the hints, so you can later cross-check with solutions that use this same approach.\nFor most questions, the code required is quite similar to the examples in Buffalo chapter 7 (and the slides). If the hints are too cryptic for you, I recommend you first go back to the section on the relevant command in the chapter.\nExercise 1: Pseudogenes with grep, uniq -c, and others\nIn this exercise, we will start using the file Mus_musculus.GRCm38.75_chr1.gtf that we’ve seen in class.\nLet’s say that you need to extract annotation information for all pseudogenes on mouse chromosome 1 from the GTF, and write the results to a new GTF file. We’ll do that in this exercise with a detour along the way.\nCount the number of lines with the string “pseudogene”.\nHints Use grep -c to count the matches.\n\n\nDoes this mean there are over a 1,000 pseudogenes on this chromosome? No, because there are “nested” genomic features in the file: for each gene, several “subfeatures” are listed, such as individual exons. This genomic feature type information is listed in column 3.\nFor lines that match “pseudogene” as above, which different feature types are listed in column 3, and what are their counts (i.e., create a count table for column 3)?\nAre there any coding sequences (“CDS”), start codons (“start_codon”) and stop codons (“stop_codon”)?\nHints\n\nTo answer this question, compute count tables using sort | uniq -c after grepping for “pseudogene”.\n\n\nIf you are more restrictive in your grep matching to only match “pseudogene” as a full “word”, do you still see CDS or start/stop codons?\nHints Use grep -w to only match full “words”: consecutive alphanumeric characters and the underscore.\nThis would avoid matching longer words that contain “pseudogene”, such as “transcribed_pseudogene”.\n\n\nWe have “exon”, “transcript” and “gene” features in column 3, but we want just the gene-level coordinates of the pseudogenes: after grepping for “pseudogene” like in the previous question, select only rows for which the feature type in column 3 is gene.\nPipe your output into head or less; we don’t want to write to a new file yet.\nHints\n\nThe “gene” you want to match is in its own column. Matching restrictively using the -w flag to only match whole “words” will work in this case.\nEven better would be to explicitly match the tabs surrounding “gene”. This can be done using the -P flag to grep (grep -P) and the \\t symbol for tabs.\nYet another approach that is even more explicit uses awk to make the match specifically for the 3rd column (recall that $3 is the third column in awk, so you can use $3 == \"gene\"). If you try with awk, you’ll have to make sure to ignore the header lines, which are not tabular.\n\n\nComprehensively check that your output only contains “gene” in column 3.\nHints\n\nPipe your previous command into cut and then sort | uniq -c to quickly check which values are present in column 3.\n\n\nNow, you are almost ready to write the results to a new GTF file, called pseudogenes.gtf, with the features you selected above: gene-level pseudogene features.\nOne challenge is that you also want the header, which you won’t get when redirecting the output from the previous command.\nA straightforward solution would be to first write the header to a new file, and then append the lines with the selected features.\nAlternatively, you can use subshells to do this in a single go. If you want to try this, make sure you have read the Buffalo section on subshells (p. 169-170) first.\nHints\n\nStructure the subshell solution as follows:(<select-header> ; <select-from-rest-of-file>) > pseudogenes.gtf\n\n\nExercise 2: Calculations with awk\nIn this exercise, you will continue working with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nHow many elements are at least 1,000 base pairs long?\nHints\nRemove the header from consideration using grep -v.\nFilter by feature length using awk, and get the length by subtracting column 4 from column 5.\n\nHow many genes (“gene” in column 3) on the + strand (“+” in column 7) are at least 1,000 base pairs long?\nHints\nAdjust your command for the previous question as follows:\nMatch columns 3 (feature type) and 7 (strand) with awk, using == to only select matching rows.\nUse the && operator in awk to combine conditions.\n\nWhat’s the combined size of all genes?\n(On this chromosome, that is: this GTF only contains a single chromosome.)\nHints\nFirst, remove the header from consideration using grep -v.\nNext, select “gene”-level features only, which you can do with awk as above, with grep -w, or with grep -P with \\t for tab.\nNext, use a variable in awk:\nInitialize the variable in a BEGIN statement;\nAdd the length of the gene (column 5 - column 4) for each row;\nPrint the result with an END statement.\n\n\nWhat is the mean UTR length?\nHints\nThe code is very similar to that for the previous question; just grep for “UTR” instead, and calculate the mean by dividing the final value of awk variable by the number of rows (NR) in the END statement.\n\n\nExercise 3: Replacements with sed\nThe genotypes.txt contains diploid genotypes for 4 loci for 9 individuals.\nRemove all underscores from genotypes.txt and write the output to a new file.\nHints\nUse the sed 's/<pattern>/<replacement>/<modifiers>' <file> construct.\nYou’ll need the modifier to do global (i.e., >1 per line) replacements.\n\n\nReplace the / delimiter for genotypes (G/G) in genotypes.txt by a colon (G:G) and write the output to a new file.\nHints\nIn the sed 's/<pattern>/<replacement>/<modifiers>' <file> construct, forward slashes / will not work here, because the pattern we are looking for is a forward slash. Use a different symbol instead, e.g. a #:sed 's#<pattern>#<replacement>#<modifiers>' <file>\n\n\nBonus exercises\nExercise 4: FASTA\nFirst, convert the FASTQ file contaminated.fastq to a FASTA file using the following awk command, and try to understand what it does, by and large:\nawk '{ if(NR%4 == 1) { printf(\">%s\\n\", substr($0,2)); } else if(NR%4 == 2) print; }' \\\n    contaminated.fastq > contaminated.fasta\nSelect FASTA entries (sequence and header) from contaminated.fasta with the sequence AGATCGGAAGA and write them to a new file, contaminated_sel.fasta. (Your new file should have 6 sequences, so 12 lines.)\nHints\n\nMatch the sequence with grep and use the -B option to also retrieve the preceding line, which is the sequence header/identifier. In addition, use --no-group-separator to prevent grep from printing -- between matches (alternatively, you could use another call to grep with -v to filter out -- lines).\n\n\nCheck how many sequences, if any, have one ore more undetermined bases (N).\nHints\n\nMake sure you first exclude the header lines, since you only want to match N in the sequences.\n\n\nApparently, there may have been a problem with cycle 50 in the Illumina run.\nGet the frequency of each base for position 50 in the FASTA file.\nHints\n\nUse cut -c <n> to extract the 50th position.\n\n\n\nExercise 5: Exons\nIn this exercise, you will once again work with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nNow, let’s explore some statistics on exons in the GTF file.\nCreate a tab-separated, two-column file called exons.txt that lists each unique “exon_id” along with its “gene_id”, sorted by gene ID. (The “exon_id” and “gene_id” key-value pairs are in the huge final column.)\nThe first couple of lines of the file should look as follows:\n  ENSMUSG00000000544      ENSMUSE00000160546\n  ENSMUSG00000000544      ENSMUSE00000160549\n  ENSMUSG00000000544      ENSMUSE00000226264\n  ENSMUSG00000000544      ENSMUSE00000395516\nHints\n\nFirst grep for lines that contain “gene_id” followed by “exon_id”.\nUse a sed command with two backreferences (capture the backreferences with parentheses, (<match>), and recall them with \\1 and \\2) to extract just the gene ID and the exon ID.\nFinally, only keep unique lines and sort.\n\nUse the exons.txt file you created to answer the following questions.\nOn average, how many exons does each gene have?\nHints\n\nThe number of genes is simply the number of lines in exons.txt.\nCount the number of exons using uniq and then wc -l.\nThe division can be done with expr $n_genes / $n_exons although you will not get any decimals (arithmetic in bash is very limited!). The above exmplae assumes you assigned the results to variables using command substitution, e.g.: ngenes=$(wc -l ...).\n(You could also, e.g., call Python inline, but we’re not there yet.)\n\nWhat is the highest number of exons for one gene?\nHints\n\nUse uniq -c followed by a reverse numeric sort.\n\n\nCreate a count table for the number of exons per gene. It should tell you how many genes have one exon, how many have two exons, etc.\nHints\n\nYou’ll need to process the uniq -c output of exon counts to get rid of the leading spaces. You can do this with sed; use ^ * as the search pattern, or ^ + with sed -E (the + symbol to match one ore more of the preceding character is in the extended regex set).\nThen, you’ll need to create a count table from these counts, so another round of count | uniq -c.\n\nExercise 6: Miscellaneous\nIn this exercise, you will once again work with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nCount the total number of features on each strand (+ or -, column 7).\nCount the number of features from each of the three values for “gene_source” (“ensembl”, “havana”, and “ensembl_havana”).\nHints\n\nYou can either use sed with a backreference to capture the “gene_source” type, or grep -o to capture a larger match followed by cut and sed to extract just the “gene_source” type.\n\n\nHow many genes have a UTR (untranslated region)? Create a list of “gene_id”s for genes with a UTR.\nHints\n\nFirst grep for UTR, then for the “gene_id” while also extracting the match with grep -o. Once again, the extraction of a match can alternatively be done with a backreference in sed.\n\n\nSolutions\nExercise 1: Pseudogenes with grep, uniq -c, and others\nTo start with, I will assign the name of the GTF file to the variable $gtf, for quick reference in the solutions for the and the next exercises:\ngtf=Mus_musculus.GRCm38.75_chr1.gtf\n1. Count the number of lines with the string “pseudogene”.\n$ grep -c \"pseudogene\" $gtf\n\n#> 1041\nThe -c option to grep will count the number of matching lines.\n\n2. Count feature types.\n$ grep -v \"^#\" $gtf | grep \"pseudogene\" | cut -f 3 | sort | uniq -c\n\n#>     15 CDS\n#>    524 exon\n#>    223 gene\n#>      4 start_codon\n#>      4 stop_codon\n#>    264 transcript\n#>      7 UTR\nThe first line computes a count table for pseudogenes, and the second line computes an equivalent count table for all genes.\nWe use grep -v \"^#\" to exclude the header (the command would work without this line, but it’s better not to assume the header won’t match, and to explicitly remove it first). Note that the caret ^ is a regex symbol for the beginning of the line. We use the -v option to grep to invert the match.\nWe use cut -f 3 to select the third column.\nWe use the sort | uniq -c combination to create a count table.\n3. Count feature types after more restrictive matching.\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | cut -f 3 | sort | uniq -c\n\n#>    499 exon\n#>    221 gene\n#>    258 transcript\nThis is very similar to the solution for the previous exercise, but now we use the -w option to grep to only select full words (consecutive alphanumeric characters and the underscore).\n\n4. Select “pseudogene” matches for which the feature type in column 3 is “gene”.\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | grep -w \"gene\" | head\n\n# Alternative 1: More explicit match by matching tabs:\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | grep -P \"\\tgene\\t\" | head\n\n# Alternative 2: More explicit match by matching column 3 only:\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | awk '$3 == \"gene\"' | head\nTo match tabs like in the second solution, we need to use greps -P option, for \"Perl-like regular expressions. (Yes, it confusing and unfortunate that there are three types of regular expressions in grep!)\nIn the third solution we explicitly and exactly match column 3 with awk – this is the most robust solution.\n\n5. Check that your output only contains “gene” in column 3.\n$ grep -v \"^#\" $gtf | grep \"pseudogene\" | grep -w \"gene\" | \\\n      cut -f 3 | sort | uniq -c\n\n#> 223 gene\nWe select the third column and use the sort | uniq -c combination to create a count table, which will quickly tell us whether there are multiple values in the column.\n\n6. Write the pseudogene selection to a new GTF file.\nSolution when writing the header and the rest of the file separately (don’t forget to use >> for appending the output of the second line, if you use this method!):\n$ grep \"^#\" $gtf > pseudogenes.gtf\n$ grep -v \"^#\" $gtf | \\\n      awk '$2 == \"pseudogene\" && $3 == \"gene\"' \\\n      >> pseudogenes.gtf\nSolution with subshells:\n$ (grep \"^#\" $gtf; grep -v \"^#\" $gtf | \\\n      awk '$2 == \"pseudogene\" && $3 == \"gene\"') \\\n      > pseudogenes.gtf\nNote: rather than with grep -v \"^#\", we can also exclude the header using !/^#/ in awk:\n$ (grep \"^#\" $gtf; \\\n      awk '!/^#/ && $2 == \"pseudogene\" && $3 == \"gene\"' $gtf) \\\n      > pseudogenes.gtf\n\nExercise 2: Calculations with awk\n1. How many features are at least 1,000 bp long?\n$ grep -v \"^#\" $gtf | \\\n      awk '$5 - $4 > 999' | \\\n      wc -l\n\n#> 8773\nFirst, we again exclude the header like in exercise 1 (this can also be done using awk: see below).\nNext we subtract column 4 ($4) from column 5 ($5) and use as a condition that this difference should be larger than 999.\n# Alternative that excludes the header in awk:\n$ awk '!/^#/ && $5 - $4 > 999' $gtf | \\\n      wc -l\n\n#> 8773\n\n2. How many genes on the + strand are at least 1,000 bp long?\n$ grep -v \"^#\" $gtf | \\\n      awk '$3 == \"gene\" && $7 == \"+\" && $5 - $4 > 999' | \\\n      wc -l\n\n#> 725\nWe chain together different conditions using &&.\nThe second line uses awk to match + in column 7, and to select lines where the difference between column 5 and 4 (i.e., the feature length) is larger than 999 bp.\nThe third line simply counts the number of lines we have left.\n2. What’s the combined size of all genes?\n$ grep -v \"^#\" $gtf | \\\n      grep -w \"gene\" | \\\n      awk 'BEGIN { s=0 }; { s += $5 - $4 }; END { print s }'\n  \n#> 78935729    # 78,935,729 => ~79 Mbp\nThe first two lines are the same as for the previous question (see the solution there for further details).\nIn the third line, we use a variable in awk to compute a sum of the feature lengths (i.e., column 5 minus column 4). With the BEGIN statement, we initialize the variable s before we start processing lines. Then, for each line, we add the feature length to the value using the += operator (shorthand for s = s +). Finally, with the END statement, we report the final value of the variable s after we have processed all lines.\n3. What is the mean UTR length?\n$ grep -v \"^#\" $gtf | \\\n      grep -w \"UTR\" | \\\n      awk 'BEGIN { s=0 }; { s += $5 - $4 }; END { print s/NR }'\n      \n#> 472.665      \nThis code is very similar to that for the previous question (see that solution for more details), but now we divide the sum (variable s) by NR, an automatic variable in awk that holds the current record (line) number. Since the END statement is executed after all lines have been processed, the record number will be the total number of lines in the file.\n\nExercise 3: Replacements with sed\n1. In genotypes.txt, remove all underscores.\n$ sed 's/_//g' genotypes.txt > genotypes2.txt\n\n$ cat genotypes2.txt\n#> id      indA    indB    indC    indD\n#> S000    G/G     A/G     A/A     A/G\n#> S001    A/T     A/T     T/T     T/T\n#> S002    C/T     T/T     C/C     C/T\n#> S003    C/T     C/T     C/T     C/C\n#> S004    C/G     G/G     C/C     C/G\n#> S005    A/T     A/T     A/T     T/T\n#> S006    C/G     C/C     C/G     C/G\n#> S007    A/G     G/G     A/A     G/G\n#> S008    G/T     G/T     T/T     G/T\n#> S009    C/C     C/C     A/A     A/C\nSince there are multiple underscores on line 1, you need the g modifier.\n\n2. In genotypes.txt, replace the / delimiter by a colon (:).\n$ sed 's#/#:#g' genotypes.txt > genotypes3.txt\n\n$ cat genotypes3.txt\n#> id      ind_A   ind_B   ind_C   ind_D\n#> S_000   G:G     A:G     A:A     A:G\n#> S_001   A:T     A:T     T:T     T:T\n#> S_002   C:T     T:T     C:C     C:T\n#> S_003   C:T     C:T     C:T     C:C\n#> S_004   C:G     G:G     C:C     C:G\n#> S_005   A:T     A:T     A:T     T:T\n#> S_006   C:G     C:C     C:G     C:G\n#> S_007   A:G     G:G     A:A     G:G\n#> S_008   G:T     G:T     T:T     G:T\n#> S_009   C:C     C:C     A:A     A:C\nBecause the pattern we want to match is a /, we use a different symbol in the sed syntax: here, I chose a #.\n\nExercise 4: FASTA\n2. Write sequences with AGATCGGAAGA into a new file.\n$ grep -B 1 --no-group-separator \"AGATCGGAAGA\" contaminated.fasta \\\n      > contaminated_sel.fasta\n\n$ wc -l < contaminated_sel.fasta\n\n#> 12\n\n# Alternative to `--no-group-separator`;\n# remove the lines with `--` afterwards:\n$ grep -B 1 \"AGATCGGAAGA\" contaminated.fasta | \\\n      grep -v \"--\" > contaminated_sel.fasta\nUse B 1 to get the sequence ID line for each match.\nUse --no-group-separator (top solution) or pipe into grep -v \"--\" to avoid having a group separator, which grep inserts for multiline output like this, in the output.\n3. Check how many sequences have one ore more undetermined bases (N).\nMake sure you first exclude the header lines, since you only want to match “N” in the sequences themselves:\n$ grep -v \"^>\" contaminated.fasta | grep -c \"N\"\n\n#> 0\nThere are no Ns in the sequence.\n\n4. Get the frequency of each base for position 50 in the fasta file.\n$ grep -v \"^>\" contaminated.fasta | cut -c 50 | sort | uniq -c \n\n#> 2 C\n#> 3 G\n#> 3 T\n\nExercise 5: Exons\n1. Create a tab-delimited file with “exon_id” and “gene_id”\n$ grep \"gene_id.*exon_id\" $gtf | \\\n      sed -n -E 's/.*gene_id \"(\\w+)\".*exon_id \"(\\w+).*/\\1\\t\\2/p' | \\\n      sort | uniq > exons.txt\nIn the first line of code, we select only lines that contain both “gene_id” and “exon_id”.\nIn the second line, we capture the values for “gene_id” and “exon_id” using backreferences in sed.\nFinally, we sort (since we want to sort by the first column, just sort will work) and then take only unique rows using uniq, and redirect to a new file.\n2. On average, how many exons does each gene have?\n$ n_genes=$(wc -l < exons.txt)\n$ n_exons=$(cut -f 1 exons.txt | uniq | wc -l)\n\n# Divide using `expr`:\n$ expr $(wc -l < exons.txt) / $(cut -f 1 exons.txt | uniq | wc -l)\n\n#> 11\n\n# Alternatively, do the division with Python:\n$ python -c \"print($n_genes / $n_exons)\"\n\n#> 11.627\nFirst, we count the number of genes, then the number of exons, and we assign each value to a variable.\nThen, we divide using expr (or Python to get decimals).\n3. What is the highest number of exons for one gene?\n$ cut -f 1 exons.txt | uniq -c | sort -rn | head\n\n#>    134 ENSMUSG00000026131\n#>    116 ENSMUSG00000066842\n#>    112 ENSMUSG00000026207\n#>    108 ENSMUSG00000026609\n#>    102 ENSMUSG00000006005\n#>     97 ENSMUSG00000037470\n#>     93 ENSMUSG00000026141\n#>     92 ENSMUSG00000026490\n#>     91 ENSMUSG00000048000\n#>     82 ENSMUSG00000073664\n“ENSMUSG00000026131” has as many as 134 exons!!\n\n3. Create a count table for the number of exons per gene.\n$ cut -f 1 exons.txt | uniq -c | \\\n      sed 's/^ *//' | cut -d \" \" -f 1 | \\\n      sort -rn | uniq -c | \\\n      sort -rn > exon_count_table.txt\n\n$ head exon_count_table.txt     \n#>    646 1   # 646 genes with 1 exon\n#>    186 2   # 186 genes with 2 exons\n#>     94 3   # etc...\n#>     71 5\n#>     71 4\n#>     53 8\n#>     51 7\n#>     51 6\n#>     44 11\n#>     41 10\nIn the first line, we get the number of exons for each gene.\nIn the second line, we clean up the uniq -c output.\nIn the third line, we create the count table of the number of exons.\nFinally we sort this and redirect the output to a file.\nExercise 6: Miscellaneous\n1. Count the number of elements on each strand.\n$ grep -v \"^#\" $gtf | cut -f 7 | sort | uniq -c\n\n#> 40574 +\n#> 40652 -\n\n2. Count the number of elements for each of the three values for “gene_source”.\n# Capture the gene_source with sed:\n$ sed -n -E 's/.*gene_source \"(\\w+)\".*/\\1/p' $gtf | \\\n      sort | uniq -c\n\n#> 18209 ensembl\n#> 61089 ensembl_havana\n#> 1928 havana\n\n# Alternatively, capture the gene_source with `grep -o`:\n$ grep -E -o 'gene_source \"\\w+\"' $gtf | \\\n      cut -f2 -d\" \" | sed 's/\"//g' | \\\n      sort | uniq -c\n      \n#> 18209 ensembl\n#> 61089 ensembl_havana\n#> 1928 havana\n\n3. How many genes have a UTR? Create a list of gene_ids for genes with a UTR.\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n   grep -E -o 'gene_id \"\\w+\"' | sort | uniq | wc -l\n\n#> 1179\n\n# Create the list of genes:\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n      grep -E -o 'gene_id \"\\w+\"' | \\\n      cut -f2 -d\" \" | sed 's/\"//g' | \\\n      sort | uniq > genes_with_UTR.txt\n\n# Alternative with sed:\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n      sed -n -E 's/.*gene_id \"(\\w+)\".*/\\1/p' | \\\n      sort | uniq | wc -l > genes_with_UTR.txt\n\n\n\n\n",
      "last_modified": "2021-02-09T15:22:08-05:00"
    },
    {
      "path": "w05_exercises.html",
      "title": "Week 5 Exercises",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nBackground\nExercise 1: Getting set up\nExercise 2: Create a script to compute stats for a FASTQ file\nExercise 3: Modify the looping script\nExercise 4: Bells and whistles\n\nBonus exercises\nExercise 5: Find the longest file\nExercise 6: Plant-pollinator networks\nExercise 7: Data explorer\n\nSolutions\nExercise 1\nExercise 2\nExercise 3\nExercise 4\nExercise 5\nExercise 6\nExercise 7\n\n\nThe main exercises will work with some FASTQ files. If you don’t care much for DNA sequence files, and perhaps start to get lost in the technicalities, make sure to carry on to the three bonus exercises.\nMain exercises\nBackground\nThese exercises will work with 6 FASTQ files with sequences from the V4 region of 16S rRNA, generated in a metabarcoding experiment.\nThe FASTQ files come in pairs: for every sample, there is a FASTQ file with forward reads (or “read 1” reads) that contains _R1_ in its filename, and a FASTQ file with corresponding reverse reads (or “read 2” reads) that contain _R2_ in its filename. So, our 6 FASTQ files consist of 3 pairs of files with forward and reverse reads for 3 different biological samples.\nThe sequences were generated by first amplifying environmental samples with a pair of universal 16S primers, and these primer sequences are expected to be present in the FASTQ sequences. You will search for these primer sequences below, and there are two things to be aware of:\nA primer can also be present in the FASTQ sequence as its reverse complement, so we will search for reverse complements too.\nThe primers contain a few variable sites, which are indicated using ambiguity codes. For instance, an R means that the site can be either an A or a G, and an N means that the site can be any of the four bases. See here for a complete overview of these ambiguity codes.\nHere are the primer sequences and their reverse complements:\nForward primer (“515F”): GAGTGYCAGCMGCCGCGGTAA / TTACCGCGGCKGCTGRCACTC.\nReverse primer (“806R”): TTACCGCGGCKGCTGRCACTC / ATTAGAWACCCBNGTAGTCCGT.\nExercise 1: Getting set up\nCreate a directory for this week’s exercises and move into it.\nIn these exercises, you will be working with and modifying one of the scripts in Buffalo’s Chapter 12, which is printed below. Save this script as fastq_stat_loop.sh.\n#!/bin/bash\nset -e -u -o pipefail\n\n# Specify the input samples file (3rd column = path to FASTQ file):\nsample_info=samples.txt\n\n# Create a Bash array from the third column of \"$sample_info\":\nsample_files=($(cut -f 3 \"$sample_info\"))\n\n# Loop through the array:\nfor fastq_file in ${sample_files[@]}; do\n\n  # Strip .fastq from each FASTQ file, and add suffix:\n  results_file=\"$(basename $fastq_file .fastq)-stats.txt\"\n\n  # Run \"fastq_stat\" on a file:\n  fastq_stat \"$fastq_file\" > stats/$results_file\n\ndone\nThe FASTQ files you’ll work with are inside the directory /fs/ess/PAS1855/data/week05/fastq. Copy these files into an appropriate directory inside your own directory for these exercises, like data/.\n\nExercise 2: Create a script to compute stats for a FASTQ file\nUnfortunately, the fastq_stat program referenced in Buffalo’s script is an imaginary program… So, let’s create a script fastqc_stat.sh that actually produces a few descriptive statistics for a a FASTQC file.\nSet up a script skeleton with the header lines we’ve been discussing: a shebang line and the various set settings for robust scripts.\nThe script should process one command-line argument, the input file. Assign the automatic placeholder variable for the first argument to a variable with a descriptive name, like fastq_file.\nHints\n\n\nThe automatic placeholder variable for the first argument is $1.\n\n\n\nIn the sections below, you can print all your output to standard out, i.e. simply use echo with no redirection.\nAlso, just for the purpose of testing the commands while developing them below, it will be convenient to assign one of the FASTQ files to a variable fastq_file.\n\n\nHave the script report its own name as well as the name of the FASTQ file that is being processed.\nHints\n\n\nRecall that the name of the script is automatically stored in $0.\n\n\nCompute and report the number of sequences in the FASTQ file.\nHints\n\nThe number of sequences can be assumed to be the total number of lines divided by 4 (or alternatively, the number of lines consisting only of a +). Recall that FASTQ files have 4 lines per sequence: a header, the sequence, a + divider, and the quality scores.\nTo make the division, you can use syntax like expr 12 / 4 – and you can use command substitution, $(wc -l ...), to insert the number of lines in the division.\n\n\nAs mentioned in the Background section, the primer sequences should be present in the FASTQ files. Prior to sequence analyses, these are usually removed with a specialized program like cutadapt, but we can use our grep skills to quickly search for them.\nWe will search for the forward primers only in the forwards reads, and for the reverse primers only in the reverse reads. Therefore, start with an if statement that tests whether the file name contains forward (_R1_) or reverse (_R2_) reads.If it contains forward reads, you should be counting occurrences of the forward primer or its reverse complement. Similarly, if it contains reverse reads you should be counting occurrences of the reverse primer or its reverse complement.\nYou’ll also have to replace the ambiguity codes, like R, with character classes that enumerate the possible alternative bases.\nHints\n\n\nThe test in the if statement should be a grep command that examines whether the filename contains _R1_ (pipe echo output into grep). The grep standard output should be redirected to /dev/null, since we’re only interested in the exit status: If grep finds a match, the commands following then will be executed; if it doesn’t, the commands following else will be executed.\nYou can assume that matches will only be made in the sequences themselves (and not the headers or quality scores), so you can grep the file as is: you don’t need to first select the lines with sequences.To replace ambiguity codes with character classes in the grep regular expression: an R in the primers becomes [AG] in the grep expression, e.g. the partial sequence ATRG would become AT[AG]G in your regular expression.\nYou’ll need both the primer and its reverse complement in a single grep regular expression: separate them with an logical “or” (|) and to enable this, use grep -E for extended regular expressions.\n\n\nBonus: Print a count table of sequence lengths.\nHints\n\n\nYou’ll have to select only the lines with the actual DNA sequences, and the best way of doing that is using awk like so (see the Solution for an explanation of why this works):\nawk '{ if(NR%4 == 2) }'\nNext, you need to count the number of characters in each line, which is best done in the same awk command using print length($0), which will print the number of characters in the line.\nAfter that, it’s the familiar sort | uniq -c idiom to create a count table.\n\n\nMake the script executable.\nHints\n\n\nUse the chmod command.\n\n\nRun the script for a single FASTQ file by calling it from the command line.\n\nExercise 3: Modify the looping script\nNow, let’s modify Buffalo’s script fastq_stat_loop.sh.\nCurrently, the metadata file that contains the list of files to process is hard-coded as samples.txt. Also, the file names have to be extracted from a specific column from the metadata file. This is not a very flexible setup, and is sensitive to minor changes in a file like samples.txt.\nChange the script to let it accept a list of FASTQ file names as an argument.\nHints\n\n\nRecall that the placeholder variable for the first argument that is passed to a script on the command line is $1.\nInside the command substitution ($()) that populates the array, you can simply use cat instead of cut on the file that contains the list of file names, since this file will no longer have multiple columns.\n\n\nCurrently, the output directory is also hard-coded, as stats – let’s instead add the output directory as a second argument to the script. Moreover, add code that creates this output directory if it doesn’t already exist.\nHints\n\n\nYou can write an explicit test to see if the output dir exists first, but simply using mkdir -p will also work: with the -p option, mkdir doesn’t complain when a dir already exists (and can also make multiple levels of directories at once).\n\n\nChange the line that runs the imaginary program fastq_stat to let it run your fastq_stat.sh script instead. Make sure the path to your script and the path to the output file is correct.\nIn each iteration of the loop, let the script report which FASTQ file will be analyzed.\nNow that the script takes arguments, we need another file or script to create the list of filenames and to submit the script with the appropriate arguments. Specifically, in this file, we need:\nA line that creates a new file just containing the FASTQ file names (akin to column 3 from samples.txt – but with the paths to our actual FASTQ files).\nA line that runs the fastq_stat_loop.sh script. The file that contains the list of FASTQ files should be passed to the script as the first argument, and the output directory as the second argument.\nAs for the actual path to the output dir, you can use whatever (relative!) path makes sense to you.\nCreate the file containing the code outlined above. You can save this file either as a .sh script (e.g. fastqc_runner.sh), or put these lines in a Markdown file inside a code block. Either option is reasonable because these lines would likely be run interactively, as opposed to the fastq_stat_loop.sh script which will be run non-interactively. It’s still important to save these interactive commands in a file, so you know what you did and can easily reproduce it.\nMake fastq_stat_loop.sh executable and run it.\nThe loop script should run fastq_stat.sh on all your FASTQ files. Check the output, which should be in one file per FASTQ file in the output dir you designated. You should be seeing that the vast majority of reads contain the primer sequences. Be proud – with a quick script that only runs for a couple of seconds, and no specialized software, you have queried hundreds of thousands of sequences!\n\nExercise 4: Bells and whistles\nIn this exercise, you will touch up your fastqc_stat.sh script to include tests for robustness, and to report what the script is doing.\nFor the tests, check whether they work (…)! For instance, to check the test for the number of arguments, try running the script with no arguments, and also with two arguments, and see if the script produces the error messages you wrote.\nWrite an if statement to check whether the FASTQ file exists / is a regular file and whether it can be read. If not, give an error and exit the script with exit code 1.\nHints\n\n\nGo back to this week’s slides for an example of testing whether a file is a regular file (-f) and whether it can be read (-r).\nTo exit with exit code 1, simply use: exit 1. This should be done after printing any error messages you, or those won’t actually be printed.\n\n\nCheck whether exactly 1 argument was passed to the script on the command line. If not, return an error, succinctly report how to use the script (“usage: …”), and exit with exit code 1.\nHints\n\n\nGo back to this week’s slides for an example of a very similar test.\nThe number of arguments that were passed to a script are automatically available in $#.\nYou can test for equality using <integer> -eq <integer> (e.g. 10 -eq 10 which will evaluate to true) and you can negate a test (\"number of argument is not equal to 1) using a ! before the comparison expression.\n\n\nAdd date commands at the start and the end of the script, so you’ll be able to tell how long it took the script to complete.\n\nBonus exercises\nExercise 5: Find the longest file1\nWrite a shell script called longest.sh that takes two arguments: the name of a directory and a file extension (like txt). The script should print the name of the file that has the most lines among all files with with that extension in that directory.\nMake sure the script has the shebang and set headers, and make the script executable.\nThen, run your script to learn which FASTQ file has the most lines (and sequences):\n$ ./longest.sh data/fastq fastq\n… should print the name of the .fastq file in data/fastq with the highest number of lines and therefore sequences.\nHints\n\nYou can count lines for many files at once using wc -l: simply provide it with a globbing pattern.\n\nExercise 6: Plant-pollinator networks\nThis exercise is slightly modified after 1.10.3 from the CSB book. The Saavedra2013 directory can be found inside the CSB repository at CSB/unix/data/Saavedra2013, and the following code assumes you are in the directory CSB/unix/sandbox. (If you no longer have the repository, download it again using git clone https://github.com/CSB-book/CSB.git.)\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files as an argument, and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces. Don’t forget to make your script executable and to add the standard header lines. Your script should return:\n$ ./netsize.sh ../data/Saavedra2013/n1.txt\n\n#> Filename: ../data/Saavedra2013/n1.txt\n#> Number of rows: 97\n#> Number of columns: 80\nWrite a script that prints the numbers of rows and columns for each network, taking the directory containing all the files as an argument:\n$ ./netsize_all.sh ../data/Saavedra2013\n\n#> ../data/Saavedra2013/n10.txt     14      20\n#> ../data/Saavedra2013/n11.txt     270     91\n#> ../data/Saavedra2013/n12.txt     7       72\n#> ../data/Saavedra2013/n13.txt     61      17\n#> …\nHints\n\nTo find the number of columns, use awk and recall awk’s NF (number of fields => number of columns) keyword.\nTo combine them in a script, use command substitution to assign the result of a command to a variable. (For example: mytxtfiles=$(ls *.txt) stores the list of .txt files in the variable $mytxtfiles.)\nNext, you need to write a for loop.\nYou can now use the script you’ve just written in combination with sort to answer the questions (remember the option -k to choose a column and -r to reverse the sorting order).\nYou can use echo -e to print tabs using \\t: echo -e \"column1 \\t column2\".\n\nExercise 7: Data explorer\nThis is slightly modified after exercise 1.10.4 from the CSB book. The Buzzard2015_data.csv file can be found inside the CSB repository at CSB/unix/data/Buzzard2015_data.csv.\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\n1. Write a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nDon’t forget to make your script executable and add the standard header lines.\nFor example, running the script with:\n$ ./explore.sh ../data/Buzzard2015_data.csv 7\n…should return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\nHints\n\nYou can select a given column from a csv file using the command cut. Then,\nThe column name is going to be in the first line (header); access it with head.\nFor the next few commands, you’ll need to remove the header line – the tail trick to do so is tail -n +2.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail).\nRename the placeholders $1 and $2 for the command-line arguments to named variables for the file name and column number, respectively.\n\nSolutions\nExercise 1\n1. Create a directory for these exercises.\n\nmkdir /fs/ess/PAS1855/users/$USER/week05/exercises/\n\n3. Copy the FASTQ files into your own dir.\n\nmkdir -p data/fastq/\ncp /fs/ess/PAS1855/data/week05/fastq/* data/fastq/\n\nExercise 2\n1. Create a new script with header lines.\n\nThe first two lines of the script:\n#!/bin/bash\nset -u -e -o pipefail\nSave the file as fastq_stat.sh.\n\n2. Assign the first argument to a named variable.\n\nAdd a line like this to your fastq_stat.sh script:\nfastq_file=$1\n\nFor testing the code below, we first assign one of the FASTQ filenames to the variable $fastq_file:\nfastq_file=201-S4-V4-V5_S53_L001_R1_001.fastq\n3. Let the script report its name and the name of the FASTQ file.\n\nAdd lines like these to your fastq_stat.sh script:\necho \"$0: A script to compute basic summary stats for a FASTQ file.\"\necho \"FASTQ file to be analyzed: $fastq_file\"\n\n4. Compute and report the number of sequences in the FASTQ file.\n\nAdd lines like these to your fastq_stat.sh script:\n# We save the output of our commands using command substitution, $().\n# In the wc -l command, use input redirection so the filename is not in the output.\nn_lines=$(wc -l < \"$fastq_file\")\nn_seqs=$(expr \"$n_lines\" / 4)     # Use expr for arithmetics\n\necho \"Number of sequences: $n_seqs\"\nAlternatively, you can use the (( )) syntax for arithmetics – just take care that in this case, there can be no spaces between the mathematical operator and the numbers:\nn_seqs=$((\"$n_lines\"/4))\nTo get the number of sequences, you can also count the number of lines that only have a + symbol:\nn_seqs=$(grep -c \"^+$\" $fastq_file)\nRecall that + is also a regular expression symbol, but only so in the extended regex set. Therefore, without the -E flag to grep, we are matching a literal + when we use one in our expression.\n5. Search for adapter sequences.\n\nAdd lines like these to your fastq_stat.sh script:\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelse\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\nTo initiate the if statement:\nWe redirect grep’s output to /dev/null, since we’re only interested in the exit status.\nIf grep finds a match, this evaluates to “true”, and the next code block will be executed. If no match is found, the block after else will be executed.\nInside the if statement:\nWe use the grep’s -E option to search for either of the two primer sequences at once with |.\nWe use character classes like [CT] in place of each ambiguity code.\nWe use grep’s -c option to count the matches.\nOr, to explicitly check the file contains _R2 in its name, rather than assuming this must be the case if it doesn’t contain _R1, you can use elif (short for “else-if”) to add another test:\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelif echo \"$fastq_file\" | grep \"_R2_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\nWhen we test this code by itself, we get:\n#> FASTQ file contains forward (R1) reads, checking for primer 1...\n#> Number of forward primer sequences found: 44687\nThis looks good!\n\n6. Bonus: Print a count table of sequence lengths.\n\nAdd lines like these to your fastq_stat.sh script:\necho \"Count table of sequence lengths:\"\nawk '{ if(NR%4 == 2) print length($0) }' \"$fastq_file\" | sort | uniq -c\nTo select only actual sequences, and not other lines, from the FASTQ file, we use the following trick. We know that in the FASTQ file, every fourth line, starting from line number 2, contains the actual sequence (i.e.: line 2, line 6, line 10, etc). To select these lines we can use the “modulo” operator to select only line numbers (NR in awk) for which, after dividing the line number by 4, we have 2 left: NR%4 == 2.\nNext we print the number of characters on the entire line using awk’s length function: print length($0).\nFinally, we pass the sequence lengths on to the sort | uniq -c idiom, which will give us a count table.\n7. Make the script executable.\n\nAssuming it is in the working dir:\n$ chmod u+x fastq_stat.sh \n\n# Or for all scripts at once:\n$ chmod u+x *sh\n\n8. Run the script for a single FASTQ file by calling it from the command line.\n\n# Assuming you have assigned a FASTQ file to $fastq_file for testing:\n./fastq_stat.sh $fastq_file\n\nExercise 3\nChange the script to let it accept a list of FASTQ file names as an argument.\n\nAdd this line to the script:\nfile_list=\"$1\"\nNow, replace the following line:\n# Old line:\n# sample_files=($(cut -f 3 \"$sample_info\"))\n\n# New line:\nsample_files=($(cat \"$file_list\"))\n\nAdd the output directory as a second argument to the script, and create the output dir if necessary.\n\noutput_dir=\"$2\"\n\n# Create the output dir, if necessary:\nmkdir -p \"$output_dir\"\nmkdir -p will not complain if the directory already exists, and it can make multiple levels of directories at once.\n3. Modify the line in the script that calls fastq_stat to call your script.\n\n# Old line:\n# fastq_stat \"$fastq_file\" > stats/$results_file\n\n# New line:\nscripts/fastq_stat.sh \"$fastq_file\" > \"$output_dir\"/\"$results_file\"\nThe results_file line can remain the same:\nresults_file=\"$(basename $fastq_file .fastq)-stats.txt\"\n\n4. In each iteration of the loop, let the script report which FASTQ will be analyzed.\n\nAdd the following line inside the loop:\necho \"Running fastq_stat for FASTQ file $fastq_file\"\n\n5. Create a second file/script to create a list of FASTQ files and to run the loop script.\n\nTo create a list of FASTQ files:\nfile_list=fastq_file_list.txt\n\nls data/*fastq >\"$file_list\"\nTo run the loop script:\noutput_dir=results/fastq_stats\n\n./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\n6. Make fastq_stat_loop.sh executable and run it.\n\nRun these lines:\n$ chmod u+x ./fastq_stat_loop.sh\n\n$ ./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\nThe final fastq_stat_loop.sh script.\n\n#!/bin/bash\nset -e -u -o pipefail\n\nfile_list=\"$1\"\noutput_dir=\"$2\"\n\n# Create the output dir, if necessary:\nmkdir -p \"$output_dir\"\n\n# Create an array with FASTQ files\nfastq_files=($(cat \"$file_list\"))\n\n# Report:\necho \"Number of fastq files: ${#fastq_files[@]}\"\n\n# Loop through the array:\nfor fastq_file in \"${fastq_files[@]}\"; do\n\n  echo \"Running fastq_stat for FASTQ file $fastq_file\"\n\n  # Strip .fastq from each FASTQ file, and add suffix:\n  results_file=\"$(basename \"$fastq_file\" .fastq)-stats.txt\"\n\n  # Run \"fastq_stat\" on a file:\n  scripts/fastq_stat.sh \"$fastq_file\" >\"$output_dir\"/\"$results_file\"\n\ndone\n\nThe final lines in the runner script / Markdown code block.\n\nfile_list=fastq_file_list.txt\noutput_dir=results/fastq_stats\n\nls data/*fastq >\"$file_list\"\n\n./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\nExercise 4\n1. Check whether the FASTQ file is a regular file than can be read.\n\nAdd these lines to your script:\n! -f will be true if the file is not a regular/existing file\n! -r will be true if the file is not readable\nWe use || to separate the two conditions with a logical or.\nWith exit 1, we terminate the script with an exit code that indicates failure.\n\nif [ ! -f \"$fastq_file\" ] || [ ! -r \"$fastq_file\" ]; then\n  echo \"Error: can't open file\"\n  echo \"Second argument should be a readable file\"\n  echo \"You provided: $fastq_file\"\n  exit 1\nfi\nTo test your test:\n$ ./fastq_stat.sh blabla \n\n2. Check whether only one argument was provided.\n\nAdd these lines to your script:\n$# is the number of command-line arguments passed to the script\n-eq will test whether the numbers to the left and right of it are the same, and will return true if they are.\nWe negate this with !: if the number of arguments is NOT 1, then error out.\nexit 1 will exit with exit code 1, which signifies an error / failure.\n\nif [ ! \"$#\" -eq 1 ]; then   # If the number of args does NOT equal 1, then\n  echo \"Error: wrong number of arguments\"\n  echo \"You provided $# arguments, while 1 is required.\"\n  echo \"Usage: fastq_stat.sh <file-name>\"\n  exit 1\nfi\nTo test your test:\n$ ./fastq_stat.sh                      # No args\n$ ./fastq_stat.sh $fastq_file blabla   # Two args\n\n3. Add date commands.\n\nSimply include two lines with:\ndate\n… in the script, one before file processing, and one after.\n\nThe final fastq_stat.sh script.\n\n#!/bin/bash\nset -u -e -o pipefail\n\necho \"$0: A script to compute basic summary stats for a FASTQ file.\"\ndate\necho\n\n# Test number of args -------------------------------------------------\nif [ ! \"$#\" -eq 1 ]; then\n  echo \"Error: wrong number of arguments\"\n  echo \"You provided $# arguments, while 1 is required.\"\n  echo \"Usage: fastq_stat.sh <file-name>\"\n  exit 1\nfi\n\n# Process command-line args and report ------------------------------------\n\nfastq_file=\"$1\"\n\necho \"FASTQ file to be analyzed: $fastq_file\"\necho\n\nif [ ! -f \"$fastq_file\" ] || [ ! -r \"$fastq_file\" ]; then\n  echo \"Error: can't open file\"\n  echo \"Second argument should be a readable file\"\n  echo \"You provided: $fastq_file\"\n  exit 1\nfi\n\n# Count primer sequences ------------------------------------------------\n\nn_lines=$(wc -l <\"$fastq_file\")\nn_seqs=$(expr \"$n_lines\" / 4)\n\necho \"Number of sequences: $n_seqs\"\n\n# Count primer sequences ------------------------------------------------\n\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelif echo \"$fastq_file\" | grep \"_R2_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\n\n# Bonus: Count table of sequence lengths ---------------------------------\necho \"Count table of sequence lengths:\"\nawk '{ if(NR%4 == 2) print length($0) }' \"$fastq_file\" | sort | uniq -c\n\n# Report ----------------------------------------------------------------\necho\necho \"Done with $0 for $fastq_file.\"\ndate\n\nExercise 5\nSolution\n\nThere are several ways to do this, here’s one example:\n#!/bin/bash\nset -u -e -o pipefail\n\ndir=\"$1\"\nextension=\"$2\"\n\nwc -l \"$dir\"/*.\"$extension\" | sort -rn | sed -n '2p'\nYou have to print the second rather than the the first line: the first line will be a total line number count across all files, which wc automatically computes.\nInstead of using sed, you could also use head -n 2 | tail -n 1 to print the second line:\nwc -l \"$dir\"/*.\"$extension\" | sort -rn | head -n 2 | tail -n 1\n(Note also that there are two files with the same number of lines: R1 and R2 files for the same sample always have the same number of reads.)\n\nExercise 6\n1. Write a script that takes one file and determines the number of rows and columns.\n\n#!/bin/bash\nset -u -e -o pipefail\n\nfile=\"$1\"\n\necho \"Filename:\"\necho \"$file\"\n\n# We can get the number of rows simply by counting the number of lines:\n# To avoid printing the filename we redirect the input like below\n# (Or we could have done: \"cat ../data/Saavedra2013/n10.txt | wc -l\")\necho \"Number of rows:\"\nwc -l < \"$file\"\n\n# To count the number of columns, we use awk - recall that NF is the number\n# of fields (columns), and to print that number only for a single line, we exit:\necho \"Number of columns:\"\nawk '{ print NF; exit }' \"$file\"\n\n# head -n 1 \"$file\" | awk '{ print NF }' # Also works\nWe can save this script as netsize.sh and make it executable using chmod u+x netsize.sh.\n\n2. Write a script that prints the number of rows and columns for each network.\n\n#!/bin/bash\nset -u -e -o pipefail\n\ndir=\"$1\"\n\n# We can loop over the files using globbing:\nfor file in \"$dir\"/*.txt; do\n\n    # Next, we can save the number of rows and columns in variables:\n    n_row=$(wc -l < \"$file\")\n    n_col=$(awk '{ print NF; exit }' \"$file\")\n    \n    # And print them all on one line:\n    echo -e \"$file \\t $n_row \\t $n_col\"\ndone\nWe can save this script as netsize_all.sh and run it as follows:\n./netsize_all.sh ../data/Saavedra2013\n\n3. Which network has the largest number of rows and which the largest number of columns?\n\n# Having written the script netsize_all.sh,\n# you can take its output and order it according to rows or columns.\n\n# Sorting by column 2 gives you the file with the largest number of rows:\n$ ./netsize_all.sh | sort -n -r -k 2 | head -n 1\n#> ../data/Saavedra2013/n58.txt 678 90\n\n# Sorting by column 3 gives you the file with the largest number of columns:\n\n$ ./netsize_all.sh | sort -n -r -k 3 | head -n 1\n#>  ../data/Saavedra2013/n56.txt 110 207\n\nExercise 7\nSolution\n\n#!/bin/bash\n\nset -u -e -o pipefail\n\nfile=$1    # $1 is the file name\ncolumn=$2  # $2 is the column of interest\n\necho \"Column name:\"\ncut -d ',' -f \"$column\" \"$file\" | head -n 1\n\n# In the next lines, we need to skip the header, which we can do using\n# tail -n +2\necho \"Number of distinct values:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort | uniq | wc -l\n\necho \"Minimum value:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort -n | head -n 1\n\necho \"Maximum value:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort -n | tail -n 1\nIf we save the script as explore.sh, and make it executable, we can run it using:\n./explore.sh ../data/Buzzard2015_data.csv 6\n\n# Column name\n# Abund.n\n# Number of distinct values:\n# 46\n# Minimum value:\n# 1\n# Maximum value:\n# 157\n\n# This works well also for alphabetical order:\n\n./explore.sh ../data/Buzzard2015_data.csv 3\n\n# Column name\n# genus\n# Number of distinct values:\n# 85\n# Minimum value:\n# Acacia\n# Maximum value:\n# Zanthoxylum\n\nThis exercise was slightly modified from Software Carpentry’s Shell Novice tutorial.↩︎\n",
      "last_modified": "2021-02-09T15:22:09-05:00"
    },
    {
      "path": "w06_UA-ssh.html",
      "title": "Optional Ungraded Assignment: `ssh` setup",
      "author": [],
      "contents": "\n\nContents\nIntroduction\nssh set-up: use shortcuts\n\n\nIntroduction\nbackground-color: #f2f5eb\nssh set-up: use shortcuts\nCreate a file called ~/.ssh/config:\n$ touch `~/.ssh/config`\nOpen the file and add your alias(es):\n> Host <arbitrary-alias-name>    \n>     HostName <remote-name>\n>     User <user-name>\n–\n\nThis is what it looks like on my machine, so I can login using “ssh jo”: \n\n\n\n\n\n",
      "last_modified": "2021-02-09T15:22:09-05:00"
    },
    {
      "path": "w06_GA-scripts.html",
      "title": "Graded Assignment II: Shell scripts at OSC",
      "author": [],
<<<<<<< HEAD
      "contents": "\n\nContents\nIntroduction\nGrading information\nGeneral information\n\nGetting set up\nA script to run Cutadapt on one pair of FASTQ files\nRunning the scrip and finishing up\nOptional (ungraded) - A script to loop over all samples\n\n\nIntroduction\nIf you did last week’s exercises, much of the following introduction will be familiar to you, as we are working with the same FASTQ files and looking at the same primers – but this time around, you will remove the primer sequences using the software Cutadapt, whereas in last week’s exercises, you just looked for their presence.\nThis assignment will work with 6 FASTQ files with sequences from the V4 region of 16S rRNA, generated in a metabarcoding experiment.\nThe FASTQ files come in pairs: for every sample, there is a FASTQ file with forward reads (or “read 1” reads) that contains _R1_ in its file name, and a FASTQ file with corresponding reverse reads (or “read 2” reads) that contain _R2_ in its file name. So, our 6 FASTQ files consist of 3 pairs of files with forward and reverse reads for 3 different biological samples.\nThe sequences were generated by first amplifying environmental samples with a pair of universal 16S primers, and these primer sequences are expected to be present in the FASTQ sequences. You will remove these primer sequences with the program Cutadapt, and there are two things to be aware of:\nA primer can also be present in the FASTQ sequence as its reverse complement, so we will search for reverse complements too.\nThe primers contain a few variable sites, which are indicated using ambiguity codes. For instance, an R means that the site can be either an A or a G, and an N means that the site can be any of the four bases. See here for a complete overview of these ambiguity codes.\nThese are the primer sequences:\nForward primer (“515F”): GAGTGYCAGCMGCCGCGGTAA.\nReverse primer (“806R”): TTACCGCGGCKGCTGRCACTC.\nGrading information\n….\nGeneral information\nFor each numbered step below, you should create at least one Git commit.\n\nGetting set up\nCreate a new dir for this assignment, and inside it, initialize a Git repository.\nCopy the FASTQ files from /fs/ess/PAS1855/data/week05/fastq into a directory data/fastq/ inside your assignment’s directory.\nCreate a .gitignore file and add a line to make Git ignore all .fastq files.\nCreate a conda environment for Cutadapt following these instructions (i.e. just the section “Installation with conda”).\nExport the environment description for your Cutadapt environment to a .yml file.\n\nA script to run Cutadapt on one pair of FASTQ files\nNow, you will write a script called cutadapt_single.sh that runs Cutadapt for one sample, i.e. one pair of FASTQ files: a file with forward (R1) reads and a file with reverse (R2) reads for that sample.\nThe following instructions all refer to what you should write inside the script:\nStart with the shebang line followed by SLURM directives. Specify at least the following SLURM directives:\nThe class’s OSC project number, PAS1855.\nA 20-minute wall-time limit.\nExplicitly ask for one node, one process (task), and one core (these are three separate directives).\n\nNext, include the familiar set settings for robust bash scripts, load OSC’s conda module and then activate your own Cutadapt conda environment (use source activate and not conda activate!).\nLet the script take 4 arguments that can be passed to it on the command-line:\nThe path to a FASTQ file with forward reads (whose value when passed from the command-line will e.g. be data/fastq/201-S4-V4-V5_S53_L001_R1_001.fastq).\nThe name of the output directory for trimmed FASTQ files (whose value will be whatever you pick, e.g. results/trim).\nThe sequence of the forward primer to be removed (whose value will be GAGTGYCAGCMGCCGCGGTAA in this case).\nThe sequence of the reverse primer to be removed (whose value will be TTACCGCGGCKGCTGRCACTC in this case).\nGive each of these variables a descriptive name rather than directly using the placeholder variables ($1, etc) – that will make your life easier when writing the rest of the script.\nCompute the reverse complement for each primer. Use tr to compute the complement and then pipe this into rev (a command we haven’t seen yet; but you can simply use rev with no options).\nMake sure you don’t just translate ACTG but also take all the possible IUPAC ambiguity codes into account! See here for a complete overview of these ambiguity codes.\nHints\n\nUse command substitution to save the reverse complement primer sequences.\nDo some testing of your commands to see if the reverse complements come out correctly!\n\n\nFrom the file name of the input FASTQ file with forward reads (which is one of the arguments to the script), infer the name of the corresponding FASTQ file with reverse reads, which will have an identical name except that _R1_ is replaced by _R2_.\nAssign output file paths (output dir + file name) for the R1 and R2 output file, inserting _trimmed before the file extension, which you can assume to be .fastq. (i.e., the output file paths should be along the lines of <output-dir>/<old-file-name>_trimmed.fastq).\nHints\n\n\nYou’ll do yourself a favor by having the script echo the file names that you have assigned, so you can easily check if you’re doing this right.\nMoreover, I recommend that you test whether you’re getting all the file names right interactively: assign one of the paths to the actual FASTQ files to the variable name you’re using for that, and ditto with an output dir. Just take care that these assignments don’t end up in your (final) script.\n\n\nCreate the output directory if it doesn’t already exist.\nThe actual call to the Cutadapt program should be as follows – just change any variable names as needed:\n$ cutadapt -a \"$primer_f\"...\"$primer_r_revcomp\" \\\n    -A \"$primer_r\"...\"$primer_f_revcomp\" \\\n    --discard-untrimmed --pair-filter=any \\\n    -o \"$R1_out\" -p \"$R2_out\" \"$R1_in\" \"$R2_in\"\nOptional (ungraded): Touch up the scripts with additional echo statements, date commands, and tests such as whether 4 arguments were provided.\n\nRunning the scrip and finishing up\nMake the script executable and submit the script as a SLURM job. Check the SLURM log file and the output files. If it didn’t work, troubleshoot until you get it working.\nDo any necessary cleaning up of files, e.g. move your SLURM log file to an appropriate place, and make sure everything is committed to the Git repository.\nCreate a GitHub repository and push your local Git repository to GitHub. Like last time, start an issue and inside it, tag @jelmerp.\n\nOptional (ungraded) - A script to loop over all samples\nCreating a script like we did above mostly makes sense if we plan to run it for multiple/many samples. Now, you will create a second script cutadapt_submit.sh that loops over all FASTQ files in a specified directory. It doesn’t need to be a “proper” script with a robust header and so on, and shouldn’t contain any SLURM directives: this script merely functions to submit SLURM jobs and can be run interactively.\nLoop over a globbing pattern that accepts all .fastq files with R1 in the name in the input directory (recall: we don’t want the R2 files included as they will be automatically added in our previous script).\nInside the loop, the cutadapt_single.sh script should be submitted as a SLURM job, similar to your single submission of the script above.\nRun the loop.\nCheck the SLURM log files and the output directory. If it didn’t work, remove all these files, troubleshoot, and try again until it is working.\n\n\n\n",
      "last_modified": "2021-02-09T10:40:07-05:00"
=======
      "contents": "\nExercise X\nNow, we’ll use the same script template to run FastQC, a ubiquitous piece of software that performs quality control (QC) checks on a FASTQ file.The syntax for FastQC is as follows:\nfastqc –outdir=\nFor instance:\nfastqc –outdir=results/QC/ data/seqs/sampleA.fastq\nModify the line in the script that calls fastq_stat to one that calls FastQC.\n\n\n\n",
      "last_modified": "2021-02-09T15:22:09-05:00"
>>>>>>> main
    }
  ],
  "collections": ["posts/posts.json"]
}
