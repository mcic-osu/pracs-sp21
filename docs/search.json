{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "author": [],
      "contents": "\nThis is the Github website for the course Practical Computing Skills for Biologists (a section of PP8300 – Current Topics in Plant Pathology), a 2-credit online-only course at Ohio State University during the Spring semester of 2021. The course is taught by Jelmer Poelstra from the MCIC.\nOnly slide decks, code-along materials, and exercises are hosted on this website. If you are a student in this course, your starting point should always be the CarmenCanvas site for this course. (Note that you can always find the link to the CarmenCanvas site in the top-right corner of this site by clicking on the graduation cap icon.)\n\n\n\n",
      "last_modified": "2021-01-11T21:03:04-05:00"
    },
    {
      "path": "index.html",
      "title": "",
      "author": [],
      "contents": "\n\n\nPractical Computing Skills for Biologists  Spring 2021  A section of Current Topics in Plant Pathology (PLNTPTH 8300)\n\n\n\n\n",
      "last_modified": "2021-01-11T21:03:04-05:00"
    },
    {
      "path": "w01_exercises.html",
      "title": "Week 1 Exercises",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nGetting set up\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\nSolutions\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\n\nThe following exercises were copied from Chapter 1 of the CSB book, with hints and solutions modified from those provided in the book’s Github repo.\n(The exercises marked as “Advanced” are omitted since they require topics not actually covered in the chapter, which we will cover in later modules.)\n\nMain exercises\nGetting set up\nYou should already have the book’s Github repository with exercise files.\nIf not, go to /fs/ess/PAS1855/users/$USER, and run git clone https://github.com/CSB-book/CSB.git.\nNow, you should have the CSB directory referred to in the first step of the first exercise.\nIntermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\nShow hints\nRecall that .. is one level up in the dir tree, and that you can combine multiple .. in a single statement.\n\n(d) Use an absolute path to go to the sandbox directory within python.\n(e) Return to the data directory within the python directory.\nShow hints\nRecall that there is a shortcut to return to your most recent working dir.\n\nIntermezzo 1.2\nTo familiarize yourself with these basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\nShow hints\nThe touch command creates an empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt.\nIntermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? Which is the last?\nShow hints\nYou can either first select the 5th column using cut, and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter.\nTo view just the first or the last line, pipe to head or tail.\n\n(b) How many families are represented in the database?\nShow hints\nStart by selecting the relevant column.\nUse a tail trick shown in the chapter to exclude the first line.\nRemember to sort before using uniq.\n1.10.1 Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (>).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\nShow hints\nTo show the size of a file you can use the -l option of the command ls, and to display human-readable file sizes, the -h option.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\nShow hints\nRecall that with the cp command, it is also possible to give the copy a new name at once.\n\n4. How many contigs are classified as isogroup00036?\nShow hints\nIs there a grep option that counts the number of occurrences?\nAlternatively, you can pass the output of grep to wc -l.\n\n5. Replace the original “two-spaces” delimiter with a comma.\nShow hints\nIn the file, the information on each contig is separated by two spaces:\n>contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n>contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\nIn Linux, we can’t write output to a file that also serves as input (see here), so the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" > myfile.txt # Don't do this! \nIn this case, we will have to save the output in a temporary file and on a separate line, overwrite the original file using mv.\n\n6. How many unique isogroups are in the file?\nShow hints\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nShow hints\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically.\n\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nHow many times were the levels of individuals 3 and 27 recorded?\nShow hints\nYou can use cut to extract just the maleID from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”.\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nShow hints\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written.\nIn the script, you can use the first two so-called positional parameters, $1 and $2, to represent the file name and the maleID, respectively.\n$1 and $2 represent the first and the second argument passed to a script like so: bash myscript.sh arg1 arg2.\n1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\n$ bash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\nShow hints\nTo build the script, you need to combine several commands:\nTo find the number of rows, you can use wc.\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n, and count characters.\n1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nFor example, running the script with:\n$ bash explore.sh ../data/Buzzard2015_data.csv 7\nshould return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\nShow hints\nYou can select a given column from a csv file using the command cut. Then:\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the positional parameters $1 and $2 for the file name and column number, respectively.\n\n\nSolutions\nIntermezzo 1.1\nSolution\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER.\n$ cd /fs/ess/PAS1855/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS1855/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\ncd -\n\nIntermezzo 1.2\nSolution\n(a) Go to the data directory within CSB/unix.\n$ cd /fs/ess/PAS1855/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory         without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\nIntermezzo 1.3\nSolution\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? And which is the last?\ncd ∼/CSB/unix/data/\n\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\n\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\n\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t\";\" -k 5 Pacifici2013_data.csv | head -n 1\n\n\nFollowing the output that you wanted, you may have gotten errors like this:\nsort: write failed: 'standard output': Broken pipe\nsort: write error\nThis may seem disconcerting, but is nothing to worry about, and has to do with the way data streams through a pipe: after head/tail exits because it has done what was asked (print one line), sort or cut may still try to pass on data.\n\n\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n\n1.10.1 Next-Generation Sequencing Data\n1. Change directory to CSB/unix/sandbox.\ncd /fs/ess/PAS1855/users/$USER/CSB/unix/sandbox\n\n2. What is the size of the file Marra2014_data.fasta?\nls -lh ../data/Marra2014_data.fasta\n# Among other output, this should show a file size of 553K\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\ncp ../data/Marra2014_data.fasta my_file.fasta\nTo make sure the copy went well, list the files in the sandbox:\nls\n\n4. How many contigs are classified as isogroup00036?\nTo count the occurrences of a given string, use grep with the option -c\ngrep -c isogroup00036 my_file.fasta \n# 16\nYou can also use a pipe and wc -l to count:\ngrep isogroup00036 my_file.fasta | wc -l\n\n5. Replace the original “two-spaces” delimiter with a comma.\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,:\ncat my_file.fasta | tr -s ' ' ',' > my_file.tmp\nmv my_file.tmp my_file.fasta\n\n6. How many unique isogroups are in the file?\nFirst, searching for > with grep will extract all lines with contig information:\ngrep '>' my_file.fasta | head -n 2\n# >contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n# >contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, use cut to extract the 4th column\ngrep '>' my_file.fasta | cut -d ',' -f 4 | head -n 2\n#gene=isogroup00001\n#gene=isogroup00001\nFinally, use sort -> uniq -> wc -l to count the number of unique occurrences:\ngrep '>' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n# 43\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nWe need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n# >contig00001,numreads=2\n# >contig00002,numreads=8\n# >contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t ‘=’ to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n# >contig00089,numreads=1\n# >contig00176,numreads=1\n# >contig00210,numreads=1\n# >contig00001,numreads=2\n# >contig00003,numreads=2\nUsing the flag -r we can sort in reverse order:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n# >contig00302,numreads=3330\nFinding that contig 00302 has the highest coverage, with 3330 reads.\n\n\n1.10.2 Hormone Levels in Baboons\n1. How many times were the levels of individuals 3 and 27 recorded?\nFirst, let’s see the structure of the file:\nhead -n 3 ../data/Gesquiere2011_data.csv \n# maleID        GC      T\n# 1     66.9    64.57\n# 1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 ../data/Gesquiere2011_data.csv | head -n 3\n# maleID\n# 1\n# 1\nThen we can pipe the results to grep -c to count the number of occurrences (note the flag -w as we want to match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 3\n# 61\n\n# For maleID 27\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 27\n# 5\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nIn the script, we just need to incorporate the arguments given when calling the script, using $1 for the file name and $2 for the individual ID, into the commands that we used above:\ncut -f 1 $1 | grep -c -w $2\nA slightly more verbose and readable example:\n#!/bin/bash\n\nfilename=$1\nind_ID=$2\necho \"Counting the nr of occurrences of individual ${ind_ID} in file ${filename}:\" \ncut -f 1 ${filename} | grep -c -w ${ind_ID}\n\n\nVariables are assigned using name=value (no dollar sign!), and recalled using $name or ${name}. It is good practice to put curly braces around the variable name. We will talk more about bash variables in the next few weeks.\n\n\nTo run the script, assuming it is named count_baboons.sh:\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n1.10.3 Plant–Pollinator Networks\nSolution\nCounting rows:\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\nTo avoid printing the file name, we can either use cat or input redirection:\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l < ../data/Saavedra2013/n10.txt \nCounting rows:\nCounting the number of columns is more work. First, we need only the first line:\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\nNow we can remove all spaces and the line terminator using tr:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\nFinally, we can use wc -c to count the number of characters in the string:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nFinal script:\n#!/bin/bash\n\nfilename=$1\n\necho \"Filename:\"\necho ${filename}\necho \"Number of rows:\"\ncat ${filename} | wc -l\necho \"Number of columns:\"\nhead -n 1 ${filename} | tr -d ' ' | tr -d '\\n' | wc -c\nTo run the script, assuming it is named counter.sh:\nbash counter.sh ../data/Saavedra2013/n10.txt\n# 5\n\n\nWe’ll learn about a quicker and more general way to count columns in a few weeks.\n\n\n\n1.10.4 Data Explorer\nSolution\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\nHere is an example of what the script could look like:\n#!/bin/bash\n\nfilename=$1\ncolumn_nr=$2\n\necho \"Column name\"\ncut -d ',' -f ${column_nr} ${filename} | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n",
      "last_modified": "2021-01-11T21:03:05-05:00"
    },
    {
      "path": "w02_exercises.html",
      "title": "Exercises: Week 2",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nExercise 1: Course notes in Markdown\nExercise 2\n\nBonus exercises\nExercise 3\nBuffalo Chapter 3 code-along\n\nSolutions\nExercise 2\n\n\nMain exercises\nExercise 1: Course notes in Markdown\nCreate a Markdown document with course notes. I recommend writing this document in VS Code.\nCover this week’s material in some detail. If you made notes last week in another format, include those too. (And try to keep using this document throughout the course!)\nSome pointers:\nUse several header levels and use them consistently: e.g. a level 1 header (#) for the document’s title, level 2 headers (##) for each week, and so on.\nThough it should foremost be a functional document for notes, try to incorporate any appropriate formatting option: e.g. bold text, italic text, inline code, code blocks, ordered/unordered lists, hyperlinks, and perhaps a figure.\nYou may even want to try your hand at a table in Markdown (e.g. for an overview of certain commands), though admittedly, the syntax for a table is not particularly graceful or easy to use.\nMake sure you know how to start a new paragraph and how to force a newline.\nExercise 2\nWhile doing this exercise, save the commands you use in a text document – either write in a text document in VS Code and send the commands to the terminal, or copy them into a text document later.\nGetting set up\nCreate a directory for this exercise, and change your working dir to go there. You can do this either in your $HOME dir (e.g. ~/pracs-sp21/w02/ex2/) or your dir in the project dir (/fs/ess/PAS1855/users/$USER/w02/ex2/).\nCreate a disorganized mock project\nUsing the touch command and brace expansions, create a mock project by creating 100s of empty files, either in a single directory or a disorganized dir structure.\nIf you want, you can create file types according to what you typically have in your project — otherwise, create files with:\nRaw data (e.g. .fastq.gz)\nReference data (e.g. .fasta),\nMetadata (e.g. .txt or .csv)\nProcessed data and results (e.g. .bam, .out)\nScripts (e.g. .sh, .py or .R)\nFigures (e.g. .png or .eps)\nNotes (.txt and/or .md)\nPerhaps some other file type you usually have in your projects.\n\nOrganize the mock project\nOrganize the mock project according to some of the principles we discussed this week.\nEven while adhering to these principles, there is plenty of wiggle room and no single perfect dir structure: what is optimal will depend on what works for you and on the project size and structure. Therefore, think about what makes sense to you, and what makes sense given the files you find yourself with.\nTry to use as few commands as possible to move the files – use wildcards!\nChange file permissions\nChange permissions to what you think is reasonable; but in any case, make sure no-one has write permissions for the raw data files.\nCreate mock alignment files\nCreate a directory alignment inside an appropriate dir in your project (e.g. analysis, results, or a dir for processed data), and create files for all combinations of 30 samples (01-30), 5 treatments (A-E), and 2 dates (08-14-2020 and 09-16-2020): sample01_A_08-14-2020.sam - sample50_H_09-16-2020.sam. These 300 files can be created with a single touch command.\nRename files in a batch\nWoops! We stored the alignment files that we created in the previous step as SAM files (.sam), but this was a mistake – the files are actually the binary counterparts of SAM files: BAM files (.bam).\nMove into the dir with your misnamed BAM files, and use a for loop to rename them: change the extension from .sam to .bam.\nHints\n\nThis can be most easily done with the basename command.\n\n\nCopy files with wildcards\nStill in the dir with your SAM files, create a new dir subset. Then, using a single cp command, copy files that satisfy the following conditions into the subset dir:\nThe sample number should be 01-19;\nThe treatment should be A, B, or C. Create a README.md in the dir that explains what you did.\nHints\n\nHint: you can use two consecutive character sets.\n\n\nBonus: a trickier wildcard selection\nStill in the dir with your SAM files, create a new dir subset2. Then, copy all files except the one for “sample28” into this dir. Do so using a single cp command, though you’ll need two separate wildcard expansion or brace expansion arguments (as in cp wildcard-selection1 wildcard-selection2 destination/).\nBonus: a trickier renaming loop\nYou now realize that your date format is suboptimal (MM-DD-YYYY to give 08-14-2020 and 09-16-2020) and that you should use the YYYY-MM-DD format. Use a for loop to rename the files.\nHints\n\nTest your commands by echo-ing and processing a single file name as an example.\n\n\nCreate a README\nInclude a README.md that described what you did; again, try to get familiar with Markdown syntax by using formatting liberally.\nBonus exercises\nExercise 3\nIf you feel like it would be good to reorganize one of your own projects, you can do so using what you’ve learned this week. But make sure you create a backup copy of the entire project first!\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3.\n\nSolutions\nExercise 2\n1. Getting set up\nmkdir ~/pracs-sp21/w02/ex2/ # or similar, whatever dir you chose\ncd !$                       # !$ is a shortcut to recall the last argument from the last commands\n\n2. Create a disorganized mock project\nAn example:\ntouch sample{001..150}_{F,R}.fastq.gz\ntouch ref.fasta ref.fai\ntouch sample_info.csv sequence_barcodes.txt\ntouch sample{001..150}.{bam,.bam.bai,_fastqc.zip,_fastqc.html}.bam.bai gene-counts.tsv DE-results.txt GO-out.txt\ntouch fastqc.sh multiqc.sh align.sh sort_bam.sh count1.py count2.py DE.R GO.R KEGG.R\ntouch Fig{1..5}.png all_qc_plots.eps weird-sample.png\ntouch dontforget.txt README.md README_DE.md tmp5.txt\ntouch slurm-84789570.out slurm-84789571.out slurm-84789572.out\n\n3. Organize the mock project\nAn example:\nCreate directories:\nmkdir -p data/{raw,meta,ref}\nmkdir results/{alignment,counts,DE,enrichment,logfiles,qc/figures}\nmkdir -p scripts\nmkdir -p figures/{ms,sandbox}\nmkdir -p doc/misc\nMove files:\nmv *fastq.gz data/raw/\nmv ref.fa* data/ref/\nmv sample_info.csv sequence_barcodes.txt data/meta/\nmv *.bam *.bam.bai results/alignment/\nmv *fastqc* results/qc/\nmv gene_counts.tsv results/counts/\nmv DE-results.txt results/DE/\nmv GO-out.txt results/enrichment/\nmv *.sh *.R *.py scripts/\nmv README_DE.md results/DE/\nmv Fig[0-9]* figures/ms\nmv weird-sample.png figures/sandbox\nmv all_qc_plots.eps results/qc/figures/\nmv dontforget.txt tmp5.txt doc/misc/\nmv slurm* results/logfiles/\n\n4. Change file permissions\nTo ensure that no-one has write permission for the raw data, you could, for example, use:\nchmod a=r data/raw/*\n\nchmod a-w data/raw/*\n\n5. Create mock alignment files\n$ mkdir -p results/alignment\n$ # rm results/alignment/* # In the example above, we already had such a dir with files\n\n# Create the files:\n$ touch sample{01..30}_{A..E}_{08-14-2020,09-16-2020}.sam\n\n# Check if we have 400 files:\n$ ls | wc -l\n# 400\n\n6. Rename files in a batch\n# Move into the directory so we don't have to deal with dirname:\ncd results/alignment\n\n# Use *globbing* to match the files to loop over (rather than `ls`):\nfor oldname in *.sam\ndo\n   # Remove the `sam` suffix using `basename $oldname sam`,\n   # use command substitution (`$()` syntax) to catch the output of the\n   # `basename` command, and paste `bam` at the end:\n   newname=$(basename $oldname sam)bam\n   \n   # Report what we have:\n   # (Using `-e` with echo we can print an extra newline with '\\n`,\n   # to separate files by an empty line)\n   echo \"Old name: ${oldname}\"\n   echo -e \"New name: ${newname} \\n\"\n   \n   # Execute the move:\n   mv ${oldname} ${newname}\ndone\nA couple of take-aways:\nNote that we don’t need a special construction to paste strings together. we simply type bam after what will be the extension-less file name from the basename command.\nWe print the old and new names to screen; this is not necessary, of course, but good practice. Moreover, this way we can test whether our loop works before adding the mv command.\nWe use informative variable names (oldname and newname), not cryptic ones like i and o.\n7. Copy files with wildcards\nCreate the new dir:\nmkdir subset\nCopy the files using three consecutive wildcard selections:\ncp sample[0-1][0-9]_[A-C]* subset/\nReport what we did, including a command substitution to insert the current date:\necho \"On $(date), created a dir \"subset\" and copied only files for samples 1-29 \\\nand treatments A-D into this dir\" > subset/README.md\n\n8. Bonus: a trickier wildcard selection\nCreate the new dir:\nmkdir subset2\nThe most straightforward way in this case is using two brace expansion selections, one for sample numbers smaller than 28, and one for sample numbers larger than 28:\ncp sample{01..27}* sample{29..30}* subset2/\nHowever, we may not always be able to use ranges, and being a little creative with wildcard expansion also works — first we select all samples not starting with a 2, and then among samples that do start with a 2, we exclude 28:\ncp sample?[^2]* sample2[^8]* subset2/\n\n9. Bonus: a trickier renaming loop\nfor oldname in sample01*.bam\ndo\n   # Use `cut` to extract month, day, year, and a \"prefix\" that contains\n   # the sample number and the treatment, and save these using command substitution:\n   month=$(echo ${oldname} | cut -d \"_\" -f 3 | cut -d \"-\" -f 1)\n   day=$(echo ${oldname} | cut -d \"_\" -f 3 | cut -d \"-\" -f 2)\n   year=$(basename ${oldname} .bam | cut -d \"_\" -f 3 | cut -d \"-\" -f 3)\n   prefix=$(echo ${oldname} | cut -d \"_\" -f 1-2)\n   \n   # Paste together the new name:\n   newname=${prefix}_${year}-${month}-${day}.bam\n   \n   # Report what we have:\n   echo \"Old name: ${oldname}\"\n   echo -e \"New name: ${newname} \\n\"\n   \n   # Execute the move:\n   mv ${oldname} ${newname}\ndone\n\n\nThis can be done more succinctly using regular expression and the sed command, both of which we’ll learn about later in the course.\n\n\n\n\n\n\n",
      "last_modified": "2021-01-11T21:06:44-05:00"
    },
    {
      "path": "w02_UA_github-signup.html",
      "author": [],
      "contents": "\nWhat?\nCreate a Github account and let me know you’ve done so.\nWhy?\nGithub is a website that hosts git repositories, i.e. version-controlled projects. In Module 3 of this course, we will be learning how to use git together with Github. Also, all graded assignments for this course will be submitted through Github. I will need to know your Github user names in advance to set up the infrastructure for this.\nHow?\nIf you already have a Github account, log in and start at step 6.\nGo to https://github.com. Click “Sign Up” in the top right. Fill out the form: When choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use Github to share your code, for instance when publishing a paper.) You can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.) You probably want to uncheck the box under Email Preferences. When you’re done, click “Create account”. You can answer the questions Github will now ask you, but you should also be able to just skip them. Check your email and click the link to verify your email address. In the far top-right of the page back on Github, click your randomly assigned avatar, and in the dropdown menu, click “Settings”. In the Emails tab (left-hand menu), deselect the box “Keep my email addresses private”. In the Profile tab, enter your Name. Still in the Profile tab, upload an avatar. This can be a picture of yourself but if you prefer, you can use something else. Here at this CarmenCanvas assignment, submit the link to your Github profile, which will be “https://github.com/”.\n\n\n\n",
      "last_modified": "2021-01-11T21:03:06-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
