{
  "articles": [
    {
      "path": "w01_exercises.html",
      "title": "Week 1 Exercises",
      "author": [],
      "contents": "\nThe following exercises were copied from Chapter 1 of the CSB book. (The exercises marked as “Advanced” are omitted since they require topics not actually covered in the chapter, which we will cover in later modules.) \nYou should already have the book’s Github repository with exercise files.\nIf not, go to /fs/ess/PAS1855/users/$USER, and run git clone https://github.com/CSB-book/CSB.git. Then, you should have the CSB directory referred to in the first step of the first exercise.\nIntermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\nShow hints\nRecall that .. is one level up in the dir tree, and that you can combine multiple .. in a single statement.\n\n(d) Use an absolute path to go to the sandbox directory within python.\n(e) Return to the data directory within the python directory.\nShow hints\nRecall that there is a shortcut to return to your most recent working dir.\n\nIntermezzo 1.2\nTo familiarize yourself with these basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\nShow hints\nThe touch command creates an empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt.\nIntermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? Which is the last?\n(b) How many families are represented in the database?\n1.10.1 Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (>).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\nShow hints\nTo show the size of a file you can use the -l option of the command ls, and to display human-readable file sizes, the -h option.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\nShow hint\nRecall that with the cp command, it is also possible to give the copy a new name at once.\n\n4. How many contigs are classified as isogroup00036?\nShow hint\nIs there a grep option that counts the number of occurrences?\nAlternatively, you can pass the output of grep to wc -l.\n\n5. Replace the original “two-spaces” delimiter with a comma.\nShow hint\nIn the file, the information on each contig is separated by two spaces:\n>contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n>contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\nIn Linux, we can’t write output to a file that also serves as input (see here), so the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" > myfile.txt # Don't do this! \nIn this case, we will have to save the output in a temporary file and on a separate line, overwrite the original file using mv.\n\n6. How many unique isogroups are in the file?\nShow hint\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nShow hint\nAgain, you can use a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically.\n\n\nOptional exercises\n1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nHow many times were the levels of individuals 3 and 27 recorded?\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nHints\nYou can use cut to extract just the maleID from the file; to match the individual 3 (or 27), you can use grep.\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written. Now you can use generic variables $1 and $2 as placeholders for the file name and the maleID; you can pass these arguments to the command line.\nFirst, you need to use cut in combination with tail, sort and uniq to extract all the distinct maleID. Then the main hurdle is to take the output of this pipeline and use it as input for a for loop in bash. The following example should help:\n#!/bin/bash\n\n# to store the results of a command into a variable,\n# surround the command with backticks (`)\nmyfiles=`ls`\nfor myfile in $myIDS\ndo\n    echo $myfile\ndone\n\n1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\n$ bash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\nHints\nTo build the script, you need to combine several commands:\nTo find the number of rows, you can use wc\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n and count characters.\nTo combine them in a script, you need to assign the result of a command to a variable. For example: mytxtfiles=`ls *.txt` stores the list of txt files in the variable mytxtfiles.\n\nYou need to write a for loop in a bash script. This might help:\n!#/bin/bash\n\nmyfiles=`ls *.txt`\nfor ff in myfiles\ndo\necho $ff\ndone\nYou can now use the script you’ve just written in combination with sort to answer the questions (remember the option -k to choose a column and -r to reverse the order).\n1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nFor example, running the script with:\n$ bash explore.sh ../data/Buzzard2015_data.csv 7\nshould return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\nHints\nYou can select a given column from a csv file using the command cut. Then:\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the generic placeholders $1 and $2 for the file name and column number, respectively.\n\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3.\n\nSolutions\nIntermezzo 1.1\nSolutions\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER.\n$ cd /fs/ess/PAS1855/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS1855/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\ncd -\n\nIntermezzo 1.2\nSolutions\n(a) Go to the data directory within CSB/unix.\n$ cd /fs/ess/PAS1855/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory         without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\nIntermezzo 1.3\nSolutions\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\n      in alphabetical order, which is the first species? Which is the last?\ncd ∼/CSB/unix/data/\n\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\n\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n\n1.10.1 Next-Generation Sequencing Data\n1. Change directory to CSB/unix/sandbox.\ncd /fs/ess/PAS1855/users/$USER/CSB/unix/sandbox\n\n2. What is the size of the file Marra2014_data.fasta?\nls -lh ../data/Marra2014_data.fasta\n# Among other output, this should show a file size of 553K\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\ncp ../data/Marra2014_data.fasta my_file.fasta\nTo make sure the copy went well, list the files in the sandbox:\nls\n\n4. How many contigs are classified as isogroup00036?\nTo count the occurrences of a given string, use grep with the option -c\ngrep -c isogroup00036 my_file.fasta \n# 16\nYou can also use a pipe and wc -l to count:\ngrep isogroup00036 my_file.fasta | wc -l\n\n5. Replace the original “two-spaces” delimiter with a comma.\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,:\ncat my_file.fasta | tr -s ' ' ',' > my_file.tmp\nmv my_file.tmp my_file.fasta\n\n6. How many unique isogroups are in the file?\nWe need to extract the isogroupXXXXX information for each contig, sort them, and use uniq to remove duplicates.\nFirst, using grep ‘>’ my_file.fasta extracts all the lines with contig information:\ngrep '>' my_file.fasta | head -n 2\n# >contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n# >contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow use cut to extract the 4th column\ngrep '>' my_file.fasta | cut -d ',' -f 4 | head -n 2\n#gene=isogroup00001\n#gene=isogroup00001\nFinally, use sort | uniq | wc -l to count the number of unique occurrences.\ngrep '>' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n# 43\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nWe need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n# >contig00001,numreads=2\n# >contig00002,numreads=8\n# >contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t ‘=’ to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n# >contig00089,numreads=1\n# >contig00176,numreads=1\n# >contig00210,numreads=1\n# >contig00001,numreads=2\n# >contig00003,numreads=2\nUsing the flag -r we can sort in reverse order:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n# >contig00302,numreads=3330\nFinding that contig 00302 has the highest coverage, with 3330 reads.\n\n\n1.10.2 Hormone Levels in Baboons\n1. How many times were the levels of individuals 3 and 27 recorded?\nFirst, let’s see the structure of the file:\nhead -n 3 ../data/Gesquiere2011_data.csv \n# maleID        GC      T\n# 1     66.9    64.57\n# 1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them.\nTo extract only the first column, we can use cut:\ncut -f 1 ../data/Gesquiere2011_data.csv | head -n 3\n# maleID\n# 1\n# 1\nThen we can pipe the results to grep -c to count the number of occurrences (note the flag -w as we want to match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 3\n# 61\n\n# For maleID 27\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 27\n# 5\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nIn the script, we just need to a) read the input from the command line (using $1 for filename and $2 for ID), and then run the commands above:\n#!/bin/bash\ncut -f 1 $1 | grep -c -w $2\nTo run the script:\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n1.10.3 Plant–Pollinator Networks\nSolution\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\nTo avoid printing the file name, we can either use cat or input redirection:\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l < ../data/Saavedra2013/n10.txt \nCounting the number of columns is more work. First, we need only the first line:\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\nNow we can remove all spaces and the line terminator using tr:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\nFinally, we can use wc -c to count the number of characters in the string:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nArmed with this information, we can write our script:\n#!/bin/bash\n\necho \"Filename:\"\necho $1\necho \"Number of rows:\"\ncat $1 | wc -l\necho \"Number of columns:\"\nhead -n 1 $1 | tr -d ' ' | tr -d '\\n' | wc -c\n\n1.10.4 Data Explorer\nSolution\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\nHere is an example of what the script could look like:\n#!/bin/bash\n\n# $1 is the file name\n# $2 is the column of interest\n\necho \"Column name\"\ncut -d ',' -f $2 $1 | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f $2 $1 | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f $2 $1 | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f $2 $1 | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n",
      "last_modified": "2021-01-11T08:51:39-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
