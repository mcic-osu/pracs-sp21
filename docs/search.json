{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site and course",
      "author": [],
      "contents": "\nThis is the GitHub website for the course Practical Computing Skills for Biologists, a 2-credit online-only course at Ohio State University during the Spring semester of 2021. The course is taught as a section of Current Topics in Plant Pathology (PLNTPTH8300) by Jelmer Poelstra from the MCIC.\nCourse description\nAs datasets have rapidly grown larger in biology, coding has been recognized as an increasingly important skill for biologists. Yet in fact, basic coding skills and “computational thinking” are highly beneficial for any scientist, paving the way for more efficient, robust, and reproducible research.\nIn this course, students will gain hands-on experience with a set of general and versatile tools for day-to-day work with data sets small and large alike. The course’s focus is on foundational skills such as working in the Unix shell, coding in Python, organizing, documenting, version-controlling and sharing research, submitting jobs to a compute cluster, and building flexible workflows. Taken together, this will allow students to reproduce their own work, and have others reproduce their work, with as little as a single command.\nMore information\nSee also the page Practical course information and the full syllabus (PDF).\nTopics taught\nShell basics and tools and Shell scripting (4 modules)\nCoding in Python (5 modules)\nComputing at OSC with SLURM and Conda (1 module)\nVersion control with Git and GitHub (1 module, applied throughout)\nProject documentation with Markdown and project organization (1 module, applied throughout)\nReproducible workflows with Snakemake (1 module)\nCourse books\nPrimary: Allesina S, Wilmes M (2019). Computing Skills for Biologists. Princeton UP.\nSecondary: Buffalo V (2015). Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media, Inc.\nCarmenCanvas website\nIf you are a student in this course, you should also refer to the CarmenCanvas site for this course.1\n\nYou can always find the link to the CarmenCanvas site in the top-right corner of this site by clicking on the graduation cap icon.↩︎\n",
      "last_modified": "2021-04-25T17:51:53-04:00"
    },
    {
      "path": "final_project.html",
      "title": "General information about your final project",
      "author": [],
      "contents": "\n\nContents\nDevising a project\nGraded aspects\nSteps (checkpoints)\n\nDevising a project\nThe goal for your final project is to apply some of the things you have learned during this course, and allow you to get you more practice. While some aspects are required in order to get a good grade (see Graded aspects below), you have a fair amount of freedom. I recommend that you take advantage of that to make your final project as useful as possible for your own research and/or personal development.\nGraded aspects for the project focus on documentation, reproducibility, and automation. Accordingly, there are no real requirements for the level of complexity or sophistication, the number of scripts, the real-world usefulness, and so on. I would in fact recommend you take care not to be too ambitious in this regard: start small and then expand if you are able to.\nSome examples of possible types of projects:\nYou may want to work with your own data or a publicly available dataset that is similar to what you are expecting to work with (though subsetting the data will be useful if you have a large genomic dataset!).\nYou may decide that just focusing on the coding and on creating a reproducible workflow is what is most useful for you, and that this may be easier with a trivial and maybe not-so-relevant dataset so you don’t get bogged down in other details.\nYou may think of trying to automate something “boring” and/or repetitive that you did manually until now.\nIf you would like to work with publicly available fungal genomic data, you should consider using your TA Zach’s MycoTools. You can find some general information in his README and more details usage information in his usage guide. If this looks interesting to you, contact Zach to get more information and to develop a project idea.\nIf you need help with selecting a dataset or a project topic, don’t hesitate to contact me (Jelmer). I don’t currently have any ready-to-go project ideas with accompanying data sets, but I could make one or more, if needed.\n\nGraded aspects\nGraded aspects of your project focus on appropriate usage of many of the tools and principles we covered during the course.\nTo receive a high grade, your project should:\nBe well-organized: contained in a single parent directory with a clear and sensible structure of subdirectories, descriptive file and directory names, no files floating around with unclear purpose or source, and so on.\nBe well-documented, with at least one README in Markdown format in the root directory of your project, and preferably with additional READMEs elsewhere as appropriate.\nBe version-controlled with Git throughout: with regular, meaningful commits. You will also need to push to GitHub for the proposal, draft, and final submission checkpoint.\nContain scripts in Bash and/or Python that do data processing and/or analysis (see also the last point). Try not to do any manual work such as editing a data file in a text editor or Excel to fix column names, since this hinders reproducibility.\nHaving some components in other languages is fine, in case you know how to do things there that we didn’t learn in the course (for example, doing some plotting in R).\nSimilarly, it is fine to call external, command-line programs (e.g. some of the bioinformatics programs we have run, or any program that may be useful for your research).\nIf your project’s heavy lifting consists mostly or almost entirely of calling such programs, though, it becomes especially important that you are creating a reproducible and easily rerunnable pipeline (see below).\n\nRun one or more scripts as SLURM jobs at OSC.\n(A locally run project could be okay if your research does not and will not require the use of supercomputers like those at OSC – check in with me if this is the case and you would therefore prefer not to use OSC.)\nBe easily re-runnable using a Snakefile or a “master” / “controller” script that glues the entire project together.\nIt is not enough to include a README.md that explains what you did or that provides instructions how to rerun the project, even if in detail. Such information is very useful, but the goal here is to provide a way to rerun the project with a single command – a command that I could also run.\nPreferably, this is done using single Snakemake control script (“Snakefile”), but a shell or Python script may also work.\n(This is a challenging aspect, and your grade won’t plummet if you don’t succeed, but it is important to try!)\n\nSteps (checkpoints)\nInformation about expectations for each of the steps (checkpoints) for the project will be provided in the content for individual weeks. Below is an overview which will be updated with the links:\nProposal – due March 23\nDraft – due April 13\nIn-class presentations – April 20 and 22\nFinal submission – due April 30\n\n\n\n",
      "last_modified": "2021-04-25T17:51:53-04:00"
    },
    {
      "path": "finalproject_present.html",
      "title": "Final project: presentations",
      "author": [],
      "contents": "\nEvery student is expected to give a 10-minute presentation about their final project during the Zoom sessions on April 20th and 22nd. [10 points]\n\nA few pointers:\nAim for the presentation itself to take about 10 minutes – the acceptable range is roughly 8-11 minutes.\nAfterwards, be prepared to answer a few questions both from your peers and the TA or instructor. You’re also expected to ask one or more questions (in total) to other students after their presentations.\nPrepare at least several slides. Your entire presentation can be given using slides. But if you want, you can also switch to showing your actual repository / scripts during part of the presentation (just be mindful of font sizes if you do the latter).\nStart with some general background about the data / research project, and an overview of the goals of the project.\nYou can see for yourself if you would like to run through some code line-by-line, or give a more high-level overview of the code you’ve written.\nAt the end, briefly mention what you still have to do – and if this is work that you will continue after this course, you can also discuss this in a broader sense. You can even explicitly ask for some advice, if you want.\nWhat you will be graded on:\nTechnical content [3 points]\nContextualization [2 points]\nDelivery [2 points]\nClarity [2 points]\nQuestions for other students [1 point]\n\n\n\n",
      "last_modified": "2021-04-25T17:51:53-04:00"
    },
    {
      "path": "finalproject_progress.html",
      "title": "Final project: Progress report",
      "author": [],
      "contents": "\n\nPlease report on your progress for your final project (due Tuesday, Apr 13). [10 points]\n\nI would like to see the following in your final project repository:\nOne or more (Bash/Python/R) scripts with a significant amount code. [4]\nThe scripts do not yet need to be complete or functioning, but make sure I can understand what you are trying to do. Also, it should be clear what the general purpose of the scripts is.\nYou can earn as many points with clarity of purpose and documentation (use comments!) as with the code itself.\nAn overview of all the scripts you envision writing, and their functions. [3]\nThis is essentially a more worked-out version of the technical description you wrote in your proposal (some remaining uncertainties are fine!).\nHaving learned about Snakemake, you should include how you want to implement being able to (re)run the entire pipeline/workflow: with Snakemake, or a Bash or Python script.\nA to-do list. [1.5]\nThis could be a separate list or it could be (partially) integrated with the overview of scripts mentioned above.\nBoth the technical description and the to-do list would be suitable for the main README.md of the project (and it would be good to continue to update these later on). But you could also create separate documents for each, whatever works better for you.\nSome general pointers:\nI will also start looking at your Git commits and associated commit messages. [1.5]\nI will not grade this harshly, and will just skim you commit history, but will subtract points if:\nYour commits do not form logical units at all – especially if they include work on multiple unrelated items. (“Oversplitting” of commits, i.e. having some really small commits that are not as much of a problem, that just tends to happen unless you amend commits.)\nYour commit messages are consistently uninformative (“Update file X” is generally not enough – especially when repeated).\n\nYou can also flag specific things that you would like feedback/advice on, perhaps best in the GitHub Issue mentioned below.\nOnce again, tag me (@jelmerp) in an “Issue” on GitHub and make sure you point me to the places you want me to look, particularly if you have a lot going on (in such cases, I won’t grade files that you don’t point me to).\nSome of you already had most of these pieces in place in your repository when I looked at your proposal. If that is the case, just point me where to look – you don’t necessarily need to restructure things for this checkpoint.\nRemember that I have added some topic overviews to the GitHub site that should be helpful as you work on you final project – see the top navigation bar.\n\n\n\n",
      "last_modified": "2021-04-25T17:51:54-04:00"
    },
    {
      "path": "finalproject_proposal.html",
      "title": "Final project: proposal",
      "author": [],
      "contents": "\nWrite a concise, informal summary of what you plan to do for your final project (due Tuesday, Mar 23). [10 points]\n\nFor general information about devising a project and expectations for your project, please see this page.\nIf you want to primarily use Python in your project, and are unsure about the possibilities because we are still in the middle of the section on Python, you can contact me to push back your due date for this proposal.\n\nSome pointers:\nCreate a directory for your project and start a Git repository. This will be a repository for the project as a whole, not just for this proposal. [1]\nWrite the proposal in Markdown and include it in your repository. Like we did with the graded assignments, when you’re done, push your repository to GitHub and tag me (@jelmerp) in an “Issue” on GitHub (please make sure to tag me in the text body for the issue, it won’t be parsed as a tag if you do this in the title of the issue). [1]\nIn the proposal, start with a general description of what your project will be about, without going into detail about coding languages and approaches. Describe what data you will work with and what kind of output your project will produce. (This can be pretty minimal if your project is “code-first” with a trivial data set.) [2]\nNext, summarize how you envision the more technical aspects: in which language(s) do you intend to code, what do you think you will need separate scripts for, and how will you structure results? Will you mostly be coding your data processing/analyses from scratch, or are you primarily running external programs? [2]\nCheck the list of graded aspects on the page with general information about the project. If you are not sure you will be able to fulfill one of these, or intend to skip something (e.g. OSC SLURM jobs), mention that here.\nBriefly mention which aspects of your project you are uncertain about, for instance because you intend to include some topics that we have yet to cover in the course, or because something will depend on how other things go. [2]\nFinally, briefly describe why you chose to pick this project. Because it will be useful for your research? Because it gives you practice with coding topics you like or that you want to / need to get better with? All of the above and/or something else? [2]\n\n\n\n",
      "last_modified": "2021-04-25T17:51:54-04:00"
    },
    {
      "path": "finalproject_submission.html",
      "title": "Final project: submission",
      "author": [],
      "contents": "\n\nContents\nHow to submit\nWhat to submit\nGraded aspects\nQuestions and advice\nLate submissions\nGood luck!!\n\nThe submission of your final project is due on April 30th. [20 points]\nHow to submit\nAs usual, open a new GitHub issue for your repository and tag @jelmerp in the text body of the Issue.\nWhat to submit\nYour repository should now contain:\nA finished set of scripts.\nFinal documentation in one or more README files that clearly describes:\nWhat the project does as a whole.\nWhat each script does.\nWhere to access the data at OSC, assuming that the data is not in your repository.\nHow the project’s scripts can be rerun using a single script or Snakefile.\n\nA single script or Snakefile that aims to rerun the full workflow.\nA file (e.g. submission_notes.md) or a section in your main README file that provides some additional information for the instructor to grade your project appropriately. Some hypothetical examples of things you may want to include:\nAdditional instructions the instructor will need to try and rerun your project.\nYou want to alert the instructor to some files files in the repository that should be ignored.\nYou want to explain why you don’t have a functioning script or Snakefile, or why you didn’t run any SLURM jobs (which can be acceptable in some cases).\n\nGraded aspects\nBelow is a long list of graded aspects and what to aim for if you want a perfect score. I’m providing a lot of detail here, so there are no surprises. The TLDR is that you should aim to have a reproducible, well-organized and well-documented workflow – workflow size/complexity on the other hand, is fairly unimportant. (See also the General Info page for the final project for some more general background.)\nCategory\nMax.  score\nMax. score if your project (examples given):\nProject organization\n2\nHas a clear and appropriate directory structure.\nHas informative and appropriate directory and file names.\nDoes not mix data, scripts, and results in individual directories.\n\nProject background and documentation\n2\nHas a clear description of its background and goals.\nHas a clear description of how different scripts are being used to achieve these goals.\nWhere appropriate, indicates what is still a work-in-progress (and optionally future directions).\n\nScript documentation\n2\nUses extensive (yet succinct) comments to document what is being done within scripts.\n\nGood practices in scripts\n4\nUses no absolute paths in scripts.\nUses scripts that take arguments where appropriate and minimizes “hard-coding” of potentially variable things like input/output dirs, file names, and some software settings. Any hard-coded variables/constants that are present are clearly set at the top of scripts.\nHas individual scripts that are not overly long and don’t do multiple unrelated things.\nHas no or only clearly annotated lines that are “commented out” in scripts.\nUses Bash scripts with proper set settings, and similar good practices as taught in the course.\n\nCoding quality and complexity\n3\nHas code that demonstrates an understanding of topics covered in the course.\nHas code that is appropriately broken up in small parts within scripts, e.g. with functions in Python.\nUses tools and commands that are (by and large) appropriate to accomplish its goals. (I will not dig in to fine details and parameter settings.)\n\nWorkflow  reproducibility\n3\nHas a script or Snakefile that includes all steps in the workflow and that can be run by anybody with access to your repository and the raw data files.\nHas information for the instructor (or any other reader of the project!) about where at OSC to find the raw data files and other details needed to try to rerun the analyses.\nBonus: good software management, e.g. Conda environments (preferred) or OSC modules and no manual installs unless necessary; YAML files describing environments.\n\nSLURM jobs at OSC\n2\nHas one or more scripts that are run as OSC jobs at SLURM.\nUses appropriate SLURM directives – either in the scripts or in a Snakemake profile YAML file.\n\nVersion control\nddddddddddddddddd\n2\ndddddd\nHas Git commit messages that are informative.\nHas reasonably appropriate commits, e.g. individual Git commits don’t consist of multiple completely unrelated edits.\nHas a single .gitignore file that ignores files like large raw data files, and in most cases, results files.\nBonus: has a Git tag for the submitted version.\ndddddddddddddddddddddddddddddddddddddddddddddd dddddddd dddddddd\nQuestions and advice\nDon’t hesitate to contact Jelmer (or, where fitting, Zach) for questions about topics like:\nSpecific expectations for the final project that are unclear to you.\nWhether you are on the right track in making some adjustments that I asked for after your progress report.\nAdvice on how to code or organize aspects of your project.\nWe’re happy to answer questions by e-mail or in a Zoom meeting!\nLate submissions\nLate submission may be accommodated depending on circumstances, but you will need to contact Jelmer before 3 pm on April 30th and we can take it from there.\nFor late submissions with no advance notice, 4 points will be subtracted for each day the submission is late. (Things like forgetting to open an Issue won’t lead to subtracted points.)\nGood luck!!\n\n\n\n",
      "last_modified": "2021-04-25T17:51:55-04:00"
    },
    {
      "path": "index.html",
      "title": "",
      "author": [],
      "contents": "\n\n\nPractical Computing Skills for Biologists  Spring 2021  A section of Current Topics in Plant Pathology (PLNTPTH 8300)\n\n\n\n\n",
      "last_modified": "2021-04-25T17:51:55-04:00"
    },
    {
      "path": "overviews_course.html",
      "title": "Practical course information",
      "author": [],
      "contents": "\n\nContents\nCourse Infrastructure\nHomework\nSynchronous Zoom Sessions\nFAQ\n\nFor an overview of course goals, grading, policies, and so on, please refer to the syllabus. Here, I will focus on practical information to help you orient to the course.\nCourse Infrastructure\nContact with Instructors\nYou can email me (Jelmer, main instructor) or Zach (TA) any time, and you should generally expect to get an answer within 24 hours, certainly on weekdays. We prefer to be referred to simply using our first names.\nOffice hours\nJelmer and Zach both have biweekly office hours, see the course’s home page on the Carmen Canvas site for times. You can reserve a slot by going to the Calendar on Carmen Canvas, clicking “Find Appointment” (right-hand side), then selecting this course and clicking “Submit”, and finally, clicking on a time slot in the main Calendar window and clicking “Reserve”.\nModules\nEach module is one week, and for all except two modules we will meet twice: There are two brief instructional breaks that each affect a module this course (no Zoom meetings on Tue, Feb 9, or on Thu, Apr 1). Therefore, those two modules are “half-modules” with half the material of regular modules.\nMaterial for each module will generally be published on the preceding Friday.\nCarmenCanvas and Github websites for this course\nThis GitHub website has all material for the course including slide decks, exercises, assignments, information about readings, and so on.\nThe Carmen Canvas site has the syllabus, Zoom and contact information, a calendar with all due dates, grades, and course announcements.\nCarmenCanvas notifications\nThe instructors will mainly be communicating with you using CarmenCanvas Announcements. Make sure that you are set up to receive a notification whenever an announcement is posted – see this page for more information. If you haven’t received a notification by the first week of the course, go back to check your settings and your email Spam folder.\nCoursebooks and other course readings\nWe will be reading parts of the following two textbooks:\nAllesina S, Wilmes M (2019). Computing Skills for Biologists. Princeton UP. Available online through the OSU library here; you can also find a PDF on the Carmen Canvas website for the course.\nThis book will be referred to as CSB for short, e.g. “CSB Ch. 1” refers to chapter 1 of this book.\nBuffalo V (2015). Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media, Inc. Available online through the OSU library here; you can also find a PDF on the Carmen Canvas website for the course.\nThis book will be referred to as Buffalo for short, e.g. “Buffalo Ch. 3” refers to chapter 3 of this book.\nIn most modules, we will read and, in class, work through a chapter of CSB, our main book for the course. In four of the modules, our primary reading will be a chapter of Buffalo so we can learn about project management and Markdown (week 2) and cover the shell and shell scripting in greater detail (weeks 4-6).1\nIn week 7, we will read part of the OSC documentation, and in week 13, we will read two papers related to automated workflow management, a topic largely omitted in both books.\nComputational infrastructure and software installation\nBecause all of the software needed for this course is available at the Ohio Supercomputer Center (OSC), we will be working at OSC. The way this works is that we can access the software through our browser after logging in at https://ondemand.osc.edu. We have an OSC “classroom project” for the course, and you will be asked to sign up at OSC in a pre-course assignment.\nBecause we will work at OSC, you are not required to install anything for this course, and we can accommodate all operating systems. However, you may still want to install software to be able to work locally: this optional assignment covers local software installation.\nHomework\nI have created a Carmen Canvas “assignment” for everything that I would like you to do outside of synchronous zoom sessions, including readings, exercises, graded assignments, and creating accounts. All these assignments have due dates associated with them so you can easily see in the Carmen Canvas course calendar when you are expected or recommended to do them.\nHere are the different types of homework (Canvas assignments), and when you are generally expected to do them:\nReadings: There are readings for every week, and you are recommended to read before the first class of the week.\nExercises: Most weeks will have one or more exercises. These don’t have to be submitted, but you are strongly recommended to do them and we may ask about them in class. It’s generally recommended to do these exercises after the second day of class of the week.\nGraded Assignments: Three graded assignments (10 points each) are due on Tuesday just before our Zoom meeting in weeks 4, 7, and 9, each covering material from the week prior. These assignments should be submitted through GitHub; you will learn how to do this during the course.\nFinal Project: Your final project is divided into 4 graded components: a proposal (week 11, 10 points), a draft (week 13, 10 points), a lightning presentation (week 15, 10 points), and the final submission (April 16th, 20 points). These components (except for the presentation) should also be submitted through GitHub.\nSetup: Computer setup such as signing up for a GitHub account or installing software.\nSurveys: There will be three brief surveys – pre-course, mid-course, and post-course.\nSynchronous Zoom Sessions\nWhat to Expect\nOverall, the Zoom sessions will be highly hands-on, so you should be prepared to be engaged during class. We will spend most of the time doing “participatory live coding” (or “code-along”), in which the instructor demonstrates and you are expected to follow along in your own terminal or console. During participatory live-coding, there will also be regular questions for students and small exercises.\nComputer Setup\nDuring participatory live coding, it will be beneficial to either have a (very) large monitor or two screens/monitors. This way, you can see what the instructor is doing and also do this yourself.\nIf you don’t have multiple monitors set up with a single device, you’re welcome to connect to the Zoom sessions with two devices; e.g., a laptop and a desktop, two laptops, etc. If you have neither a large screen nor multiple devices, you’ll have to decide if you will put the Zoom window and your own coding window side-by-side, or if you can effectively switch between windows. (The latter will work best by using Alt+Tab (Windows/Linux) or Command+Tab (Mac), and by having as few windows as possible open in your workspace.)\nYou may occasionally be asked to share your screen, so try to have a setup where this is possible. For example, if you watch the Zoom session on an iPad, but code on a computer, do also connect to the Zoom session with your computer so you can share your coding screen.\nEtiquette\nPlease keep your camera turned on as much as possible. This will be very helpful for the instructor, and probably for your fellow students too, in creating an engaging environment that resembles an in-person class meeting as much as possible. If you prefer not to have your camera on or have bandwidth problems, a note to the instructors about this would be appreciated.\nPlease have your microphone on mute by default, but feel free to unmute yourself whenever you would like to say something.\nFAQ\nDo I need any prior experience with scripting/programming or the tools that we will use?\nNo. That said, this will probably not be an “easy” course especially for those that have no prior experience with any programming language or any of the tools that we will be learning about. You should therefore try to make sure you can invest several hours per week outside of class meetings to be able to do all the readings and exercises.\nWill I need a powerful computer?\nNo. We will not be working with large amounts of data in this course and most or all of our work can be done at OSC. Therefore, you will not need a particularly powerful computer, or a large amount of hard drive space. But if your computer is very slow or nearly out of storage space, please do contact me to make sure you will not run into problems.\nShould I buy paper copies of the coursebooks?\nWhile the coursebooks can be accessed online and I also provide PDFs (see above), I recommend that you do buy a paper copy of the CSB book if you’re able to do so. If you’re working with sequencing data, I would recommend that you also buy the Buffalo book.\n\nWe will not cover the second half of CSB, which deals with LaTeX, R, and relational databases.↩︎\n",
      "last_modified": "2021-04-25T17:51:56-04:00"
    },
    {
      "path": "overviews_git.html",
      "title": "Git",
      "description": "Topic overview\n",
      "author": [],
      "contents": "\n\nContents\nRelevant course modules\nKey commands\nMiscellaneous\nTips and tricks\nExplanatory figures\nAdvanced (bonus)\nUndoing changes and viewing the past\nMerge conflicts\nForking and Pull Requests\nMiscellaneous\n\n\n\nRelevant course modules\nWeek 3\n\nKey commands\nCommand\nExplanation\nExamples\nadd\nStage files to be committed, including previously untracked files.\nTherefore, git add does two things at once for untracked files: start tracking them and stage them. Files that were already being tracked are merely staged by git add.\nStage a single file: git add file.txt Stages all eligible files, tracked and untracked: git add --all Stage shell scripts anywhere in project: git add *.sh\ncommit\nCommit all currently staged changes to create a snapshot for the repository to which you can always return later on.\ngit commit -m \"Started the book\" git commit -am \"Fix infinite loop bug\"      -m specify commit message as above      -a Also stage all changes (but won’t add untracked files)\nstatus\nGet the status of your repository: which files have changed since the last commit, which untracked files are present, etc.\n\nlog\nSee the repository’s commit history.\ngit log One-line summary for each commit: git log --oneline Show all branches in “graph” form (useful with >1 branch): git log --oneline --graph --all\ndiff\nBy default, show changes between the working dir and:\nThe stage (index) if something has been staged.\nThe last commit if nothing has been staged.\n\nLast commit vs second-to-last commit - full repo: git diff HEAD HEAD^ Last commit vs a specified commit - specific file: git diff HEAD d715c54 todo.txt Show changed between stage and last commit: git diff --staged\nmv\nMove files that are tracked by Git: better to use git mv than regular mv so Git will immediately register what happened properly.\ngit mv old.txt new.txt\nrm\nDelete files that are tracked by Git: better to use git rm than regular rm so Git will properly register what happened at once.\ngit rm tmp.txt\nbranch\nVarious functionality for Git branches, such as creating, removing, and listing branches. (To switch between branches, use git checkout.)\nList existing branches: git branch Create a new branch named “my-new-branch”: git branch my-new-branch Rename the current branch to “main”: git branch -M main Remove a branch that is no longer needed: git branch -d fastercode\ncheckout\ngit checkout has multiple functions:\ngit checkout <branch-name> will switch to the specified branch.\ngit checkout -- <file> will revert the specified file back to its last committed state.\ngit checkout <commit-id> will move you to the specified commit for looking around.\n\nMove to the master branch: git checkout master\nmerge\nMerge a specified branch into the current branch. When doing a merge, make sure that you are on the branch that you want to keep, and from there, merge the branch whose changes you want to incorporate.\nMerge branch fastercode into the current branch: git merge fastercode -m \"my msg\"\nremote\nInteract with “remote” counterparts of repositories: in our case, GitHub repositories.\nList the repo’s remote connections: git remote -v  Add a remote (syntax: git remote add <nickname> <URL>): git remote add origin git@github.com:me/my.git\npush\nUpload (changes to) your local repository to an online counterpart, in our case a GitHub repository.\nPush currently active branch to default remote connection: git push When pushing a branch for the first time, use the -u option to set up an “upstream” counterpart: git push -u origin main\npull\nDownload (changes from) an online counterpart to your local repository (in our case a GitHub repository) and merge (as in git merge) the changes into your local repo.\nMay result in a merge conflict if you are collaborating with others on the remote repo.\nPull from currently active branch in remote repo: git pull\nclone\nDownload an online repository.\nAfter this git clone, you will have a dir CSB in your current working dir with the full repo (incl. history): git clone https://github.com/CSB-book/CSB.git\ntag\nAdd a “tag” to a commit, for instance to mark the version of the scripts used in the analysis for your paper. ddddddddddddddddddddddddddddddddddddd\ngit tag -a v1.2.0 -m \"Publish version\" Tags need to be explicitly pushed like so: git push --follow-tags\n\nMiscellaneous\nCode / concept\nExplanation\nExamples\nmodule load git\nLoad the OSC Git module. If you forget to do this, Git will still work but you’ll be using a very old version.\n\n.gitignore\nA file that tells Git what (kinds of) files to ignore, i.e. files that are not to be listed among “Untracked files” and will not be added with git add --all.\n.gitignore is usually in the top-level repo dir (recommended!).\nDon’t forget to add and commit the .gitignore file.\nNote that files won’t be “retroactively ignored”: files that are already in the repo would need to be explicitly removed.\n\nExample entries: Ignore everything in the dir “data”: data/ Ignore files ending in ~ in the entire repo: *~ Ignore gzipped FASTQ files in the entire repo: *.fastq.gz\nHEAD\nA pointer to the most recent (last) commit on the current branch. Note that as a pointer, HEAD can move and will do so e.g. if you switch between branches.\n\nHEAD^, HEAD^^\nA Pointer to the second-to-last and third-to-last (and so on) commit. See figure below for details.\n\nHEAD~1, HEAD~2\nA pointer to the second-to-last and third-to-last (and so on) commit: easier than ^ as you get further back, e.g. HEAD~8. See figure below for details. ddddddddddddddddddddddddddddddddddddd\nddddddddddddddddddddddddddddddddddddd\n\nTips and tricks\nUse informative commit messages! Imagine yourself looking for some specific changes that you made in the repository a year ago: what would help you to find them?\nCommit often, using small commits. This will also help to keep commit messages informative.\nDon’t include unrelated sets of edits in a single commit. If you have worked on two disparate things in your project since the last commit, git add + git commit them separately.\nWhen collaborating: pull often. This will reduce the chances of merge conflicts.\nDon’t commit unnecessary files: include a .gitignore file to ignore these.\nDon’t have nested repositories. (If, for example, in the dir structure work/projects/gwas/ you have a Git repository in the work dir and in the gwas dir, they would be nested.)\nBe very careful with destructive commands like git reset at the commit level, or simply deleting your .git directory. As you get started with Git, it is best to make backup copies before you try these types of commands.\nFile and repository size limitations:\nBinary files cannot be tracked as effectively as plain text files: Git will just save a new version whenever there has been a change.\nRepository size: Best to keep individual repositories under a total size of 1 GB.\nFile size: GitHub will not allow files over 100 MB.\n\nExplanatory figures\nReferring to past commits:\n\n\n\nGit’s three areas (“trees”), staging, and committing:\n\n\n\nOverview of ways to undo changes that have not been committed:\n\n\n\n\nAdvanced (bonus)\nUndoing changes and viewing the past\nI need to\nUse this\nExamples\nUndo unstaged changes to a file\ngit checkout -- <file>\nNote: for the most recent Git versions (not yet on OSC), the recommended method is the new command git restore.\nAfter accidentally deleting README.md or overwriting it instead of appending to it: git checkout -- README.md\nUnstage a file\ngit reset HEAD <filename>\nNote: for the most recent Git versions (not yet on OSC), the recommended method is the new command git restore --staged.\nAfter accidentally staging README.md which was not supposed to be part of the next commit: git reset HEAD README.md\nUndo staged changes to a file\ngit checkout HEAD -- <filename>\nNote: this irrevocably discards the non-committed changes (your data is only safe with Git once it has been committed!).\nAfter accidentally deleting README.md or overwriting it instead of appending to it, AND staging these changes: git checkout HEAD -- README.md\nUndo ALL unstaged changes (all files)\ngit checkout -- .\nAfter making edits to several files that you all want to get rid of, to revert back to the last commit: git checkout -- .\nUndo ALL staged and unstaged changes (all files)\ngit reset --hard HEAD\nThis will take you back to the state of your repository right after the last commit.\n\nView the state of my repo for a past commit\ngit checkout <commit-id>\nThis is only for “looking around”. If you decide you want to go back to your repo as it was in an earlier commit, use git revert or git reset (see below).\ngit checkout 4dce25f git checkout HEAD^^ To go back, assuming you’re on branch master: git checkout master\nUndo one or more commits\nTo undo one or more commits, i.e. to roll the state of your repository back to how it was before the commit you want to undo, there are two main commands:\ngit revert: Undo the changes made by commits by reverting them in a new commit. (Safe – does not change history.)\ngit reset: Delete commits as if they were never made. (Unsafe – changes history.)\n(git reset HEAD^ undoes changes made by the last commit as it resets to the second-to-last commit.)\nUndo changes by most recent commit: git revert HEAD Undo changed by second-to-last commit: git revert HEAD^ Undo changes by any arbitrary commit: git revert e1c5739 Undo last commit and completely discard all changes made by it: git reset --hard HEAD^ Undo last commit and put all changes made by that commit in the working dir: git reset HEAD^ Undo last commit and stage all changes made by it (“peel off commit”): git reset --soft HEAD^\nRetrieve a file version from a past commit dddddddddddddddddddd\ngit checkout --  or git show ddddddddddddddddddddddddddddddddddddd\nRetrieve from second-to-last commit: git checkout HEAD^^ -- README.md Retrieve from arbitrary commit: git checkout e1c5739 -- README.md To not change after all and go back to the current version: git checkout HEAD -- README.md Look at the version from the last commit: git show HEAD:README.md Revert to arbitrary earlier version using redirection: git show ad4ca74:README.md > README.md ddddddddddddddddddddddddddddddddddddd\n\nMerge conflicts\nA merge conflict can occur when all three of the following conditions are met:\nYou are trying to merge two branches (this includes pulling from remote: recall that a pull includes a merge).\nOne or more files have committed changes on both of these branches since their divergence.\nSome of these changes were made in the same part(s) of file(s).\nWhen this occurs, Git has no way of knowing which changes to keep, and will report a merge conflict. When it does so, your merge process has basically been paused, and Git wants you to make changes to resolve the conflict, after which a git commit will complete the merge. To resolve a merge conflict:\nUse git status to find the conflicting files.\nOpen and edit those files manually to a version that fixes the conflict. Note that Git has added <<<<<<<, =======, and >>>>>>> markers to help you see the conflict – but you will want to remove those.\nUse git add to tell Git you have resolved the conflict in a particular file.\nUse git status to check that all changes are staged, at which point Git should tell you “All conflicts fixed but you are still merging. (use \"git commit\" to conclude merge)”.\nUse git commit -m \"My merge message\" to conclude the merge.\n\nForking and Pull Requests\nForking is a GitHub concept that will create a personal GitHub repo that remains linked to the source repository. For instance, you could keep your fork up-to-date with the source, and you could issue a Pull Request from your fork. See these slides for details and screenshots.\n\nMiscellaneous\nCode / concept\nExplanation\nExamples\nstash\ngit stash temporarily saves files.\nIt can be useful when you need to pull from remote, but are prevented from doing so because you have changes in your working dir (and those are not appropriate for a separate commit / new branch). In such a case, stash the changes, pull, and get your stashed changes back (see example on right).\nStash changes to tracked files:git stash (Add -u to include untracked files) Pull from the remote repository: git pull Apply stashed changes: git stash apply\ncommit --amend\nAdd something to a commit that has already been made, for instance if you notice a type or forgot a file.\nNote: amending commits “changes history” and as such is not recommended by everyone – but at any rate, do not amend commits that have been pushed online!\ngit commit --amend --no-edit (No edit will keep the same commit message)\ncheckout -b\nCreate a new branch and move to it immediately.\ngit checkout -b my-new-branch\ngit log <filename>\nShow commits in which a specific file was changed (!). ddddddddddddddddddddddddddddddddddddd\nSay you have a script that no longer works and you need to see the changes that were made to it: git log my-failing-script.sh\n\n\n\n\n",
      "last_modified": "2021-04-25T17:51:56-04:00"
    },
    {
      "path": "overviews_glossary.html",
      "title": "Glossary",
      "author": [],
      "contents": "\nKEY TERMS\nBuffalo Book: Bioinformatics Data Skills (Buffalo 2015) CSB Book: Computing Skills for Biologists (Allesina & Wilmes 2019) git Software for version control Github A website that hosts git projects, which are known as repositories Markdown A simple text markup language (think LaTeX or HTML but much simpler) OSC The Ohio Supercomputer Center Python A programming language shell\nThe Unix shell is a command-line interface to your operating system that runs within a terminal. - There are several shell flavors, and in this course, we will be working with the bash shell.\nSnakemake Software for automated workflow (pipeline) management SLURM The compute job scheduling system used at the Ohio Supercomputer Center Unix A family of operating systems that includes Mac and Linux, but not Windows\nADDITIONAL TERMS\nCLI Command-line Interface – a software interface with which one interacts by typing commands (cf. GUI) dir Directory, which is Unix-speak for a folder on your computer GUI Graphical User Interface – a visual software interface with which one interacts by clicking Hard-coding\n\n\n\n",
      "last_modified": "2021-04-25T17:51:56-04:00"
    },
    {
      "path": "overviews_markdown.html",
      "title": "Markdown",
      "description": "Topic overview\n",
      "author": [],
      "contents": "\n\nContents\nRelevant course modules\nMarkdown syntax\nWhitespace\nTables\nHTML\nRendering\n\nRelevant course modules\nWeek 2\n\nMarkdown syntax\nThe “extended syntax” options below are not supported by all interpreters, but they are by most, such as GitHub-flavored Markdown.\nSyntax\nResult\n*italic*\nitalic (alternative: single underscore _)\n**bold**\nbold (alternative: double underscore __)\n<https://website.com>\nClickable link: https://website.com\n[link text](website.com)\nLink with custom text: link text\n![](path/to/figure.png)\nFigure\n# My Title\nHeader level 1 (largest)\n## My Section\nHeader level 2\n### My Subsection\nHeader level 3 – and so forth\n- List item - Another item - Third item\nBulleted (unordered) list\n1. List item 1. Another item 1. Third item\nNumbered (ordered) list  (numbering will be automatic)\n`inline code`\ninline code formatting\n``` or 4 leading spaces\nStart/end of generic code block (``` is extended syntax)\n```bash\nStart of bash-formatted code block (end with ```)\n---\nHorizontal rule (line)\n> Text\nBlockquote (think quoted text in email)\nblank line\nNew paragraph (white space between lines)\ntwo spaces at end of line\nForce a line break. ddddddddddddddddddddddddddddddddddddddddddddd\n~~strikethrough~~\nstrikethrough (extended syntax)\nFootnote ref[^1]\nFootnote ref1(extended syntax)\n[^1]: Text\nThe actual footnote (extended syntax)\n\nWhitespace\nMarkdown generally ignores single line breaks: use two spaces at the end of a line to force a line break, or when appropriate, leave a blank line to start a new paragraph.\nMultiple consecutive blank lines (or spaces) will be treated the same as a single blank line (or space). For extra vertical whitespace, use the HTML tag <br>, each of which forces a new line break.\n\nTables\nNote that the vertical bars | don’t have to be aligned.\n| blabla | blabla |  |———–|———–|\n| blabla | blabla |\n| blabla | blabla |\nblabla\nblabla\nblabla\nblabla\nblabla\nblabla\n\nHTML\nMost Markdown interpreters accept HTML syntax. Some simple examples:\nFigures – a centered figure using 50% of the page width:\n<p align=\"center\">\n<img src=my.png width=50%>\n<\/p>\nText colors:\ninline <span style=\"color:red\">colored<\/span> text\ninline colored text\nOther:\nSyntax\nResult\nsuperscript<sup>2<\/sup>\nsuperscript2\nsuperscript<sub>2<\/sub>\nsubscript2\n<br>\nlinebreak / empty line\n<!-- text -->\ncomment\n\nRendering\nTo render Markdown files, i.e. convert them to a format like HTML or PDF, use Pandoc:\npandoc README.md > README.html\npandoc -o README.pdf README.md\nGitHub will automatically show rendered versions of Markdown files.\nIn VS Code, when editing a Markdown file, click *Open Preview to the Side* to see a preview.\n\n\n\n",
      "last_modified": "2021-04-25T17:51:57-04:00"
    },
    {
      "path": "overviews_python.html",
      "title": "Topic Overview: Python",
      "author": [],
      "contents": "\n\nContents\nRelevant course modules\nPython overview TBA\n\n\n\n\nRelevant course modules\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\n\nPython overview TBA\n\n\n\n",
      "last_modified": "2021-04-25T17:51:57-04:00"
    },
    {
      "path": "overviews_shell.html",
      "title": "Shell tools",
      "description": "Topic overview\n",
      "author": [],
      "contents": "\n\nContents\nRelevant course modules\nBasic commands\nData tools\nMiscellaneae\nShell wildcards\nRegular expressions\nMore details for a few commands\nless\nsed\nsed flags:\nawk\n\nKeyboard shortcuts\n\n\nRelevant course modules\nWeek 1\nWeek 2\nWeek 4\nWeek 5\nWeek 6\n\nBasic commands\nCommand\nDescription\nExamples / options\npwd\nPrint current working directory (dir).\npwd\nls\nList files in working dir (default) or elsewhere.\nls data/      -l long format      -h human-readable file sizes      -a show hidden files\ncd\nChange working dir. As with all commands, you can use an absolute path (starting from the root dir /) or a relative path (starting from the current working dir).\ncd /fs/ess/PAS1855 (With absolute path) cd ../.. (Two levels up) cd - (To previous dir)\ncp\nCopy files or, with -r, dirs and their contents (i.e., recursively).  If target is a dir, file will keep same name; otherwise, a new name can be provided.\ncp *.fq data/ (All .fq files into dir data) cp my.fq data/new.fq (With new name) cp -r data/ ~ (Copy dir and contents to home dir) \nmv\nMove/rename files or dirs (-r not needed).  If target is a dir, file will keep same name; otherwise a new name can be provided.\nmv my.fq data/ (Keep same name) mv my.fq my.fastq (Simple rename) mv file1 file2 mydir/ (Last arg is destination)\nrm\nRemove files or dirs/recursively (with -r).  With -f (force), any write-protections that you have set will be overridden.\nrm *fq (Remove all matching files) rm -r mydir/ (Remove dir & contents)      -i Prompt for confirmation      -f Force remove\nmkdir\nCreate a new dir.  Use -p to create multiple levels at once and to avoid an error if the dir exists.\nmkdir my_new_dir mkdir -p new1/new2/new3\ntouch\nIf file does not exist: create empty file.  If file exists: change last-modified date.\ntouch newfile.txt\ncat\nPrint file contents to standard out (screen).\ncat my.txt cat *.fa > concat.fq (Concatenate files)\nhead\nPrint the first 10 lines of a file or specify number with -n <n> or shorthand -<n>.\nhead -n 40 my.fq (print 40 lines) head -40 my.fq (equivalent)\ntail\nLike head but print the last lines.\ntail -n +2 my.csv (“trick” to skip first line) tail -f slurm.out (“follow” file)\nless\nView a file in a file pager; type q to exit. See below for more details.\nless myfile      -S disable line-wrapping\ncolumn -t\nView a tabular file with columns nicely lined up in the shell.\nNice viewing of a CSV file: column -s \",\" -t my.csv\nhistory\nPrint previously issued commands.\nhistory | grep \"cut\" (Find previous cut usage)\nchmod\nChange file permissions for file owner (user, u), “group” (g), others (o) or everyone (all; a). Permissions can be set for reading (r), writing (w), and executing (x). ddddddddddddddddddddddddddddddddddddd\nchmod u+x script.sh (Make script executable) chmod a=r data/raw/* (Make data read-only)      -R recursive ddddddddddddddddddddddddddddddddddddddddddddd\n\nData tools\nCommand\nDescription\nExamples and options\nwc -l\nCount the number of lines in a file.\nwc -l my.fq\ncut\nSelect one or more columns from a file.\nSelect columns 1-4: cut -f 1-4 my.csv      -d \",\" comma as delimiter\nsort\nSort lines.\nThe -V option will successfully sort chr10 after chr2. etc.\nSort column 1 alphabetically, column 2 reverse numerically: sort -k1,1 -k2,2nr my.bed      -k 1,1 by column 1 only      -n numerical sorting      -r reverse order      -V recognize number with string\nuniq\nRemove consecutive duplicate lines (often from single-column selection): i.e., removes all duplicates if input is sorted.\nUnique values for column 2: cut -f2 my.tsv | sort | uniq\nuniq -c\nIf input is sorted, create a count table for occurrences of each line (often from single-column selection).\nCount table for column 3: cut -f3 my.tsv | sort | uniq -c\ntr\nSubstitute (translate) characters or character classes (like A-Z for uppercase letters). Does not take files as argument; piping or redirection needed.\nTo “squeeze” (-s) is to remove consecutive duplicates (akin to uniq).\nTSV to CSV: cat my.csv | tr \"\\t\" \",\" Uppercase to lowercase: tr A-Z a-z < in.txt > out.txt      -d delete      -s squeeze\ngrep\nSearch files for a pattern and print matching lines (or only the matching string with -o).\nDefault regex is basic (GNU BRE): use -E for extended regex (GNU ERE) and -P for Perl-like regex.\nTo print lines surrounding a match, use -A n (n lines after match) or -B n (n lines before match) or -C n (n lines before and after match).\nddddddddddddddddddddddddddddddddddddddd\nMatch AAC or AGC: grep \"A[AG]C\" my.fa Omit comment lines: grep -v \"^# my.gff      -c count      -i ignore case      -r recursive      -v invert      -o print match only\n\nMiscellaneae\nSymbol\nMeaning\nexample\n/\nRoot directory.\ncd /\n.\nCurrent working directory.\ncp data/file.txt . (Copy to working dir) Use ./ to execute script if not in $PATH: ./myscript.sh\n..\nOne directory level up.\ncd ../.. (Move 2 levels up)\n~ or $HOME\nHome directory.\ncp myfile.txt ~ (Copy to home)\n$USER\nUser name.\nmkdir $USER\n>\nRedirect standard out to a file.\necho \"My 1st line\" > myfile.txt\n>>\nAppend standard out to a file.\necho \"My 2nd line\" >> myfile.txt\n2>\nRedirect standard error to a file.\nSend standard out and standard error for a script to separate files: myscript.sh >log.txt 2> err.txt\n&>\nRedirect standard out and standard error to a file.\nmyscript.sh &> log.txt\n|\nPipe standard out (output) of one command into standard in (input) of a second command\nThe output of the sort command will be piped into head to show the first lines: sort myfile.txt | head\n{}\nBrace expansion. Use .. to indicate numeric or character ranges (1..4 => 1, 2, 3, 4) and , to separate items.\nmkdir Jan{01..31} (Jan01, Jan02, …, Jan31) touch fig1{A..F} (fig1A, fig1B, …, fig1F) mkdir fig1{A,D,H} (fig1A, fig1D, fig1D)\n$()\nCommand substitution. Allows for flexible usage of the output of any command: e.g., use command output in an echo statement or assign it to a variable.\nReport number of FASTQ files: echo \"I see $(ls *fastq | wc -l) files\" Substitute with date in YYYY-MM-DD format: mkdir results_$(date +%F) nlines=$(wc -l < $infile)\n$PATH\nContains colon-separated list of directories with executables: these will be searched when trying to execute a program by name. ddddddddddddddddddddddddddddddddddddd\nAdd dir to path: PATH=$PATH:/new/dir (But for lasting changes, edit the Bash configuration file ~./bashrc.) dddddddddddddddddddddddddddddddd\n\nShell wildcards\nWildcard\nMatches\n\n*\nAny number of any character, including nothing.\nls data/*fastq.gz (Matches any file ending in “fastq.gz”) ls *R1* (Matches any file containing “R1” somewhere in the name.)\n?\nAny single character.\nls sample1_?.fastq.gz (Matches sample1_A.fastq.gz but not sample1_AA.fastq.gz)\n[] and [^]\nOne or none (^) of the “character set” within the brackets. ddddddddddddddddddddddddddddddddddddd\nls fig1[A-C] (Matches fig1A, fig1B, fig1C) ls fig[0-3] (Matches fig0, fig1, fig2, fig3) ls fig[^4]* (Does not match files with a “4” after “fig”) ddddddddddddddddddddddddddddddddddddddd\n\nRegular expressions\nNote: ERE = GNU “Extended Regular Expressions”. If “yes” in ERE column, then the symbol needs ERE to work1: use a -E flag for grep and sed (note that awk uses ERE by default) to turn on ERE.\nSymbol\nERE\nMatches\nExample\n.\n\nAny single character\nMatch Olfr with none or any characters after it: grep -o \"Olfr.*\"\n*\n\nQuantifier: matches preceding character any number of times\nSee previous example.\n+\nyes\nQuantifier: matches preceding character at least once\nAt least two consecutive digits: grep -E [0-9]+\n?\nyes\nQuantifier: matches preceding character at most once\nOnly a single digit: grep -E [0-9]?\n{m} / {m,} / {m,n}\nyes\nQuantifier: match preceding character m times / at least m times / m to n times\nBetween 50 and 100 consecutive Gs: grep -E \"G{50,100}\"\n^ / $\n\nAnchors: match beginning / end of line\nExclude empty lines: grep -v \"^$\" Exclude lines beginning with a “#”: grep -v \"^#\"\n\\t\n\nTab (To match in grep, needs -P flag for Perl-like regex)\necho -e \"column1 \\t column2\"\n\\n\n\nNewline (Not straightforward to match since Unix tools are line-based.)\necho -e \"Line1 \\n Line2\"\n\\w\n(yes)\n“Word” character: any alphanumeric character or “_”. Needs -E (ERE) in grep but not in sed.\nMatch gene_id followed by a space and a “word”: grep -E -o 'gene_id \"\\w+\"'  Change any word character to X: sed s/\\w/X/\n|\nyes\nAlternation / logical or: match either the string before or after the |\nFind lines with either intron or exon: grep -E \"intron|exon\"\n()\nyes\nGrouping\nFind “AAG” repeated 10 times: grep (AAG){10}\n\\1, \\2, etc.\nyes\nBackreferences to groups captured with (): first group is \\1, second group is \\2, etc. ddddddddddddddddddddddddddddddddddddd\nInvert order of two words: sed -E 's/(\\w+) (\\w+)/\\2 \\1/' ddddddddddddddddddddddddddddddddddddd\n\nMore details for a few commands\nless\nKey\nFunction\nq\nExit less\nspace / b\nGo down / up a page. (pgup / pgdn usually also work.)\nd / u\nGo down / up half a page.\ng / G\nGo to the first / last line (home / end also work).\n/<pattern> or ?<pattern>\nSearch for <pattern> forwards / backwards: type your search after / or ?.\nn / N\nWhen searching, go to next / previous search match.dddddddddddddddddddddddddddddddddddddddddddddddddddd\nsed\nsed flags:\nFlag\nMeaning\n-E\nUse extended regular expressions\n-e\nWhen using multiple expressions, precede each with -e\n-i\nEdit a file in place\n-n\nDon’t print lines unless specified with p modifier\nsed examples\n# Replace \"chrom\" by \"chr\" in every line,\n# with \"i\": case insensitive, and \"g\": global (>1 replacements per line)\nsed 's/chrom/chr/ig' chroms.txt\n\n# Only print lines matching \"abc\":\nsed -n '/abc/p' my.txt\n\n# Print lines 20-50:\nsed -n '20,50p'\n\n# Change the genomic coordinates format chr1:431-874 (\"chrom:start-end\")\n# ...to one that has a tab (\"\\t\") between each field:\necho \"chr1:431-874\" | sed -e 's/:/\\t/' -e 's/-/\\t/'\n#> chr1    431     874\n\n# Invert the order of two words:\necho \"inverted words\" | sed -E 's/(\\w+) (\\w+)/\\2 \\1/'\n#> words inverted\n\n# Capture transcript IDs from a GTF file (format 'transcript_id \"ID_I_WANT\"'):\n# (Needs \"-n\" and \"p\" so lines with no transcript_id are not printed.) \ngrep -v \"^#\" my.gtf | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p'\n\n# When a pattern contains a `/`, use a different expression delimiter:\necho \"data/fastq/sampleA.fastq\" | sed 's#data/fastq/##'\n#> sampleA.fastq\nawk\nRecords and fields: by default, each line is a record (assigned to $0). Each column is a field (assigned to $1, $2, etc).\nPatterns and actions: A pattern is a condition to be tested, and an action is something to do when the pattern evaluates to true.\nOmit the pattern: action applies to every record.\nawk '{ print $0 }' my.txt     # Print entire file\nawk '{ print $3,$2 }' my.txt  # Print columns 3 and 2 for each line\nOmit the action: print full records that match the pattern.\n# Print all lines for which:\nawk '$3 < 10' my.bed          # Column 3 is less than 10\nawk '$1 == \"chr1\"' my.bed     # Column 1 is \"chr1\"\nawk '/chr1/' my.bed           # Regex pattern \"chr1\" matches\nawk '$1 ~ /chr1/' my.bed      # Column 1 _matches_ \"chr1\"\n\nawk examples\n# Count columns in a GTF file after excluding the header\n# (lines starting with \"#\"):\nawk -F \"\\t\" '!/^#/ {print NF; exit}' my.gtf\n\n# Print all lines for which column 1 matches \"chr1\" and the difference\n# ...between columns 3 and 2 (feature length) is less than 10:\nawk '$1 ~ /chr1/ && $3 - $2 > 10' my.bed\n\n# Select lines with \"chr2\" or \"chr3\", print all columns and add a column \n# ...with the difference between column 3 and 2 (feature length):\nawk '$1 ~ /chr2|chr3/ { print $0 \"\\t\" $3 - $2 }' my.bed\n\n# Caclulate the mean value for a column:\nawk 'BEGIN{ sum = 0 };            \n     { sum += ($3 - $2) };             \n     END{ print \"mean: \" sum/NR };' my.bed\nawk comparison and logical operators\nComparison\nDescription\na == b\na is equal to b\na != b\na is not equal to b\na < b\na is less than b\na > b\na is greater than b\na <= b\na is less than or equal to b\na >= b\na is greater than or equal to b\na ~ /b/\na matches regular expression pattern b\na !~ /b/\na does not match regular expression pattern b\na && b\nlogical and: a and b\na || b\nlogical or: a or b [note typo in Buffalo]\n!a\nnot a (logical negation)\nawk special variables and keywords\nkeyword/variable\nmeaning\nBEGIN\nUsed as a pattern that matches the start of the file\nEND\nUsed as a pattern that matches the end of the file\nNR\nNumber of Records (running count; in END: total nr. of lines)\nNF\nNumber of Fields (for each record)\n$0\nContains entire record (usually a line)\n$1 - $n\nContains one column each\nFS\nInput Field Separator (default: any whitespace)\nOFS\nOutput Field Separator (default: single space)\nRS\nInput Record Separator (default: newline)\nORS\nOutput Record Separator (default: newline)\nawk functions\nFunction\nMeaning\nlength(<string>)\nReturn number of characters\ntolower(<string>)\nConvert to lowercase\ntoupper(<string>)\nConvert to uppercase\nsubstr(<string>, <start>, <end>)\nReturn substring\nsplit(<string>, <array>, <delimiter>)\nSplit into chunks in an array\nsub(<from>, <to>, <string>)\nSubstitute (replace) regex\ngsub(<from>, <to> <string>)\n>1 substitution per line\nprint\nPrint, e.g. column: print $1\nexit\nBreak out of record-processing loop;  e.g. to stop when match is found\nnext\nDon’t process later fields: to next iteration\n\n\n\nKeyboard shortcuts\nShortcut\nFunction\nTab\nTab completion\n⇧ / ⇩\nCycle through previously issued commands\nCtrl+Shift+C\nCopy selected text\nCtrl+Shift+V\nPaste text from clipboard\nCtrl+A / Ctrl+E\nGo to beginning/end of line\nCtrl+U / Ctrl+K\nCut from cursor to beginning / end of line2\nCtrl+W\nCut word before before cursor3\nCtrl+Y\nPaste (“yank”)\nAlt+.\nLast argument of previous command (very useful!)\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\nCtrl+C\nKill (stop) currently active command\nCtrl+D\nExit (a program or the shell depending on the context)\nCtrl+Z\nSuspend (pause) a process: then use bg to move to background.\nWhen using the default regular expressions in grep and sed, Basic Regular Expressions (BRE), the symbol would need to be preceded by a backslash to work.↩︎\nCtrl+K doesn’t work by default in VS Code, but can be set there.↩︎\nDoesn’t work by default in VS Code, but can be set there.\n\n↩︎\n",
      "last_modified": "2021-04-25T17:51:58-04:00"
    },
    {
      "path": "overviews_shellscript.html",
      "title": "Shell scripting",
      "description": "Topic overview\n",
      "author": [],
      "contents": "\n\nContents\nRelevant course modules\nBash script essentials\nInside the script\nExecuting scripts\n\nScripting-related code\nOverview\nfor loops\nif statements\nFile test operators\nComparison operators\n\n\n\nRelevant course modules\nWeek 5\n\nBash script essentials\nIn VS Code, don’t forget to install the shellcheck extension!\nInside the script\nCode\nExplanation\nExample\n#!/bin/bash\n“Shebang” line: points the computer to the Bash interpreter at /bin/bash.\n\nset -u -e -o pipefail\nBash strict settings: exit script with error if an unset variable is referenced (-u), if a general error occurs (but with exceptions; -e), or if an error occurs in a shell pipeline (-o pipefail).\n\n$0\nName of the script (Note: does not work for scripts submitted as SLURM jobs).\n\n$1, $2, etc\n“Positional parameters”: first, second, etc, arguments passed to the script from the shell.\n./script.sh my_arg1 my_arg2 $1 will be “my_arg1” and $2 will be “my_arg2”.\n$#\nNumber of arguments passed to the script.\n./script.sh my_arg1 my_arg2 $# will be 2.\n>&2\nRedirect standard out to standard error: e.g., “manually” designate an echo statement to represent standard error.\necho \"Error: Invalid line nr.\" >&2\nexit 1\nExit the script with exit code 1 (= failure). dddddddddddddddddddddddddddddddddddddddddd\necho \"Error: need 3+ args\" && exit 1\nExecuting scripts\nCommand\nExplanation\nbash myscript.sh\nScripts that are not executable (i.e., have no execute permission) and/or have no shebang line should be run by explicitly calling bash.\n./myscript.sh scripts/myscript.sh\nScripts that are executable and have a shebang line can be called directly by name. Note: if they are in the current working dir, preface with ./ (otherwise the script will be looked for only in $PATH).\n./myscript.sh in.txt out.txt\nCall a script with two arguments, which will be available inside the script as $1 and $2.\n\nScripting-related code\nOverview\nCode\nExplanation\nExample\n=\nAssign a variable. Note: no spaces around the = !\nnlines=200 nlines=$(wc -l my.csv)\n$\nRecall/reference a variable with $.\nPreferably quote variables too, especially in scripts, to prevent unwanted shell expansion in case of spaces and other special characters in variable values.\nOptionally, put variable names in curly braces {}.\necho $nlines (After assignment e.g. nlines=200) echo \"$nlines\" (Safer: quoted) echo \"${nlines}\" (Optionally: “embraced”)\n[ ]\nTest statement. Spaces required around the brackets (see examples)!\n[ 9 -gt 5 ] (Returns true: 9 is greater than 5) [ $var1 -lt $var2 ] (Returns true if $var1 is less than $var2) [ -d my_dir ] (Returns true if dir exists and is a dir)\n()\nUse to assign an array: a collection of items that can e.g. be looped over.\nsample_names=(zmaysA zmaysB zmaysC) sample_files=($(cut -f 3 samples.txt)) sample_files=($(cat fastq_files.txt))\n${array[@]}\nPrint all values in an array.\necho ${sample_names[@]}\n&&\nChain commands: execute second command only if the first succeeds.\ncd data && ls data git add --all && git commit -m \"Add README\" && git push\n||\nChain commands: execute second command only if the first fails.\ncd \"$outdir\" || echo \"Cannot change directory!\" [ -d \"$outdir\" ] || mkdir \"$outdir\"\nbasename\nStrip any directory names from a path, and optionally a suffix too.\nbasename data/A.fq (Returns A.fq) basename data/A.fq .fq (Returns A)\nexpr\nSimple arithmetic in the shell (but: no decimals, only integers!) ddddddddddddddddddddddddddddddddddddd\nnseqs=$(expr $nlines / 4) (Divide a value by 4)\nfor loops\nBasic example showing the syntax:\nfor i in 1 2 3; do\n    echo \"Now the variable 'i' is: $i\"\ndone\n#> Now the variable 'i' is: 1\n#> Now the variable 'i' is: 2\n#> Now the variable 'i' is: 3\nIn each iteration, one of the items provided after in will be assigned to the variable name provided after for, which can then be used inside the loop.\nPractical examples:\n# Loop over files using globbing - better than using `ls`:\nfor fastq_file in data/raw/*fastq.gz; do\n      echo \"File $fastq_file has $(wc -l < $fastq_file) lines.\"\n      # More processing...\ndone\n  \n# Loop to rename files:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" _001.fastq).fq\n    echo \"Old/new name: $oldname $newname\"\n    mv \"$oldname\" \"$newname\"\ndone\n\n# Loop using an array to submit a script for each sample:\nmy_samples=($(cut -f1 my_metadata.txt))\nfor my_sample in ${my_samples[@]}; do\n    my_script.sh $my_sample\ndone\nif statements\nBasic syntax:\nif <some_test>; then\n    # Commands to run if test evaluated to true\nfi\n  \nif <some_test>; then\n    # Commands to run if test evaluated to true\nelse\n    # Commands to run if test evaluated to false\nfi\nThe test is usually done with the [] syntax for a test, e.g. [ -d my_dir ] which will evaluate to true is my_dir is an existing directory.\nPractical examples:\n# Differential processing based on (e.g.) the number of samples:\nn_samples=$(wc -l < samples.txt)\nif [ \"$n_samples\" -gt 9 ]; then  # If the nr of samples is >9\n    echo \">9 samples: processing files with algorithm A...\"\nelse\n    echo \"<= 9 samples: processing files with algorithm B...\"\nfi\n\n# Test whether the correct number of arguments (here: 2)\n# were provided to the script:\nif [ ! \"$#\" -eq 2 ]; then\n      echo \"Error: wrong number of arguments\"\n      echo \"You provided $# arguments, while 2 are required.\"\n      echo \"Usage: line.sh <line-number> <file>\"\n      exit 1\nfi\n\n# Test whether the input file is a regular file (-f) and can be read (-r):\nif [ ! -f $file ] || [ ! -r $file ]; then\n    echo \"Error: can't open file\"\n    echo \"Second argument should be a readable file\"\n    echo \"You provided: $file\"\n    exit 1\nfi\n\n# Use a command's exit status - for grep, a match is success is true:\nif grep \"AGATCGG\" contimated.fasta > /dev/null; then\n    echo \"OH NO! File is contaminated!\"\n    exit 1\nfi\n\n# Remove all empty files from a directory:\nfor file in *; do\n    if [ ! -s \"$file\" ]; then\n        rm \"$file\"\n    fi\ndone\nFile test operators\nOperator\nReturns true if:\n-f\nFile is a regular file (e.g. not a directory)\n-d\nFile is a directory\n-e\nFile exists\n-s\nFile is not zero size\n-h\nFile is a symbolic link\n-r / -w / -x\nFile has read/write/execute permissions\nComparison operators\nString\nDescription\n-z str\nString str is null (empty)\nstr1 = str2\nStrings str1 and str2 are identical\nstr1 != str2        \nStrings str1 and str2 are different                \nInteger\nDescription\nint1 -eq int2\nIntegers int1 and int2 are equal\nint1 -ne int2\nIntegers int1 and int2 are not equal\nint1 -lt int2\nInteger int1 is less than int2\nint1 -gt int2\nInteger int1 is greater than int2\nint1 -le int2\nInteger int1 is less than or equal to int2\nint1 -ge int2      \nInteger int1 is greater than or equal to int2\n\n\n\n",
      "last_modified": "2021-04-25T17:51:58-04:00"
    },
    {
      "path": "overviews_slurm.html",
      "title": "OSC, SLURM, and Conda",
      "description": "Topic overview\n",
      "author": [],
      "contents": "\n\nContents\nRelevant course modules\nUsing SLURM\nSubmitting batch (non-interactive) jobs\nSubmitting interactive jobs\nMonitoring and managing SLURM jobs\n\nUsing modules to load software\nSoftware management with Conda\n\n\nRelevant course modules\nWeek 6\n\nUsing SLURM\nSubmitting batch (non-interactive) jobs\nSLURM directives can be provided:\nAs arguments to sbatch when submitting the job (see below), or\nInside the script on lines starting with #SBATCH (see below).\nIf the same directive is provided in both places, the command-line (sbatch call) value will override the one in the script.\nMost important sbatch options\nFor more options, see the SLURM documentation.\nResource/use\nshort\nlong\ndefault\nexample / remarks\nProject to be billed\n-A\n--account\nN/A\n--account=PAS0471 -A PAS1855\nTime limit\n-t\n--time\n1 hour\n-t 60 (60 min) -t 2:30 (2 min and 30 sec) -t 5:00:00 (5 h) -t 2-12 (2 days and 12 h) --time=60 (60 min)\nNumber of nodes\n-N\n--nodes\n1\n--nodes=2 Only ask >1 node if you have explicit parallelization with e.g. MPI (uncommon in bioinformatics).\nNumber of cores\n-c\n--cpus-per-task\n1\n--cpus-per-task=4 For jobs with multi-threading (common).\nNumber of “tasks” (processes)\n-n\n--ntasks\n1\n--ntasks=2For jobs with multiple processes (not as common).\nNumber of tasks per node\n-\n--ntasks-per-node\n1\n--ntasks-per-node=2For jobs with multiple processes (not as common).\nMemory limit per node\n-\n--mem\n(4G)\n--mem=40G The default unit is MB (MegaBytes) – use “G” for GB.\nLog output file\n-o\n--output\nslurm-%j.out\n--output=slurm-fastqc-%j.out (It’s useful to include a descriptive name, but be sure to also include %j, the job number.)\nError output file\n-e\n--error\nN/A\n--error=slurm-fastqc-%j.err (Note: by default, stderr is included with stdout in -o / --output; use -e/--error to separate.)\nJob name\n-\n--job-name\nN/A\n--job-name=fastqc (Useful to distinguish jobs when looking at the queue.)\nPartition (queue type)\n-\n--partition\nany\n--partition=longserial  See these OSC docs for more info.\nGet email when job starts/ends/fails\n-\n--mail-type\nN/A\n--mail-type=START (When job starts) --mail-type=END (When job ends) --mail-type=FAIL (When job fails) --mail-type=ALL (Any event) \nJob can’t start until specified time\n-\n--begin\nN/A\n--begin=2021-02-01T12:00:00\nJob can’t start until dependency job has finished ddddddddddddddddddddd\n-\n--dependency\nN/A\n--dependency=afterany:123456 dddddddddddddddddddddddddddddddddd\nSubmitting the job with sbatch\nTo submit a script to the queue with sbatch, simply prepend sbatch, optionally with sbatch options, before the call to the script:\nsbatch [sbatch-options] <script> [script-arguments]\nSome examples with and without sbatch options and script arguments:\n# No sbatch options (must be provided in script) and no script arguments:\nsbatch myscript.sh\n\n# No sbatch options, one script argument:\nsbatch myscript.sh sampleA.fastq.gz\n\n# Two sbatch options, no script arguments:\nsbatch -t 60 -A PAS1855 --mem=20G myscript.sh\n\n# Two sbatch options, one script argument:\nsbatch -t 60 -A PAS1855 --mem=20G myscript.sh sampleA.fastq.gz\nExample script header with SLURM directives\nNote: Because SLURM directives are a special type of comments, they need to occur before any lines that are executed in order to be parsed. For instance, they should be placed above the set header below:\n#!/bin/bash\n\n#SBATCH --account=PAS1855\n#SBATCH --time=00:45:00\n#SBATCH --mem=8G\n\nset -e -u -o pipefail\nSLURM environment variables\nInside the script, SLURM environment variables will be available, such as:\nVariable\nCorresponding option\nDescription\n$SLURM_SUBMIT_DIR\nN/A\nPath to dir from which job was submitted.\n$TMPDIR\nN/A\nPath to a dir available during the job (fast I/O).\n$SLURM_JOB_ID\nN/A\nJob ID assigned by SLURM.\n$SLURM_JOB_NAME\n--job-name\nJob name supplied by the user.\n$SLURM_CPUS_ON_NODE\n-c / --cpus-per-task\nNumber of CPUs (~ cores/threads) available on 1 node.\n$SLURM_NTASKS\n-n / --ntasks\nNumber of tasks (processes).\n$SLURM_MEM_PER_NODE\n--mem\nMemory per node.\n$SLURMD_NODENAME\nN/A\nName of the node running the job.\nAs an example of how these environment variables can be useful, the command below uses $SLURM_CPUS_ON_NODE in its call to the program STAR inside the script:\nSTAR --runThreadN \"$SLURM_CPUS_ON_NODE\" --genomeDir ...\nThis way, we don’t risk having a mismatch between the resources requested and the resources (attempting to be) used, and we only have to modify the number of threads in one place (in the resource request to SLURM).\n\nSubmitting interactive jobs\nCommand\nExplanation\nExample\nsinteractive\nOSC convenience wrapper around srun / salloc. Only accepts short options (e.g. -A, not --account). Default time is 30 minutes and maximum time is 60 minutes.\nsinteractive -A PAS1855 -t 60\nsrun\nStart an interactive job with any set of options; needs --pty /bin/bash to enter a Bash shell on the reserved node. ddddddddddddddddddddddddddddddddddddd\nsrun -A PAS1855 -t 60 --pty /bin/bash\n\nMonitoring and managing SLURM jobs\nCommand\nExplanation\nExample\nsqueue\nCheck the SLURM job queue: will only show queued and running jobs, no finished jobs. (Use -u or you will see everyone’s jobs!)\nsqueue -u $USER (show all my jobs) squeue -u $USER -l (long format, more info)\nscancel\nCancel one or more jobs.\nscancel 2526085 (Cancel job 2526085) scancel -u $USER (Cancel all my jobs)\nscontrol\nInformation about any job, mostly a summary of the resources available to the job and other options that were implicitly or explicitly set.\nscontrol show job 2526085 (Show stats for job 2526085) scontrol show job $SLURM_JOB_ID (for usage INSIDE a script)\nsstat\nInformation about running jobs, including memory usage  (=> can be included in script).\nsstat -j $SLURM_JOB_ID --format=jobid,avecpu,averss,maxrss,ntasks\nsacct\nInformation about finished jobs, including memory usage. ddddddddddddddddddddddddddddddddddddd\nsacct -j 2978487\nNotes:\nIf you need to check CPU and memory usage of your jobs, see also the XDMoD tool on OSC Ondemand.\nIn sstat and sacct, MaxRSS is the maximum amount of memory used by the job.\n\nUsing modules to load software\nCommand\nExplanation\nExample\nmodule spider\nSee all modules (software available through the module system).\nmodule spider (See all modules) module spider python (All modules matching “Python”)\nmodule avail\nSee modules that can be loaded given the current software environment.\nmodule avail (See all modules) module avail python (All modules matching “Python”)\nmodule load\nLoad a specific module. After loading the module, the software will be available in your $PATH and can thus be called directly.\nmodule load python (Load default version) module load python/3.7-2019.10 (Load a specific version)\nmodule list\nList currently loaded modules\nmodule list\nmodule unload\nUnload a currently loaded module. ddddddddddddddddddddddddddddddddddddd\nmodule unload python dddddddddddddddddddddddddddddddddddddddddd\nSoftware management with Conda\nTo use Conda at OSC, first load the module python/3.6-conda5.2 using module load python/3.6-conda5.2.\nConda commands and options:\nCommand\nExplanation\nExample\ncreate\nCreate a new Conda environment.\nconda create -n my-env conda create cutadapt-env cutadapt\nenv create\nCreate a new Conda environment using a YAML file describing the environment (see conda export below).\nconda env create --file environment.yml\n-y\nDon’t prompt for confirmation when installing/removing things.\nconda install -y cutadapt\nactivate\nActivate a specific Conda environment, so you can use the software installed in that environment.\nNote: the examples use source activate rather than conda activate, see below for additional setup to enable conda activate.\nsource activate cutadapt-env source activate cutadapt-env Activate -“stack”- a second environment: source activate --stack <second-env-name>\ninstall\nInstall software into the currently active environment.\nconda install python=3.7 Specify channel for installation: conda install -c bioconda cutadapt\nconfig\nConfigure Conda (see below).\nconda config --add channels bioconda\nexport\nExport a YAML file that describes the environment.\nExport the active environment: conda env export > environment.yml Export any environment by name: conda env export -n multiqc-env > multiqc-env.yml\nenv list\nList all your environments.\nconda env list\nlist\nList all packages (software) in an environment.\nconda list -n multiqc-env\ndeactivate\nDeactivate the currently active environment.\nconda deactivate\nremove\nRemove an environment entirely.\nconda env remove -n multiqc-env\nsearch\nSearch for a software package. ddddddddddddddddddddddddddddddddddddd\nconda search 'bwa*'\nConda channels\nConda “channels” are like repositories, each of which carry overlapping sets of software. A one-time setup step to set the channel priorities in the order that is generally desired – run these lines in the following order:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Highest priority\nChannels can also be specified for individual installations, in order to override these defaults:\nconda install -c bioconda cutadapt\nConda setup to enable conda activate\nTo enable conda activate to work (in addition to source activate), add the following lines to your Bash configuration file at ~/.bashrc (which you can open with VS Code or Nano and edit):\nif [ -f /apps/python/3.6-conda5.2/etc/profile.d/conda.sh ]; then\n      source /apps/python/3.6-conda5.2/etc/profile.d/conda.sh\nelif [ -f /usr/local/python/3.6-conda5.2/etc/profile.d/conda.sh ]; then\n      source /usr/local/python/3.6-conda5.2/etc/profile.d/conda.sh\nfi\nFor immediate effects, you’ll need to run the ~/.bashrc file:\nsource ~/.bashrc\n\n\n\n",
      "last_modified": "2021-04-25T17:51:59-04:00"
    },
    {
      "path": "overviews_snakemake.html",
      "title": "Snakemake",
      "description": "Topic overview\n",
      "author": [],
      "contents": "\n\nContents\nRelevant course modules\nCommand-line interface\nRules\nRule directives\n\nPlaceholders and wildcards\nFunctions\nMisc. tips & tricks\nA few worked examples\n\n\nRelevant course modules\nWeek 13\n\nCommand-line interface\nRules or output files can optionally be provided as arguments:\nsnakemake -j1 – run the first rule and all its dependencies.\nsnakemake -j1 my_rule – run the rule my_rule and all its dependencies.\nsnakemake -j1 my_output_file – run whatever rules are needed to be run to produce the output file my_output_file. In the Snakefile, my_output_file can either be literally specified as an output file, or can be inferred by Snakemake from an output directive given possible wildcard values.\n\nIf a Snakefile is called either of the following (relative to the dir from which snakemake is called), it will be automatically detected: Snakefile / snakefile / workflow/Snakefile / workflow/snakefile. Otherwise, use the -s option to specify the Snakefile to run.\nOption\nMeaning\nExample\n-j / --jobs / --cores\nMandatory option: maximum number of jobs to be run in parallel. At OSC, this will be the max. nr. of SLURM jobs to be submitted; when running locally, this should not exceed the number of cores.\nsnakemake -j1\nsnakemake -j100\n-n / --dryrun\nDon’t run anything, just report what would be run.\n\n-p / --printshellcmds\nPrint commands from shell directives that will be executed.\n\n-r / --reason\nGive reason of execution for every job.\n\n-q / --quiet\nSnakemake will print less output to screen – can be useful with -n just to get an overview of jobs that will be run.\n\n-s / --snakefile\nName of / path to the Snakefile.\nRun the Snakefile rules.smk: snakemake -s rules.smk\n--use-conda\nIn combination with conda directive(s) in the Snakefile, will run jobs in a Conda environment.\n\n--cluster\nBasically a prefix that should be added to any command in order to submit to a cluster.\nAt OSC, this will need to be at a minimum sbatch --account=PAS1855, and any number of SLURM options can be added.\nWith additional non-default SLURM options, it becomes more practical to use a profile (see below).\nHave at most 50 jobs in the queue: snakemake -j50 --cluster \\      \"sbatch --account=PAS1855\"\nEvery SLURM job should have a time limit of 10 minutes: snakemake -j50 --cluster \\      \"sbatch --account=PAS1855 --time=10\"\n---lint\nRun the Snakemake “linter” on the Snakefile – check syntax, some best practices, and so on.\n\n-f / --force <target>\nForce creation of a specified target file, or if nothing is specified, the first rule.\nForce-run whatever is needed to produce smpA.bam: snakemake -f smpA.bam Force-run the first rule: snakemake -f\n-F / --forceall <rule>\nForce running a specified rule and all dependencies of that rule, or if nothing is specified, the first rule.\nForce-run “rule map:” snakemake -F map Force-run the workflow if first rule is “rule all”: snakemake -F\n-R / --forcerun\nForce creation of a list of target files.\nUseful in combination with the --list-code-changes option which will output a list of output files affected by changes in the Snakefile.\nThe command substitution will pass all relevant output files to the -R option: snakemake -j1 -R \\      $(snakemake --list-code-changes)\n--report\nCreate an HTML report with runtime statistics, workflow\n\n--dag\nCreate a “Directed Acyclic Graph” (DAG) of all jobs in the workflow.\nsnakemake --dag | \\     dot -T svg > jobs.svg\n--rulegraph\nCreate a “Directed Acyclic Graph” (DAG) of all rules in the workflow (better for larger workflows). ddddddddddddddddddddddddddddddddddddddd\nsnakemake --rulegraph | \\     dot -T svg > rules.svg ddddddddddddddddddddddddddddddddddddddddddd\nRules\nIt is common to have a first rule in the Snakefile called rule all that only has an input directive listing all final output files. (Recall that “final” here means any output files that are not used as input by other rules, e.g. MultiQC output.)\nRelationships/dependencies between rules are implicit: Snakemake infers these from input and output directives.\nThe order of rules in the Snakefile is not important except that the first rule is run by default.\nRule directives\nDirective\nExpected values\nExamples\ninput\nInput file(s) for the rule.\nWhen using a wildcard {}, multiple jobs are run for the rule, each with one of the possible wildcard values.\nIf you need to run all wildcard values (samples/files) at once, use expand().\ninput: \"ref.fa\" With wildcard: input: \"{smp}.fq\" Inputs can be named: input: fq=\"my.fq\", ref=\"ref.fa\" Use the expand() function: input: expand()\noutput\nLike input but specifies output files: files produced by the rule. If using wildcards, output should have the same wildcard(s) as input.\noutput: {smp}.bam\nlog\nLike output but meant for logging and error output: log files are not deleted if jobs fail.\nRecall that 2> redirects standard error whereas &> redirects standard out and standard error.\nWildcards can be used: log: log/map_{sample}.log\nUsing log files in actions: map.sh {input.fq} {input.ref} >{output} 2>{log} fastqc.sh {input} res &>{log}\nshell\nRun any arbitrary shell command: e.g. calling an external program or script.\nOther “action directives” that we did not use in this course include run for Python code entered directly in the Snakefile.\nshell: \"map.sh {input.ref} {input.fq} > {output}\"\nshell: \"cat {smp}.txt | tr a-z A-Z > {smp}.out\"\nshell: \"fastqc {input}\"\nparams\nCan be used to clearly and separately indicate certain variables/parameters that are used in the action.\nparams: minqual=\"30\", minlen=\"100\"\nparams: indir=\"res\" shell: \"my.sh {params.indir} {wildcards.smp}\"\nthreads\nThe number of CPUs/cores/threads to be used by a single job – corresponds to SLURM’s --cpus-per-task option.\nrule STAR:\n    ...\n    threads: 8\n    shell: \"STAR --runThreadN {threads} ...\"\nresources\nMostly arbitrary key-value pairs with resources: the keys should match those specified in a config.yaml profile config file (see below), so the appropriate resources are requested for the SLURM job.\nresources: mem_mb=50000\nconda\nUsed to specify a YAML file with a Conda environment description. Snakemake will perform the one-time Conda installation and use the resulting Conda environment when running the rule.\nNote: this also requires the --use-conda option to be specified when running Snakemake! ddddddddddddddddddddddddddddddddddddd\nconda: \"envs/fastqc.yaml\"\nAn example YAML file: \nchannels:\n  - bioconda\ndependencies:\n  - fastqc=0.11.8\n\nPlaceholders and wildcards\nNote that wildcards operate entirely within a single rule and not across rules! That is, even though it often makes sense to use the exact same wildcard across multiple rules, Snakemake will resolve them separately for each rule.\nPlaceholder\nExplanation\nExamples\n{input}\nRefers to the file(s) specified in the input directive – to be used in “action directives” such as shell. (If input has a wildcard, one file is passed for each job i.e. iteration of the rule.)\nshell: \"trim.sh {input} > {output}\"\nNamed input:shell: map.sh {input.ref} {input.fq} > {output}\n{output}\nRefers to the file(s) specified in the output directive – to be used in “action directives” such as shell.\nshell: \"trim.sh {input} > {output}\"\n{…}\nA wildcard, which can be given any name.\nUsed as-is in input, output, and log directives, for which Snakemake resolves the values of the wildcards by looking for requested output files it can produce.\nTo use a wildcard in a shell directive, assuming the wildcard is called sample, use: {wildcards.sample}. ddddddddddddddddddddddddddddddddddddd\ninput: {sample}.fq output: {sample}.bam\nTo use a wildcard in an action like a shell directive: shell: my.sh -i {wildcards.sample} > {output}\n\nFunctions\nSnakemake provides a few convenience functions, most notably expand() and glob_wildcards(). Note that you can interactively test these functions in Python after importing them using:\n# Just for interactive testing -- does not need to be done in a Snakefile:\nfrom snakemake.io import expand, glob_wildcards\nexpand()\nexpand() is a more succinct alternative to a list comprehension, which will replace one or more placeholders {} with all possible values from a list. If multiple lists are provided, all combinations of these lists will be generated – see the second example below:\n# Example with a single list \"SAMPLES\":\nSAMPLES=[\"sampleA\", \"sampleB\", \"sampleC\"] \nexpand(\"res/{sample}.bam\", sample=SAMPLES)\n#> ['res/smpA.bam', 'res/smpB.bam', 'res/smpC.bam']\n\n# Example with two lists, \"SAMPLES\" and \"READS\":\nSAMPLES = [\"sampleA\", \"sampleB\", \"sampleC\"]\nREADS = [\"R1\", \"R2\"]\nexpand(\"{sample}_{read}.fastq.gz\", sample=SAMPLES, read=READS)\n#> ['sampleA_R1.fastq.gz', 'sampleA_R2.fastq.gz',\n#>  'sampleB_R1.fastq.gz', 'sampleB_R2.fastq.gz',\n#>  'sampleC_R1.fastq.gz', 'sampleC_R2.fastq.gz']\nglob_wildcards()\nglob_wildcards() will perform shell globbing, i.e. search for existing files, and store one or more sets of values –usually sample names– that are extracted from the file names (akin to regex backreferences):\n# Typical usage at the top of a Snakefile:\nSAMPLES = glob_wildcards(\"data/{sample}.fastq\").sample\n\n# Or equivalently, with a trailing comma rather than trailing `.<wildcard-name>`:\nSAMPLES, = glob_wildcards(\"data/{sample}.fastq\")\n\n# With two wildcards:\nSAMPLES,READS = glob_wildcards(\"data/{sample}_{read}.fastq\")\n# Checking how it works in IPython -- with one wildcard:\n\n!ls data/\n#> data/sampleA.fastq data/sampleB.fastq data/sampleC.fastq\n\nglob_wildcards(\"data/{sample}.fastq\").sample\n#> ['sampleA', 'sampleB', 'sampleC']\n# Checking how it works in IPython -- with two wildcards:\n\n!ls data/\n#> A_R1.fastq.gz A_R2.fastq.gz B_R1.fastq.gz B_R2.fastq.gz\n\nglob_wildcards(\"data/{sample}_{read}.fastq.gz\").sample\n#> ['A', 'A', 'B', 'B']\nglob_wildcards(\"data/{sample}_{read}.fastq.gz\").read\n#> ['R1', 'R2', 'R1', 'R2']\nOthers\ntemp() will mark files as temporary – to be deleted if the workflow finishes:\noutput: temp(\"mapped/{sample}.bam\")\nprotected() will mark files as protected (no write permissions):\noutput: protected(\"sorted_reads/{sample}.bam\")\n\nMisc. tips & tricks\nConfiguration file\nTo avoid hardcoding certain run-specific variables in the Snakemake (sample names, output directories, parameters for software, and so on), you can use a YAML or JSON-formatted configuration file and include a configfile directive somewhere at the top of the Snakefile:\n# Include this line in the Snakefile to read the file \"config.yml\":\nconfigfile: \"config.yml\"\n\n# Now, the contents of \"config.yml\" is available in a dictionary:\nOUT_DIR=config[\"output_dir\"]\n# Say \"config.yaml\" just contains the following line:\noutput_dir: path/to/output/\nProfile dir with SLURM and other settings\nA configuration file (or set of files) can also be used to pass options to a Snakemake call. This is particularly handy with SLURM options, since a long command like snakemake -j100 --cluster \"--account=PAS1855 --time=12:00:00 --mem=12G\" is not very practical to type whenever running Snakemake.\nAdd a file called config.yaml in a directory with an arbitrary name – but something like slurm_profile makes sense.\nIn this file, specify options that can also be passed to snakemake on the command line, e.g.:\n# Just beware that options are followed by a colon \":\" in a YAML file:\n\ncluster: \"sbatch --account={resources.account}\n                 --time={resources.time_min}\n                 --mem={resources.mem_mb}\n                 --cpus-per-task={resources.cpus}\n                 --output=log/slurm-{rule}_{wildcards}.out\"\ndefault-resources: [cpus=1, mem_mb=1000, time_min=5, account=PAS1855]\n\n# You can also specify other options than cluster-specific ones:\njobs: 100\nlatency-wait: 30\nuse-conda: true     # \"--use-conda\" at the command line becomes this\nThen, use the --profile option and specify the name of the directory containing the config.yaml file:\n# (With the above config.yaml, we now no longer need to add the -j option:)   \nsnakemake --profile slurm_profile\nWhen using Snakemake with SLURM, it often makes sense to designate come rules as “local rules” meaning that they will not be submitted as SLURM jobs. This can be done with the localrules directive that can be specified somewhere near the top of the Snakefile:\nlocalrules: all, clean\nAlso, be aware that the main Snakemake process will run as long any job in the workflow is running, which could be days in some cases. Even though this process takes almost no resources, it would still be killed by OSC as soon as it runs over 20 minutes. Therefore, it often makes sense to run Snakemake as a SLURM job itself. Below, we create a script called snakemake_script.sh and then submit it:\n#!/bin/bash\n\n#SBATCH --account=PAS1855\n#SBATCH --time=24:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n\nset -e -u -o pipefail\n\nsnakemake -j1 -p\nsbatch snakemake_script.sh\n“Token” output file\nUse a “token” output file if a rule has no unique output, for example because it only modifies an existing file. Then, touch the token file in the rule’s action:\nrule token_example:\n    input:  'file.txt'\n    output: 'token_file'   # The name of the file is arbitrary - but it will be created\n    shell: \"my_command {input} && touch {output}\"\n\nA few worked examples\nSAMPLES = glob_wildcards(\"data/{smp}.fastq\").smp\n\nrule all:\n    input: \"res/count_table.txt\",\n\nrule trim:\n    input: \"data/{smp}.fastq\",\n    output: \"res/{smp}_trim.fastq\",\n    shell: \"scripts/trim.sh {input} > {output}\"\n\nrule map:\n    input: \"res/{smp}_trim.fastq\",\n    output: \"res/{smp}.bam\",\n    shell: \"scripts/map.sh {input} > {output}\"\n\nrule count:\n    input: expand(\"res/{smp}.bam\", smp=SAMPLES),\n    output: \"res/count_table.txt\",\n    shell: \"scripts/count.sh {input} > {output}\"\nSAMPLES = glob_wildcards(\"data/{smp}.fastq\").smp\n\nREF_FA = \"metadata/ref.fa\"\n\nlocalrules: all\n\nrule all:\n    input:\n        \"res/count_table.txt\",\n         expand(\"res/{smp}.fastqc.html\", smp=SAMPLES)\n\n\nrule trim:\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}_trim.fastq\",\n    log:\n        \"log/trim/{smp}.log\",\n    shell:\n        \"scripts/trim.sh {input} >{output} 2>{log}\"\n\n\nrule map:\n    input:\n        fastq=\"res/{smp}_trim.fastq\",\n        ref=REF_FA,\n    output:\n        \"res/{smp}.bam\",\n    log:\n        \"log/map/{smp}.log\",\n    shell:\n        \"scripts/map.sh {input.fastq} {input.ref} >{output} 2>{log}\"\n\n\nrule count:\n    input:\n        expand(\"res/{smp}.bam\", smp=SAMPLES),\n    output:\n        \"res/count_table.txt\",\n    log:\n        \"log/count/count.log\",\n    shell:\n        \"scripts/count.sh {input} >{output} 2>{log}\"\n\n\nrule fastqc:\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}.fastqc.html\",\n    log:\n        \"log/fastqc/{smp}.log\",\n    shell:\n        \"scripts/fastqc.sh {input} res &>{log}\"\n\n\n\n",
      "last_modified": "2021-04-25T17:51:59-04:00"
    },
    {
      "path": "resources.html",
      "title": "Further resources",
      "author": [],
      "contents": "\n\nContents\nShell\nBooks\nOnline guides and tutorials\nFurther reading\n\nGit\nBooks\nOnline guides and tutorials\nFurther reading\n\nPython\nBooks\nCourses\nOnline guides and tutorials\nFurther reading\n\nSnakemake\nMiscellaneous\nBooks similar to CSB\nReproducibility and best practices\nGenomics and applied bioinformatics\n\n\n\nThis list of further resources is organized by the main topics covered in this course – see the Contents on the left-hand site.\n\nShell\nBooks\nThe first two are available online at the OSU library:\nThe Linux Command Line: A Complete Introduction (William Shotts, 2019)\nLinux Command Line and Shell Scripting Bible (Christine Bresnahan, 2015)\nCommand Line Kung Fu: Bash Scripting Tricks, Linux Shell Programming Tips, and Bash One-liners (Jason Cannon, 2014)\nOnline guides and tutorials\nNice collections of one-liners, mostly for bioinformatics\nBy Bonnie I-Man Ng\nBy Stephen Turner\nBy Tommy Ming\nBy Amy Williams\n\nThe Bash Guide by Maarten Billemont with separate very useful FAQ and pitfalls pages.\nBash Guide for Beginners by Machtel Garrels.\nFurther reading\nTen simple rules for getting started with command-line bioinformatics (Brandies & Hogg 2021, PLoS Computational Biology)\nFive reasons why researchers should learn to love the command line (Perkel 2021, Nature)\n\nGit\nBooks\nI wouldn’t necessarily recommend diving so deep into Git as to read a book about it, but this book provides an excellent reference, is quite accessible, and is freely available online:\nPro Git (Chacon & Straub, 2014)\nOnline guides and tutorials\nGeneral\nAn overview of Git tutorials\nHappy Git and GitHub for the useR\nSomewhat R-centric, but a very accessible introduction to Git.\nGit best practices by Seth Robertson\nAtlassian Git tutorials\nAtlassian is behind Bitbucket, an alternative to GitHub that also hosts Git repositories, and its Git tutorials are very useful.\nA quick GitHub overview of some Git and GitHub functionality including branching and Pull Requests, and how to do these things in your browser at GitHub.\nInteractive practice\nGit-it – a small application to learn and practice Git and GitHub basics.\nVisualizing Git\nThese visualizations can help to get some intuition for Git. Note that at the prompt, you can only type Git commands and since there are no actual files involved, you can’t use git add – just commit straight away.\nLearn Git branching\nUndoing\nSome slides on undoing that we did not get to in our Git week.\nHow to undo (almost) anything with Git – by the GitHub blog\nOh Shit, Git!?! – by Katie Sylor-Miller\nGit flight rules – by Kate Hudson.\nCovers much more than undoing.\nFurther reading\nExcuse me, do you have a moment to talk about version control? (Bryan 2017, PeerJ)\nTen Simple Rules for Taking Advantage of Git and GitHub (Perez-Riverol et al. 2016, PLoS Comutational Biology)\n\nPython\nBooks\nThese are all available online, freely or via the OSU library:\nPython for the life sciences: a gentle introduction to python for life scientists (Alexandar Lancestar, 2019).\nVery explicitly geared towards biologists with no or little programming experience, and takes a very practical and project-oriented approach. From what I’ve seen of the book, I can highly recommended it!\nPython for Bioinformatics (Sebastian Bassi, 2018).\nStarts with an introduction to Python and then has chapters that each describe practical problems/projects for Python. (Associated GitHub repository.)\nPython programming for biology, bioinformatics, and beyond (Tim Stevens, 2015).\nStarts with an introduction to Python and then has chapters on topics like “Pairwise sequence alignments”, “Sequence variation and evolution”, and “High-throughput sequences”.\nReproducible Bioinformatics with Python (Ken Youens-Clark, 2021).\nA slightly more advanced book that does not start with an introduction to Python, but you should be able to follow the book with what you’ve learned in this course.\nPython Data Science Handbook (Jake VanderPlas, 2016).\nFocused on NumPy, Pandas, visualization with Matplotlib, and machine learning with scikit-learn. Freely available online.\nPython for Data Analysis (Wes McKinney, 2017).\nFocused on NumPy, Pandas, and visualization with Matplotlib.\nDive Into Python 3 (Mark Pilgrim, 2009).\nCourses\nPython for everybody – Includes course materials and lectures, and is also available at Coursera and edX.\nProgramming for Biology – This is the Cold Spring Harbor course that your TA Zach took, and the materials are available online.\nUsing Python for Research Free edX course – you can also find links to just the videos towards the bottom of this page.\nData analysis with Python at freeCodeCamp\nVideos from the MIT course “Introduction to Computer Science and Programming in Python”\nOnline guides and tutorials\nGeneral\nA YouTube playlist of Microsoft videos introducing Python\nPEP 8 — the Style Guide for Python Code\nLibrary-specific material\nThe official NumPy tutorial\nPandas tutorials and documentation\nMaterial for a BioPython workshop\nThe BioPython Tutorial and Cookbook\nVS Code and Python\nVS Code documentation on its “Python Interactive Window”.\nVS Code documentation on its Jupyter Notebook implementation.\nJupyter Notebooks / JupyterLab\nYou can also use the Jupyter Notebooks / JupyterLab as an Interactive App at OSC OnDemand. If you’re interested in using this, I would recommend trying JupyterLab which can run Jupyter Notebooks but also regular Python scripts, a shell, and so forth.\n6-minute video: How to use JupyterLab.\nThis JupyterLab documentation provides a nice introduction to JupyterLab features (the link goes to documentation for a version close to the one at OSC).\nFor a general introduction to Jupyter Notebooks, see also How to Use Jupyter Notebook in 2020: A Beginner’s Tutorial.\nFurther reading\nTen simple rules for writing and sharing computational analyses in Jupyter Notebooks – Rule et al. 2019, PLoS Computational Biology\nStack Overflow: “The Incredible Growth of Python” (2017)\n\nSnakemake\nThe official Snakemake tutorial.\nCan be run in your browser!\nA short Snakemake introduction by Vince Buffalo\nA useful Snakemake tutorial by NBIS\nA Carpentries lesson on working on a compute cluster, with a large section on Snakemake starting on this page.\nFurther reading\nWorkflow systems turn raw data into scientific knowledge (Perkel 2019, Nature “Toolbox” feature).\nSustainable data analysis with Snakemake (Mölder et al. 2020, Zenodo).\n\nMiscellaneous\nBooks similar to CSB\nA Primer for Computational Biology\nPractical Computing for Biologists (Haddock & Dunn, 2011)\nReproducibility and best practices\nGood enough practices in scientific computing (Wilson et al. 2017, PLoS Computational Biology)\nStreamlining data-intensive biology with workflow systems (Reiter et al. 2021, GigaScience)\nReproducible Research: A Retrospective (Peng & Hicks 2021, Annu Rev Public Health)\nTen Simple Rules for Reproducible Computational Research (Sandve et al. 2013, PLoS Computational Biology)\nThe plain person’s guide to plain text social science (Kieran Healy)\nGenomics and applied bioinformatics\nA few courses on genomic data analysis with lots of great online material available:\nIntroduction to Bioinformatics and Computational Biology by Shirley Liu\nApplied Genomics course by Michael Schatz\nApplied Computational Genomics course by Aaron Quinlan\n\nSites with online material for many computational biology workshops:\nUC Davis Bioinformatics Core\nHarvard Chan Bioinformatics Core\nBioinformatics.ca\nSummer Institute in Statistical Genetics\nGenomics Curriculum of Data Carpentry\n\nRosalind – A website with bioinformatics exercises\n\n\n\n",
      "last_modified": "2021-04-25T17:52:00-04:00"
    },
    {
      "path": "w01_exercises.html",
      "title": "Week 1 Exercises",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nGetting set up\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\nSolutions\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\n\nThe following exercises were copied from Chapter 1 of the CSB book, with hints and solutions modified from those provided in the book’s Github repo.\n(The exercises marked as “Advanced” are omitted since they require topics not actually covered in the chapter, which we will cover in later modules.)\n\nMain exercises\nGetting set up\nYou should already have the book’s Github repository with exercise files.\nIf not, go to /fs/ess/PAS1855/users/$USER, and run git clone https://github.com/CSB-book/CSB.git.\nNow, you should have the CSB directory referred to in the first step of the first exercise.\nIntermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\n\nShow hints\n\nRecall that .. is one level up in the dir tree, and that you can combine multiple .. in a single statement.\n\n(d) Use an absolute path to go to the sandbox directory within python.\n(e) Return to the data directory within the python directory.\n\nShow hints\n\nRecall that there is a shortcut to return to your most recent working dir.\n\nIntermezzo 1.2\nTo familiarize yourself with these basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\n\nShow hints\n\nThe touch command creates an empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt.\nIntermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? Which is the last?\n\nShow hints\n\nYou can either first select the 5th column using cut, and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter.\nTo view just the first or the last line, pipe to head or tail.\n\n(b) How many families are represented in the database?\n\nShow hints\n\nStart by selecting the relevant column.\nUse a tail trick shown in the chapter to exclude the first line.\nRemember to sort before using uniq.\n1.10.1 Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (>).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\n\nShow hints\n\nTo show the size of a file you can use the -l option of the command ls, and to display human-readable file sizes, the -h option.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\nShow hints\n\nRecall that with the cp command, it is also possible to give the copy a new name at once.\n\n4. How many contigs are classified as isogroup00036?\n\nShow hints\n\nIs there a grep option that counts the number of occurrences?\nAlternatively, you can pass the output of grep to wc -l.\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\nShow hints\n\nIn the file, the information on each contig is separated by two spaces:\n>contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n>contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\nIn Linux, we can’t write output to a file that also serves as input (see here), so the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" > myfile.txt # Don't do this! \nIn this case, we will have to save the output in a temporary file and on a separate line, overwrite the original file using mv.\n\n6. How many unique isogroups are in the file?\n\nShow hints\n\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\nShow hints\n\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically.\n\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nHow many times were the levels of individuals 3 and 27 recorded?\n\nShow hints\n\nYou can use cut to extract just the maleID from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”.\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\n\nShow hints\n\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written.\nIn the script, you can use the first two so-called positional parameters, $1 and $2, to represent the file name and the maleID, respectively.\n$1 and $2 represent the first and the second argument passed to a script like so: bash myscript.sh arg1 arg2.\n1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\n$ bash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\n\nShow hints\n\nTo build the script, you need to combine several commands:\nTo find the number of rows, you can use wc.\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n, and count characters.\n1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nFor example, running the script with:\n$ bash explore.sh ../data/Buzzard2015_data.csv 7\nshould return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\n\nShow hints\n\nYou can select a given column from a csv file using the command cut. Then:\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the positional parameters $1 and $2 for the file name and column number, respectively.\n\n\nSolutions\nIntermezzo 1.1\n\nSolution\n\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER.\n$ cd /fs/ess/PAS1855/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS1855/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\ncd -\n\nIntermezzo 1.2\n\nSolution\n\n(a) Go to the data directory within CSB/unix.\n$ cd /fs/ess/PAS1855/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory         without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\nIntermezzo 1.3\n\nSolution\n\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? And which is the last?\ncd ∼/CSB/unix/data/\n\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\n\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\n\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t\";\" -k 5 Pacifici2013_data.csv | head -n 1\n\n\nFollowing the output that you wanted, you may have gotten errors like this:\nsort: write failed: 'standard output': Broken pipe\nsort: write error\nThis may seem disconcerting, but is nothing to worry about, and has to do with the way data streams through a pipe: after head/tail exits because it has done what was asked (print one line), sort or cut may still try to pass on data.\n\n\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n\n1.10.1 Next-Generation Sequencing Data\n1. Change directory to CSB/unix/sandbox.\n\ncd /fs/ess/PAS1855/users/$USER/CSB/unix/sandbox\n\n2. What is the size of the file Marra2014_data.fasta?\n\nls -lh ../data/Marra2014_data.fasta\n# Among other output, this should show a file size of 553K\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\ncp ../data/Marra2014_data.fasta my_file.fasta\nTo make sure the copy went well, list the files in the sandbox:\nls\n\n4. How many contigs are classified as isogroup00036?\n\nTo count the occurrences of a given string, use grep with the option -c\ngrep -c isogroup00036 my_file.fasta \n# 16\nYou can also use a pipe and wc -l to count:\ngrep isogroup00036 my_file.fasta | wc -l\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,:\ncat my_file.fasta | tr -s ' ' ',' > my_file.tmp\nmv my_file.tmp my_file.fasta\n\n6. How many unique isogroups are in the file?\n\nFirst, searching for > with grep will extract all lines with contig information:\ngrep '>' my_file.fasta | head -n 2\n# >contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n# >contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, use cut to extract the 4th column\ngrep '>' my_file.fasta | cut -d ',' -f 4 | head -n 2\n#gene=isogroup00001\n#gene=isogroup00001\nFinally, use sort -> uniq -> wc -l to count the number of unique occurrences:\ngrep '>' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n# 43\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\nWe need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n# >contig00001,numreads=2\n# >contig00002,numreads=8\n# >contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t ‘=’ to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n# >contig00089,numreads=1\n# >contig00176,numreads=1\n# >contig00210,numreads=1\n# >contig00001,numreads=2\n# >contig00003,numreads=2\nUsing the flag -r we can sort in reverse order:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n# >contig00302,numreads=3330\nFinding that contig 00302 has the highest coverage, with 3330 reads.\n\n\n1.10.2 Hormone Levels in Baboons\n1. How many times were the levels of individuals 3 and 27 recorded?\n\nFirst, let’s see the structure of the file:\nhead -n 3 ../data/Gesquiere2011_data.csv \n# maleID        GC      T\n# 1     66.9    64.57\n# 1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 ../data/Gesquiere2011_data.csv | head -n 3\n# maleID\n# 1\n# 1\nThen we can pipe the results to grep -c to count the number of occurrences (note the flag -w as we want to match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 3\n# 61\n\n# For maleID 27\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 27\n# 5\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\n\nIn the script, we just need to incorporate the arguments given when calling the script, using $1 for the file name and $2 for the individual ID, into the commands that we used above:\ncut -f 1 $1 | grep -c -w $2\nA slightly more verbose and readable example:\n#!/bin/bash\n\nfilename=$1\nind_ID=$2\necho \"Counting the nr of occurrences of individual ${ind_ID} in file ${filename}:\" \ncut -f 1 ${filename} | grep -c -w ${ind_ID}\n\n\nVariables are assigned using name=value (no dollar sign!), and recalled using $name or ${name}. It is good practice to put curly braces around the variable name. We will talk more about bash variables in the next few weeks.\n\n\nTo run the script, assuming it is named count_baboons.sh:\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n1.10.3 Plant–Pollinator Networks\n\nSolution\n\nCounting rows:\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\nTo avoid printing the file name, we can either use cat or input redirection:\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l < ../data/Saavedra2013/n10.txt \nCounting rows:\nCounting the number of columns is more work. First, we need only the first line:\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\nNow we can remove all spaces and the line terminator using tr:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\nFinally, we can use wc -c to count the number of characters in the string:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nFinal script:\n#!/bin/bash\n\nfilename=$1\n\necho \"Filename:\"\necho ${filename}\necho \"Number of rows:\"\ncat ${filename} | wc -l\necho \"Number of columns:\"\nhead -n 1 ${filename} | tr -d ' ' | tr -d '\\n' | wc -c\nTo run the script, assuming it is named counter.sh:\nbash counter.sh ../data/Saavedra2013/n10.txt\n# 5\n\n\nWe’ll learn about a quicker and more general way to count columns in a few weeks.\n\n\n\n1.10.4 Data Explorer\n\nSolution\n\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\nHere is an example of what the script could look like:\n#!/bin/bash\n\nfilename=$1\ncolumn_nr=$2\n\necho \"Column name\"\ncut -d ',' -f ${column_nr} ${filename} | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n",
      "last_modified": "2021-04-25T17:52:01-04:00"
    },
    {
      "path": "w01_readings.html",
      "title": "Week 1 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, you will get an introduction to and overview of the course (Tuesday session) and will learn the basics of working in a Unix shell environment (Thursday session).\nSome of the things you will learn this week:\nCourse intro & overview (Tuesday class)\nWhat to expect from this course.\nWhich tools and languages we will use.\nWhat is expected of you during this course.\nGet up to speed on the infrastructure of the course.\nUnix shell basics (Thursday class, Readings, and Exercises)\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing the shell, learn how to:\nNavigate around your computer.\nCreate and manage directories and files.\nFind and view text files.\nSearch within, manipulate, and extract information from text files.\n\nReadings\nThe first few pages of our primary book, Computing Skills for Biologists by Allesina & Wilmes (CSB for short), should give you an introduction to the rationale behind the book, and behind this course, too. Try to read this before Tuesday’s class.\nOur main text for this week will be Chapter 1 from CSB, which will introduce you to using the Unix shell. Please read this chapter before Thursday’s class when we will go through the chapter by means of “participatory live-coding” (code-along). Highlight sections that you find challenging and ask about these in class as needed. Because repetition is beneficial, you are welcome to actively work through the code in the chapter beforehand – this will be particularly useful for those who have never used a shell before.\n(Side note: we will cover many of the commands from this week in more detail in weeks 4-6, using the Buffalo book.)\nRequired readings\nCSB Chapter 0: “Introduction Building a Computing Toolbox” – Introduction & Section 0.1 only\nCSB Chapter 1: “Unix”\nFurther resources\nBuffalo Preface and Chapter 1: “How to Lean Bioinformatics”\nA Software Carpentry shell tutorial – For another angle on similar material to what we cover this week.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:01-04:00"
    },
    {
      "path": "w01_UA_installation.html",
      "title": "Week 1 -- Optional assignment: <br> Local software installation",
      "author": [],
      "contents": "\nWhat?\nIn case you want to be able to work locally on your computer (in addition to working at OSC, as we will in class), install the following free software packages.\nFor all operating systems:\nVS Code: a text editor with a ton of functionality for coding and accessing remote computers.\nPython: the main programming language used in this course.\nFor all Windows versions:\nGit: version control software\nFor Windows 10 only:\nWSL: Windows Subsystem for Linux, a lightweight Virtual Machine-like application to run Linux inside Windows.\nFor Mac only:\nXcode command-line tools (includes Git)\nHomebrew\n(Because Mac and Linux users have a built-in Unix shell, and git for Windows ships with one, no-one should need to install a shell separately.)\nWhy?\nSince all the software needed for this course is available at the Ohio Supercomputer Center (OSC), you are not required to install anything for this course. However, you may still want to install software locally so you are not completely dependent on OSC and don’t have to work inside a browser.\nIf you run into problems while installing software, please contact the instructors, but be aware that troubleshooting installation in Windows may be challenging for us.\nHow?\nVS Code\nFollow these instructions: Windows / Mac / Linux.\nPython 3.7\nFollow the instructions at http://computingskillsforbiologists.com/setup/basic-programming/.\nFor Windows users only:\nGit for Windows\nIf you have Windows 10, I recommend installing WSL and then installing Git inside your Linux distribution in WSL (for instructions see the WSL section below), so you can skip the installation of Git for Windows. However, you may also want to version control directories outside of your WSL partition, in which case you should also install Git for Windows.\nDownload Git for Windows from here and install it using all the default settings for the installation, except:\nIn “Adjusting Your PATH Environment”, select “Use Git from Git Bash Only”.\nIn the prompt “Configuring the Line Ending Conversions”, choose “Checkout as-is, commit as-is”.\nFor Windows 10 users only:\nWSL\nFollow these installation instructions Additional notes:\nIf you’re doing a manual installation or are otherwise prompted to pick a Linux distribution, I would recommend picking Ubuntu 20.04.\nVS Code (which you installed earlier) has an integrated terminal, but this terminal will by default use a Windows shell. To tell it to use the bash shell from WSL instead:\nOpen the Command Palette (Ctrl + Shift + P)\nType “settings.json”, and open up the Settings (JSON) file.\nAdd this line: \"terminal.integrated.shell.windows\": \"C:\\\\WINDOWS\\\\sysnative\\\\bash.exe\"\n\nOpen a terminal in VS Code using “View” => “Integrated Terminal”, which should get you a bash shell after following the previous step.\nInstall Git by issuing the following command in the shell: sudo apt install git.\nFor Mac users only:\nXcode command-line tools (includes Git)\nOpen a terminal and enter: xcode-select --install.\nHomebrew package manager\nFollow the instructions at https://brew.sh/.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:01-04:00"
    },
    {
      "path": "w01_UA_OSC.html",
      "title": "Week 1 -- Ungraded Assignment : <br> Create an OSC account",
      "author": [],
      "contents": "\nWhat?\nGet access to OSC (or check that you still do) and to the OSC Project for this course (PAS1855).\nWhy?\nMuch of the coding during this course will be done at the Ohio Supercomputer Center (OSC), where we can access a terminal and the text editor for this course, VS Code, all from inside our browser.\nThis course has its own OSC Project (PAS1855), and access to this project will allow you to access our shared files and to reserve “compute nodes”.\nHow?\nAfter filling out the pre-course survey, you will receive an invitation email from OSC referring to the course project number PAS1855.\nIf you already have an OSC account, the email will likely tell you that you have been added to the project and don’t have to do anything. Please check whether you can log in by going to https://my.osc.edu, and see if the project PAS1855 is listed in your Project List (scroll down or click “Project” and then “Project List” in the top navigation bar).\nIf you don’t have an OSC account yet, follow the instructions in the email to sign up for OSC and accept the invitation. When you fill out the form with your personal details, provide the course OSC Project number PAS1855 in the box highlighted in red below:\n\n\n\n\n\n",
      "last_modified": "2021-04-25T17:52:02-04:00"
    },
    {
      "path": "w02_exercises.html",
      "title": "Exercises: Week 2",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nExercise 1: Course notes in Markdown\nExercise 2\n\nBonus exercises\nExercise 3\nBuffalo Chapter 3 code-along\n\nSolutions\nExercise 2\n\n\nMain exercises\nExercise 1: Course notes in Markdown\nCreate a Markdown document with course notes. I recommend writing this document in VS Code.\nMake notes of this week’s material in some detail. If you have notes from last week in another format, include those too. (And try to keep using this document throughout the course!)\nSome pointers:\nUse several header levels and use them consistently: e.g. a level 1 header (#) for the document’s title, level 2 headers (##) for each week, and so on.\nThough this should foremost be a functional document for notes, try to incorporate any appropriate formatting option: e.g. bold text, italic text, inline code, code blocks, ordered/unordered lists, hyperlinks, and perhaps a figure.\nMake sure you understand and try out how Markdown deals with whitespace, e.g. starting a new paragraph and how to force a newline.\nExercise 2\nWhile doing this exercise, save the commands you use in a text document – either write in a text document in VS Code and send the commands to the terminal, or copy them into a text document later.\nGetting set up\nCreate a directory for this exercise, and change your working dir to go there. You can do this either in your $HOME dir (e.g. ~/pracs-sp21/w02/ex2/) or your personal dir in the course’s project dir (/fs/ess/PAS1855/users/$USER/w02/ex2/).\nCreate a disorganized mock project\nUsing the touch command and brace expansions, create a mock project by creating 100s of empty files, either in a single directory or a disorganized directory structure.\nIf you want, you can create file types according to what you typically have in your project – otherwise, a suggestion is to create files with:\nRaw data (e.g. .fastq.gz)\nReference data (e.g. .fasta),\nMetadata (e.g. .txt or .csv)\nProcessed data and results (e.g. .bam, .out)\nScripts (e.g. .sh, .py or .R)\nFigures (e.g. .png or .eps)\nNotes (.txt and/or .md)\nPerhaps some other file type you usually have in your projects.\n\nOrganize the mock project\nOrganize the mock project according to some of the principles we discussed this week.\nEven while adhering to these principles, there is plenty of wiggle room and no single perfect dir structure: what is optimal will depend on what works for you and on the project size and structure. Therefore, think about what makes sense to you, and what makes sense given the files you find yourself with.\nTry to use as few commands as possible to move the files – use wildcards!\nChange file permissions\nMake sure no-one has write permissions for the raw data files. You can also change other permissions to what you think is reasonable or necessary precaution for your fictional project.\n\nHints\n\nUse the chmod command to change file permissions and recall that you can use wildcard expansion to operate on many files at once.\nSee the slides starting from here for an overview of file permissions and the chmod command.\nAlternatively, chmod also has an -R argument to act recursively: that is, to act on dirs and all of their contents (including other dirs and their contents).\n\nCreate mock alignment files\nCreate a directory alignment inside an appropriate dir in your project (e.g. analysis, results, or a dir for processed data), and create files for all combinations of 30 samples (01-30), 5 treatments (A-E), and 2 dates (08-14-2020 and 09-16-2020), like so: sample01_A_08-14-2020.sam - sample50_H_09-16-2020.sam.\nThese 300 files can be created with a single touch command. (If you already had an alignment dir, first delete its contents or rename it.)\n\nHints\n\nUse brace expansion three times: to expand sample IDs, treatments, and dates.\nNote that {01..20} will successfully zero-pad single-digit numbers.\n\nRename files in a batch\nWoops! We stored the alignment files that we created in the previous step as SAM files (.sam), but this was a mistake – the files are actually the binary counterparts of SAM files: BAM files (.bam).\nMove into the dir with your misnamed BAM files, and use a for loop to rename them: change the extension from .sam to .bam.\n\nHints\n\nLoop over the files using globbing (wildcard expansion) directly; there is no need to call ls.\nUse the basename command, or alternatively, cut, to strip the extension.\nStore the output of the basename (or cut) call using command substitution ($(command) syntax).\nThe new extension can simply be pasted behind the file name, like newname=\"$filename_no_extension\"bam or newname=$(basename ...)bam.\nCopy files with wildcards\nStill in the dir with your SAM files, create a new dir called subset. Then, using a single cp command, copy files that satisfy the following conditions into the subset dir:\nThe sample ID/number should be 01-19;\nThe treatment should be A, B, or C.\nCreate a README.md in the dir that explains what you did.\n\nHints\n\nJust like you used multiple consecutive brace expansions above, you can use two consecutive wildcard character sets ([]) here.\n\nBonus: a trickier wildcard selection\nStill in the dir with your SAM files, create a new dir subset2. Then, copy all files except the one for “sample28” into this dir.\nDo so using a single cp command, though you’ll need two separate wildcard expansion or brace expansion arguments (as in cp wildcard-selection1 wildcard-selection2 destination/).\n\nHints\n\nWhen using brace expansion ({}), simply use two numeric ranges: one for IDs smaller than and one for IDs larger than the focal ID.\nWhen trying to do this with wildcard character sets ([]), you’ll run into one of its limits: you can’t combine conditions with a logical or. Therefore, to exclude only sample 28, you have to separately select IDs that (1) do not start with a 2 and (2) start with a 2 but do not end with an 8.\nBonus: a trickier renaming loop\nYou now realize that your date format is suboptimal (MM-DD-YYYY; which gave 08-14-2020 and 09-16-2020) and that you should use the YYYY-MM-DD format. Use a for loop to rename the files.\n\nHints\n\nUse cut to extract the three elements of the date (day, month, and year) on three separate lines.\nStore the output of these lines in variables using commands substitution, like: day=$(commands).\nFinally, paste your new file name together like: newname=\"$part1\"_\"$year\" etc.\nWhen first writing your commands, it’s helpful to be able to experiment easily: start by echo-ing a single example file name, as in: echo sample23_C_09-16-2020.sam | cut ....\nCreate a README\nInclude a README.md that described what you did; again, try to get familiar with Markdown syntax by using formatting liberally.\nBonus exercises\nExercise 3\nIf you feel like it would be good to reorganize one of your own, real projects, you can do so using what you’ve learned this week. Make sure you create a backup copy of the entire project first!\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3.\n\nSolutions\nExercise 2\nGetting set up\n\n\nmkdir ~/pracs-sp21/w02/ex2/ # or similar, whatever dir you chose\ncd !$                       # !$ is a shortcut to recall the last argument from the last commands\n\nCreate a disorganized mock project\n\nAn example:\n\ntouch sample{001..150}_{F,R}.fastq.gz\ntouch ref.fasta ref.fai\ntouch sample_info.csv sequence_barcodes.txt\ntouch sample{001..150}{.bam,.bam.bai,_fastqc.zip,_fastqc.html} gene-counts.tsv DE-results.txt GO-out.txt\ntouch fastqc.sh multiqc.sh align.sh sort_bam.sh count1.py count2.py DE.R GO.R KEGG.R\ntouch Fig{01..05}.png all_qc_plots.eps weird-sample.png\ntouch dontforget.txt README.md README_DE.md tmp5.txt\ntouch slurm-84789570.out slurm-84789571.out slurm-84789572.out\n\nOrganize the mock project\n\nAn example:\n\nCreate directories:\nmkdir -p data/{raw,meta,ref}\nmkdir -p results/{alignment,counts,DE,enrichment,logfiles,qc/figures}\nmkdir -p scripts\nmkdir -p figures/{ms,sandbox}\nmkdir -p doc/misc\nMove files:\nmv *fastq.gz data/raw/\nmv ref.fa* data/ref/\nmv sample_info.csv sequence_barcodes.txt data/meta/\nmv *.bam *.bam.bai results/alignment/\nmv *fastqc* results/qc/\nmv gene-counts.tsv results/counts/\nmv DE-results.txt results/DE/\nmv GO-out.txt results/enrichment/\nmv *.sh *.R *.py scripts/\nmv README_DE.md results/DE/\nmv Fig[0-9][0-9]* figures/ms\nmv weird-sample.png figures/sandbox\nmv all_qc_plots.eps results/qc/figures/\nmv dontforget.txt tmp5.txt doc/misc/\nmv slurm* results/logfiles/\n\nChange file permissions\n\nTo ensure that no-one has write permission for the raw data, you could, for example, use:\n\nchmod a=r data/raw/*   # set permissions for \"a\" (all) to \"r\" (read)\n\nchmod a-w data/raw/*   # take away \"w\" (write) permissions for \"a\" (all)\n\nCreate mock alignment files\n\n\n$ mkdir -p results/alignment\n$ # rm results/alignment/* # In the example above, we already had such a dir with files\n$ cd results/alignment \n\n# Create the files:\n$ touch sample{01..30}_{A..E}_{08-14-2020,09-16-2020}.sam\n\n# Check if we have 300 files:\n$ ls | wc -l\n# 300\n\nRename files in a batch\n\n\n# cd results/alignment  # If you weren't already there\n\n# Use *globbing* to match the files to loop over (rather than `ls`):\nfor oldname in *.sam\ndo\n   # Remove the `sam` suffix using `basename $oldname sam`,\n   # use command substitution (`$()` syntax) to catch the output of the\n   # `basename` command, and paste `bam` at the end:\n   newname=$(basename $oldname sam)bam\n   \n   # Report what we have:\n   # (Using `-e` with echo we can print an extra newline with '\\n`,\n   # to separate files by an empty line)\n   echo \"Old name: $oldname\"\n   echo -e \"New name: $newname \\n\"\n   \n   # Execute the move:\n   mv \"$oldname\" \"$newname\"\ndone\nA couple of take-aways:\nNote that we don’t need a special construction to paste strings together. we simply type bam after what will be the extension-less file name from the basename command.\nWe print the old and new names to screen; this is not necessary, of course, but good practice. Moreover, this way we can test whether our loop works before adding the mv command.\nWe use informative variable names (oldname and newname), not cryptic ones like i and o.\nCopy files with wildcards\n\n\nCreate the new dir:\nmkdir subset\nCopy the files using four consecutive wildcard selections:\nThe first digit should be a 0 or a 1 [0-1],\nThe second can be any number [0-9] (? would work, too),\nThe third, after an underscore, should be A, B, or C [A-C],\nWe don’t care about what comes after that, but do need to account for the additional characters, so will use a * to match any character:\n\ncp sample[0-1][0-9]_[A-C]* subset/\nReport what we did, including a command substitution to insert the current date:\necho \"On $(date), created a dir \"subset\" and copied only files for samples 1-29 \\\nand treatments A-D into this dir\" > subset/README.md\n\n# See if it worked:\ncat subset/README.md\n\nBonus: a trickier wildcard selection\n\n\nCreate the new dir:\nmkdir subset2\nThe most straightforward way in this case is using two brace expansion selections, one for sample numbers smaller than 28, and one for sample numbers larger than 28:\ncp sample{01..27}* sample{29..30}* subset2/\nHowever, we may not always be able to use ranges like that, and being a little creative with wildcard expansion also works — first, we select all samples not starting with a 2, and then among samples that do start with a 2, we exclude 28:\ncp sample[^2]* sample2[^8]* subset2/\n\nBonus: a trickier renaming loop\n\n\nfor oldname in *.bam\ndo\n   # Use `cut` to extract month, day, year, and a \"prefix\" that contains\n   # the sample number and the treatment, and save these using command substitution:\n   month=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 1)\n   day=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 2)\n   year=$(basename \"$oldname\" .bam | cut -d \"_\" -f 3 | cut -d \"-\" -f 3)\n   prefix=$(echo \"$oldname\" | cut -d \"_\" -f 1-2)\n   \n   # Paste together the new name:\n   # (This will fail without quotes around prefix, because the underscore\n   # is then interpreted as being part of the variable name.)\n   newname=\"$prefix\"_\"$year\"-\"$month\"-\"$day\".bam\n   \n   # Report what we have:\n   echo \"Old name: $oldname\"\n   echo -e \"New name: $newname \\n\"\n   \n   # Execute the move:\n   mv \"$oldname\" \"$newname\"\ndone\n\n\nThis renaming task can be done more succinctly using regular expressions and the sed command – we’ll learn about both of these topics later in the course.\n\n\n\n\n\n\n",
      "last_modified": "2021-04-25T17:52:02-04:00"
    },
    {
      "path": "w02_readings.html",
      "title": "Week 2 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, we’ll talk about some best practices for project organization, managing your project’s files in the Unix shell, and documenting your project with Markdown files. We’ll also spend a bit of time getting to know VS Code, the text editor that you will spend a lot of time in during this course.\nIn an ungraded assignment to get set up for upcoming modules, you will sign up for a GitHub account.\nLike last week, there are exercises but no graded assignments (the first one will be issued next week).\nSome of the things you will learn this week:\nSome best practices for project organization, documentation, and management.\nHow to apply these best practices this in the shell.\nHow to use Markdown for documentation (and beyond).\nGet to know our text editor for the course, VS Code, a bit better.\nReadings\nThis week’s reading is Chapter 2 of our secondary book, “Bioinformatics Data Skills” (“Buffalo” for short – the author is Vince Buffalo), which deals with project organization and (file) management.\nWhile very useful for any biologist interested in computational skills, the Buffalo book is geared towards bioinformatics data, especially genomic data from Next-Generation Sequencing (NGS) experiments. If you don’t work with sequencing data, don’t fret about understanding the specifics of some of the examples and recommendations, and hopefully, the more general underlying ideas will still be helpful.\nThe chapter also contains several tips and tricks for using the shell for project organization, which we will work through in the Zoom sessions. For more material along these lines, optional reading is Buffalo Chapter 3.\nRequired readings\nBuffalo Chapter 2: “Setting up and Managing a Bioinformatics Project”\nOptional readings\nBuffalo Chapter 3: “Remedial Unix Shell”\nFurther resources\nWilson et al. 2017, PLOS Computational Biology: “Good enough practices in scientific computing”\nKieran Healy: “The Plain Person’s Guide to Plain Text Social Science”\n\n\n\n",
      "last_modified": "2021-04-25T17:52:03-04:00"
    },
    {
      "path": "w02_UA_github-signup.html",
      "title": "Week 2 -- Ungraded Assignment: <br> Create a GitHub account",
      "author": [],
      "contents": "\nWhat?\nCreate a Github account and let me know you’ve done so.\nWhy?\nGithub is a website that hosts Git repositories, i.e. version-controlled projects. In Module 3 of this course, we will be learning how to use Git together with GitHub. In addition, all graded assignments for this course will be submitted through Github.\nHow?\nIf you already have a GitHub account, log in and start at step 6.\nGo to https://github.com.\nClick “Sign Up” in the top right.\nFill out the form:\nWhen choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use GitHub to share your code, for instance when publishing a paper.)\nYou can choose for yoursefl whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.)\nYou’ll probably want to uncheck the box under “Email Preferences”.\nWhen you’re done, click “Create account”.\n\nYou can answer the questions that GitHub will now ask you, but you should also be able to just skip them.\nCheck your email and click the link to verify your email address.\nIn the far top-right of the page back on GitHub, click your randomly assigned avatar, and in the dropdown menu, click “Settings”.\nIn the “Emails” tab (left-hand menu), deselect the box “Keep my email addresses private”.\nIn the “Profile” tab, enter your name.\nStill in the “Profile tab”, upload an avatar. This can be a picture of yourself but if you prefer, you can use something else.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:03-04:00"
    },
    {
      "path": "w03_GA_git.html",
      "title": "Graded Assignment I: Shell, Markdown, and Git",
      "author": [],
      "contents": "\n\nContents\nIntroduction\nPart I – Create a new Git repo\nPart II – Add some Markdown content\nPart III – Add some “data” files\nPart IV – Renaming files on a new branch\nPart V – Create and sync an online version of the repo\n\n\nIntroduction\nIn this assignment, you will primarily be practicing your Git skills. You will also show that you can write in Markdown and can write a fo loop to rename files.\nThe instructions below will take you through it step-by-step. All of the the steps involve things that we have gone through in class and/or that you have practiced with in last week’s exercises.\nGrading information:\nThe number of points (if any) that you can earn for each step are denoted in cursive between square brackets (e.g. [0.5]). In total, you can earn 10 points with this assignment, which is 10% of your grade for the class.\nIf you make a mistake like an unnecessary commit, or something that requires you to subsequently make an extra commit: this is no problem. Points will not be subtracted for extra commits as long as you indicate what each commit is for & I’m still able to figure out which commits are the ones that I requested.\nPoints are not subtracted for minor things like typos in commit messages, either.\nBasically, you should not feel like you have to restart the assignment unless things have become a total mess…\nSome general hints:\nDon’t forget to constantly check the status of your repository (repo) with git status: before and after nearly all other Git commands that you issue. This will help prevent mistakes, and will also help you understand what’s going on as you get up and running with Git.\nIt’s also a good idea to regularly check the log with git log; recall that git log --oneline will provide a quick overview with one line per commit.\n\nPart I – Create a new Git repo\nCreate a new directory at OSC.\nA good place for this directory is in /fs/ess/PAS1855/users/$USER/week03/, but you are free to create it elsewhere (I will only be checking the online version of your repo).\nI suggest the name pracs-sp21-GA1 for the dir, (“GA1” for “Graded Assignment 1”), but you are free to pick a name that makes sense to you.\nLoad the OSC git module. Don’t forget to do this, or you will be working with a much older version of Git.\nInitialize a local Git repository inside your new directory. [0.5]\nCreate a README file in Markdown format. [0.5]\nThe file should be named README.md, and for now, just contain a header saying that this is a repository for your assignment.\nStage and then commit the README file. [0.5]\nInclude an appropriate commit message.\nPart II – Add some Markdown content\nCreate a second Markdown file with some more contents. [1.5]\nThis file can have any name you want, and you can also choose what you want to write about.\nUse of a good variety of Markdown syntax, as discussed and practiced in week 2: headers, lists, hyperlinks, and so on. (You’ll have to include some inline code and a code block later on, so you may or may not choose to use those now.) Also, make sure to read the next step before you finish writing.\nAs long as you are not writing minimalistic dummy text without any meaning (like “Item 1, Item 2”), you will not be graded for what you are writing about, so feel free to pick something you like – and don’t worry about the details.\n(If you’re not feeling inspired, here are some suggestions: lecture and reading notes for this week; a table of Unix and/or Git commands that we’ve covered; things that you so far find challenging or interesting about Git; how computational skills may help you with your research.)\nHints\nIn VS Code, open the Markdown preview on the side, so you can experiment and see whether your formatting is working the way you intend.\n\nCreate at least two commits while you work on the Markdown file. [0.5]\nTry to break your progress up into logical units that can be summarized with a descriptive commit message.\nHints\n\nBad commits/commit messages:\n“First part of the file” along with “Second part of the file”.\nGood commits and commit messages:\n“Summarized key concepts in the Git workflow” along with “Made a table of common Git commands”.\n\nUpdate the README.md file. [0.5]\nBriefly describe the current contents of your repo now that you actually have some contents.\nStage and commit the updated README file. [0.5]\nPart III – Add some “data” files\nCreate a directory with dummy data files. [0.5]\nCreate a directory called data and inside this directory, create 100 empty files with a single touch command and at least one brace expansion (e.g. for samples numbered 1-100, or 20 samples for 5 treatments). Give the files the extension .txt.\nStage and commit the data directory and its contents. [0.5]\nAs always, include a descriptive message with your commit.\nPart IV – Renaming files on a new branch\nAs it turns out, there was a miscommunication, and and all the “data” files will have to be renamed. You realize that this is risky business, and you don’t want to lose any data.\nIt occurs to you that one good way of going about this is creating a new Git branch and performing the renaming task on that branch. Then, if anything goes wrong, it will be easy to go back: you can just switch back to the master branch.\n(Note that there are many other ways of undoing and going back in Git, but this one is perhaps conceptually the easiest, and safe.)\nGo to a new branch. [0.5]\nCreate a new branch called rename-data-files or something similar, and move to the new branch. (Also, check whether this worked.)\nWrite a for loop to rename the files. [1.5]\nYou can keep it as simple as switching the file extension to .csv, or do something more elaborate if you want.\nBecause these files are being tracked by Git, recall that there is a Git-friendly modification to the command to rename files: use that in the loop.\n\nHints\n\nThese slides from last week have a similar renaming loop example.\nTo replace the last part of the filename, like the extension, recall that you can strip suffices using the basename command.\nBefore you go ahead and actually rename the files, use echo in your loop to print the (old and) new filenames: if it looks good, then add the renaming command to the loop.\n\nIf you just used mv and forgot to use git mv, don’t fret. Have a look at the status of the repo, then do git add --all and check the status again: Git figured out the rename.\nIf things go wrong, for instance you renamed incorrectly, or even deleted the files, you can start over in a few ways:\nYou can delete any files in data/, commit the deletion, and recreate the files. (The extra commits will not affect your grade, but use your commit messages to clarify what you’re doing.)\nA solution that would always work, including in case these files were actually valuable and you couldn’t just recreate them, is as follows:\nLike above, delete any files in data/ and commit the deletion.\nSwitch back to the master branch. Your data/ files should be back.\nCreate a new renaming branch and move there.\nOptionally, delete the old/failed renaming branch.\n\nCommit the changes made by the renaming operation.\nMerge into the master branch. [0.5]\nWith the files successfully renamed, go back to the master branch.\nThen, merge the rename-data-files branch into the master branch.\n(Optionally, remove the rename-data-files branch, since you will no longer need it.)\nUpdate the README file.\nIn the README file, describe what you did, including some inline code formatting, and put the code for the for loop in a code block.\n(No, the repo doesn’t really make sense as a cohesive unit anymore, but that’s okay while we’re practicing. :) )\nStage and commit the updated README file. [0.5]\nPart V – Create and sync an online version of the repo\nPhew, those were a lot of commits! Let’s share all of this work with the world.\nCreate a Github repository. [0.5]\nGo to https://github.com, sign in, and create a new repository.\nIt’s a good idea to give it the same name as your local repo, but these names don’t have to match.\nYou want to create an empty GitHub repository, because you will upload all the contents from your local repo: therefore, don’t check any of the boxes to create files like a README.\n\nPush your local repo online. [0.5]\nWith your repo created, follow the instructions that Github now gives you, under the heading “…or push an existing repository from the command line”.\nThese instructions will comprise three commands: git remote add to add the “remote” (online) connection, git branch to rename the default branch from master to main, and git push to actually “push” (upload) your local repo.\nWhen you’re done, click the Code button, and admire the nicely rendered README on the front page of your Github repo.\nHints\n\nAssuming that you’re using SSH authentication, which you should have set up in this week’s ungraded assignment, make sure you use the SSH link type: starting with git@github.com rather than HTTPS.\n\n\nCreate an issue to mark that you’re done! [0.5]\nFind the “Issues” tab for your repo on Github:\n\n\n\nIn the Issues tab, click the green button New Issue to open a new issue for you repo.\nGive it a title like “I finished my assignment”, and in the issue text, tag @jelmerp. You can say, for instance, “Hey @jelmerp, can you please take a look at my repo?”.\n\n",
      "last_modified": "2021-04-25T17:52:04-04:00"
    },
    {
      "path": "w03_readings.html",
      "title": "Week 3 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, you will be introduced to formal version control with Git, and will learn how to link up local repositories with remote counterparts on GitHub.\nBe aware that Git is a rather challenging topic. Therefore, if you can, complete the main reading before Tuesday’s lecture, and also read the Buffalo chapter at some point this week.\nA good way to get used to Git is to make dummy repositories where you’re just editing one or a few simple text files with dummy lines of text. That way, you can get used to the basic workflow, and freely experiment also with commands to undo things and move back in time. We’ll do this in our Zoom meetings and I recommend you do it outside of there, too.\nYour first graded assignment is due on Tuesday, February 2nd, and will be about course content from this week and the first two weeks.\nThere is also an ungraded assignment to set up Git and authentication for Github, which you should have completed by the first lecture of the week on Tuesday, January 26th.\nSome of the things you will learn this week:\nUnderstand why you should use a formal Version Control System (VCS) for research projects.\nLearn the basics of the most widely used VCS: Git.\nLearn how to put your local repositories online at GitHub, and how to keep local and online (“remote”) repositories in sync.\nLearn about single-user and multi-user workflows with Git and GitHub.\nLearn how to use Git branches to safely make experimental changes.\nLearn how to undo things and “travel back in time” for your project using Git.\nReadings\nThis week’s main reading is the CSB chapter on Git, chapter 2. We will also roughly work our way through this chapter in the Zoom sessions.\nThe optional reading is the Buffalo chapter on Git, chapter 5. Like the CSB chapter, this starts with the very basics of Git; but it goes a bit further. As mentioned, Git is a challenging subject, so I recommend that you read both of these chapters to really get the hang of it.\nThere are also some useful further resources mentioned below.\nRequired readings\nCSB Chapter 2: “Version Control”\nOptional readings\nBuffalo Chapter 5: “Git for Scientists”\nFurther resources\nGit-it is a small application to learn and practice Git and GitHub basics. Download and installation instructions are here.\nGitHub has a nice little overview of some Git and GitHub functionality including branching and Pull Requests, and how to do these things in your browser at GitHub.\nFor some more background on why to use version control, and another perspective on some Git basics, I recommend the article “Excuse me, do you have a moment to talk about version control?” by Jenny Bryan.\nEspecially if you work with R a lot, I would recommend checking out Happy Git and GitHub for the useR, also by Jenny Bryan. This is a very accessible introduction to Git.\nIf you want to try some online exercises with Git with helpful visuals of what Git commands do, try https://learngitbranching.js.org/. (But be aware that this does fairly quickly move to fairly advanced topics, including several that we will not touch on in the course.)\nA good list of even more Git resources…\n\n\n\n",
      "last_modified": "2021-04-25T17:52:04-04:00"
    },
    {
      "path": "w03_UA_git-setup.html",
      "title": "Week 3 -- Ungraded Assignment : <br> Git setup and GitHub authentication",
      "author": [],
      "contents": "\nPart 1: Git setup\nThese instructions are for setting up Git at OSC, but from Step 3 onwards, you can also follow them to set up Git for your local computer.\nOpen up a terminal at OSC.\nYou can do this after logging in at https://ondemand.osc.edu in one of two ways:\nDirect shell access: Clusters (top blue bar) > Pitzer Shell Access.\nIn VS Code: Interactive Apps > Code Server > then open a terminal using Ctrl+backtick > break out of the Singularity shell by typing bash.\n\nLoad the OSC Git module.\n(Note: Git is available in any OSC shell without loading any modules, but that is a rather old version – so we load a newer one.)\nmodule load git/2.18.0\nUse git config to make your name known to Git.\nSomewhat confusingly, mote that this should be your actual name, and not your GitHub username:\ngit config --global user.name 'John Doe'\nUse git config to make your email address known to Git.Make sure that you use the same email address you used to sign up for GitHub.\ngit config --global user.email 'doe.391@osu.edu'\nUse git config to set a default text editor for Git.\nOccasionally, when you need to provide Git with a “commit message” to Git and you haven’t entered one on the command line, Git will open up a text editor for you. Even though we’ll be mostly be working in VS Code during this course, in this case, it is better to select a text editor that can be run within the terminal, like nano (or vim, if you are familiar with it). To specify Nano as your default text editor for Git:\ngit config --global core.editor \"nano -w\"\n\n# You could set VS Code as your editor on your local computer,\n# if you use it there:\n# git config --global core.editor \"code --wait\"\nCheck whether you successfully changed the settings:\ngit config --global --list\n\n# user.name=John Doe\n# user.email=doe.39@osu.edu\n# colour.ui=true\n# core.editor=nano -w\nSet colors if needed.\nMake sure you see colour.ui=true in the list (like above), so Git output in the terminal will use colors. If you don’t see this line, set it using:\ngit config --global color.ui true\n\nPart 2: GitHub authentication\nGitHub authentication: Background\nTo be able to link local Git repositories to their online counterparts on GitHub, we need to set up GitHub authentication.\nRegular password access (over HTTP/HTTPS) is now “deprecated” by GitHub, and two better options are to set up SSH access with an SSH key, or HTTPS access with a Public Access Token.\nWe’ll use SSH, as it is easier – though still a bit of drag – and because learning this procedure will also be useful for when you’ll be setting up SSH access to the Ohio Supercomputer Center. (But note that GitHub now labels HTTPS access as the “preferred” method.)\nFor everything on GitHub, there are separate SSH and HTTPS URLs, and GitHub will always show you both types of URLs. When using SSH, we need to use URLs with the following format:\ngit@github.com:<USERNAME>/<REPOSITORY>.git\n(And when using HTTPS, you would use URLS like https://github.com/<USERNAME>/<REPOSITORY>.git)\nSetting up GitHub SSH authentication\nThese instructions are for setting up authentication at OSC, but you can repeat the same steps to set up authentication for your local computer.\nIn a terminal at OSC, use the ssh-keygen command to generate a public-private SSH key pair like so:\nssh-keygen -t rsa\nYou’ll be asked three questions, and for all three, you can accept the default by just pressing Enter:\n# Enter file in which to save the key (<default path>):\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again: \n\n\n\nNow, you have a file called id_rsa.pub in your ~/.ssh folder, which is your public key. To enable authentication, we will put this public key on GitHub – our public key interacts with our private key, which we do not share.\nPrint the public key to screen using cat:\ncat ~/.ssh/id_rsa.pub\nCopy the public key, i.e. the contents of the public key file, to your clipboard. Make sure you get all of it, including the “ssh-rsa” part (but beware that your new prompt may start on the same line as the end the key):\n\n\n\nIn your browser, go to https://github.com and log in.\nGo to your personal Settings. (Click on your avatar in the far top-right of the page, and select Settings in the drop-down menu.)\nClick on SSH and GPG keys in the sidebar.\nClick the green New SSH key button.\nGive the key an arbitrary, informative name, e.g. “OSC” to indicate that you are using this key at OSC.\nPaste the public key, which you copied to your clipboard earlier, into the box.\n\n\nClick the green Add SSH key button. Done!\n\n\n\n\n",
      "last_modified": "2021-04-25T17:52:04-04:00"
    },
    {
      "path": "w04_exercises.html",
      "title": "Exercises: Week 4",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nExercise 1: Pseudogenes with grep, uniq -c, and others\nExercise 2: Calculations with awk\nExercise 3: Replacements with sed\n\nBonus exercises\nExercise 4: FASTA\nExercise 5: Exons\nExercise 6: Miscellaneous\n\nSolutions\nExercise 1: Pseudogenes with grep, uniq -c, and others\nExercise 2: Calculations with awk\nExercise 3: Replacements with sed\nExercise 4: FASTA\nExercise 5: Exons\nExercise 6: Miscellaneous\n\n\nMain exercises\nThese exercises will use the same files that we’ve been working with in class: those in chapter-07-unix-data-tools in the bds-files repository for the Buffalo book.\nYou can either work directly in that directory, or create a “sandbox” directory with the Buffalo files copied.\nFor many of the exercises, there are multiple ways of solving them. The hints and solutions provide one or a few ways, but you may well end up using a different approach. If you struggle, try to follow the approach suggested by the hints, so you can later cross-check with solutions that use this same approach.\nFor most questions, the code required is quite similar to the examples in Buffalo chapter 7 (and the slides). If the hints are too cryptic for you, I recommend you first go back to the section on the relevant command in the chapter.\nExercise 1: Pseudogenes with grep, uniq -c, and others\nIn this exercise, we will start using the file Mus_musculus.GRCm38.75_chr1.gtf that we’ve seen in class.\nLet’s say that you need to extract annotation information for all pseudogenes on mouse chromosome 1 from the GTF, and write the results to a new GTF file. We’ll do that in this exercise with a detour along the way.\nCount the number of lines with the string “pseudogene”.\nHints\nUse grep -c to count the matches.\n\n\nDoes this mean there are over a 1,000 pseudogenes on this chromosome? No, because there are “nested” genomic features in the file: for each gene, several “subfeatures” are listed, such as individual exons. This genomic feature type information is listed in column 3.\nFor lines that match “pseudogene” as above, which different feature types are listed in column 3, and what are their counts (i.e., create a count table for column 3)?\nAre there any coding sequences (“CDS”), start codons (“start_codon”) and stop codons (“stop_codon”)?\nHints\n\nTo answer this question, compute count tables using sort | uniq -c after grepping for “pseudogene”.\n\n\nIf you are more restrictive in your grep matching to only match “pseudogene” as a full “word”, do you still see CDS or start/stop codons?\nHints\nUse grep -w to only match full “words”: consecutive alphanumeric characters and the underscore.\nThis would avoid matching longer words that contain “pseudogene”, such as “transcribed_pseudogene”.\n\n\nWe have “exon”, “transcript” and “gene” features in column 3, but we want just the gene-level coordinates of the pseudogenes: after grepping for “pseudogene” like in the previous question, select only rows for which the feature type in column 3 is gene.\nPipe your output into head or less; we don’t want to write to a new file yet.\nHints\n\nThe “gene” you want to match is in its own column. Matching restrictively using the -w flag to only match whole “words” will work in this case.\nEven better would be to explicitly match the tabs surrounding “gene”. This can be done using the -P flag to grep (grep -P) and the \\t symbol for tabs.\nYet another approach that is even more explicit uses awk to make the match specifically for the 3rd column (recall that $3 is the third column in awk, so you can use $3 == \"gene\"). If you try with awk, you’ll have to make sure to ignore the header lines, which are not tabular.\n\n\nComprehensively check that your output only contains “gene” in column 3.\nHints\n\nPipe your previous command into cut and then sort | uniq -c to quickly check which values are present in column 3.\n\n\nNow, you are almost ready to write the results to a new GTF file, called pseudogenes.gtf, with the features you selected above: gene-level pseudogene features.\nOne challenge is that you also want the header, which you won’t get when redirecting the output from the previous command.\nA straightforward solution would be to first write the header to a new file, and then append the lines with the selected features.\nAlternatively, you can use subshells to do this in a single go. If you want to try this, make sure you have read the Buffalo section on subshells (p. 169-170) first.\nHints\n\nStructure the subshell solution as follows:(<select-header> ; <select-from-rest-of-file>) > pseudogenes.gtf\n\n\nExercise 2: Calculations with awk\nIn this exercise, you will continue working with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nHow many elements are at least 1,000 base pairs long?\nHints\nRemove the header from consideration using grep -v.\nFilter by feature length using awk, and get the length by subtracting column 4 from column 5.\n\nHow many genes (“gene” in column 3) on the + strand (“+” in column 7) are at least 1,000 base pairs long?\nHints\nAdjust your command for the previous question as follows:\nMatch columns 3 (feature type) and 7 (strand) with awk, using == to only select matching rows.\nUse the && operator in awk to combine conditions.\n\nWhat’s the combined size of all genes?\n(On this chromosome, that is: this GTF only contains a single chromosome.)\nHints\nFirst, remove the header from consideration using grep -v.\nNext, select “gene”-level features only, which you can do with awk as above, with grep -w, or with grep -P with \\t for tab.\nNext, use a variable in awk:\nInitialize the variable in a BEGIN statement;\nAdd the length of the gene (column 5 - column 4) for each row;\nPrint the result with an END statement.\n\n\nWhat is the mean UTR length?\nHints\nThe code is very similar to that for the previous question; just grep for “UTR” instead, and calculate the mean by dividing the final value of awk variable by the number of rows (NR) in the END statement.\n\n\nExercise 3: Replacements with sed\nThe genotypes.txt contains diploid genotypes for 4 loci for 9 individuals.\nRemove all underscores from genotypes.txt and write the output to a new file.\nHints\nUse the sed 's/<pattern>/<replacement>/<modifiers>' <file> construct.\nYou’ll need the modifier to do global (i.e., >1 per line) replacements.\n\n\nReplace the / delimiter for genotypes (G/G) in genotypes.txt by a colon (G:G) and write the output to a new file.\nHints\nIn the sed 's/<pattern>/<replacement>/<modifiers>' <file> construct, forward slashes / will not work here, because the pattern we are looking for is a forward slash. Use a different symbol instead, e.g. a #:sed 's#<pattern>#<replacement>#<modifiers>' <file>\n\n\nBonus exercises\nExercise 4: FASTA\nFirst, convert the FASTQ file contaminated.fastq to a FASTA file using the following awk command, and try to understand what it does, by and large:\nawk '{ if(NR%4 == 1) { printf(\">%s\\n\", substr($0,2)); } else if(NR%4 == 2) print; }' \\\n    contaminated.fastq > contaminated.fasta\nSelect FASTA entries (sequence and header) from contaminated.fasta with the sequence AGATCGGAAGA and write them to a new file, contaminated_sel.fasta. (Your new file should have 6 sequences, so 12 lines.)\nHints\n\nMatch the sequence with grep and use the -B option to also retrieve the preceding line, which is the sequence header/identifier. In addition, use --no-group-separator to prevent grep from printing -- between matches (alternatively, you could use another call to grep with -v to filter out -- lines).\n\n\nCheck how many sequences, if any, have one ore more undetermined bases (N).\nHints\n\nMake sure you first exclude the header lines, since you only want to match N in the sequences.\n\n\nApparently, there may have been a problem with cycle 50 in the Illumina run.\nGet the frequency of each base for position 50 in the FASTA file.\nHints\n\nUse cut -c <n> to extract the 50th position.\n\n\n\nExercise 5: Exons\nIn this exercise, you will once again work with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nNow, let’s explore some statistics on exons in the GTF file.\nCreate a tab-separated, two-column file called exons.txt that lists each unique “exon_id” along with its “gene_id”, sorted by gene ID. (The “exon_id” and “gene_id” key-value pairs are in the huge final column.)\nThe first couple of lines of the file should look as follows:\n  ENSMUSG00000000544      ENSMUSE00000160546\n  ENSMUSG00000000544      ENSMUSE00000160549\n  ENSMUSG00000000544      ENSMUSE00000226264\n  ENSMUSG00000000544      ENSMUSE00000395516\nHints\n\nFirst grep for lines that contain “gene_id” followed by “exon_id”.\nUse a sed command with two backreferences (capture the backreferences with parentheses, (<match>), and recall them with \\1 and \\2) to extract just the gene ID and the exon ID.\nFinally, only keep unique lines and sort.\n\nUse the exons.txt file you created to answer the following questions.\nOn average, how many exons does each gene have?\nHints\n\nThe number of genes is simply the number of lines in exons.txt.\nCount the number of exons using uniq and then wc -l.\nThe division can be done with expr $n_genes / $n_exons although you will not get any decimals (arithmetic in bash is very limited!). The above exmplae assumes you assigned the results to variables using command substitution, e.g.: ngenes=$(wc -l ...).\n(You could also, e.g., call Python inline, but we’re not there yet.)\n\nWhat is the highest number of exons for one gene?\nHints\n\nUse uniq -c followed by a reverse numeric sort.\n\n\nCreate a count table for the number of exons per gene. It should tell you how many genes have one exon, how many have two exons, etc.\nHints\n\nYou’ll need to process the uniq -c output of exon counts to get rid of the leading spaces. You can do this with sed; use ^ * as the search pattern, or ^ + with sed -E (the + symbol to match one ore more of the preceding character is in the extended regex set).\nThen, you’ll need to create a count table from these counts, so another round of count | uniq -c.\n\nExercise 6: Miscellaneous\nIn this exercise, you will once again work with the GTF file Mus_musculus.GRCm38.75_chr1.gtf.\nCount the total number of features on each strand (+ or -, column 7).\nCount the number of features from each of the three values for “gene_source” (“ensembl”, “havana”, and “ensembl_havana”).\nHints\n\nYou can either use sed with a backreference to capture the “gene_source” type, or grep -o to capture a larger match followed by cut and sed to extract just the “gene_source” type.\n\n\nHow many genes have a UTR (untranslated region)? Create a list of “gene_id”s for genes with a UTR.\nHints\n\nFirst grep for UTR, then for the “gene_id” while also extracting the match with grep -o. Once again, the extraction of a match can alternatively be done with a backreference in sed.\n\n\nSolutions\nExercise 1: Pseudogenes with grep, uniq -c, and others\nTo start with, I will assign the name of the GTF file to the variable $gtf, for quick reference in the solutions for the and the next exercises:\ngtf=Mus_musculus.GRCm38.75_chr1.gtf\nCount the number of lines with the string “pseudogene”.\n\n\n$ grep -c \"pseudogene\" $gtf\n\n#> 1041\nThe -c option to grep will count the number of matching lines.\n\nCount feature types.\n\n\n$ grep -v \"^#\" $gtf | grep \"pseudogene\" | cut -f 3 | sort | uniq -c\n\n#>     15 CDS\n#>    524 exon\n#>    223 gene\n#>      4 start_codon\n#>      4 stop_codon\n#>    264 transcript\n#>      7 UTR\nThe first line computes a count table for pseudogenes, and the second line computes an equivalent count table for all genes.\nWe use grep -v \"^#\" to exclude the header (the command would work without this line, but it’s better not to assume the header won’t match, and to explicitly remove it first). Note that the caret ^ is a regex symbol for the beginning of the line. We use the -v option to grep to invert the match.\nWe use cut -f 3 to select the third column.\nWe use the sort | uniq -c combination to create a count table.\nCount feature types after more restrictive matching.\n\n\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | cut -f 3 | sort | uniq -c\n\n#>    499 exon\n#>    221 gene\n#>    258 transcript\nThis is very similar to the solution for the previous exercise, but now we use the -w option to grep to only select full words (consecutive alphanumeric characters and the underscore).\n\nSelect “pseudogene” matches for which the feature type in column 3 is “gene”.\n\n\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | grep -w \"gene\" | head\n\n# Alternative 1: More explicit match by matching tabs:\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | grep -P \"\\tgene\\t\" | head\n\n# Alternative 2: More explicit match by matching column 3 only:\n$ grep -v \"^#\" $gtf | grep -w \"pseudogene\" | awk '$3 == \"gene\"' | head\nTo match tabs like in the second solution, we need to use greps -P option, for \"Perl-like regular expressions. (Yes, it confusing and unfortunate that there are three types of regular expressions in grep!)\nIn the third solution we explicitly and exactly match column 3 with awk – this is the most robust solution.\n\nCheck that your output only contains “gene” in column 3.\n\n\n$ grep -v \"^#\" $gtf | grep \"pseudogene\" | grep -w \"gene\" | \\\n      cut -f 3 | sort | uniq -c\n\n#> 223 gene\nWe select the third column and use the sort | uniq -c combination to create a count table, which will quickly tell us whether there are multiple values in the column.\n\nWrite the pseudogene selection to a new GTF file.\n\n\nSolution when writing the header and the rest of the file separately (don’t forget to use >> for appending the output of the second line, if you use this method!):\n$ grep \"^#\" $gtf > pseudogenes.gtf\n$ grep -v \"^#\" $gtf | \\\n      awk '$2 == \"pseudogene\" && $3 == \"gene\"' \\\n      >> pseudogenes.gtf\nSolution with subshells:\n$ (grep \"^#\" $gtf; grep -v \"^#\" $gtf | \\\n      awk '$2 == \"pseudogene\" && $3 == \"gene\"') \\\n      > pseudogenes.gtf\nNote: rather than with grep -v \"^#\", we can also exclude the header using !/^#/ in awk:\n$ (grep \"^#\" $gtf; \\\n      awk '!/^#/ && $2 == \"pseudogene\" && $3 == \"gene\"' $gtf) \\\n      > pseudogenes.gtf\n\nExercise 2: Calculations with awk\nHow many features are at least 1,000 bp long?\n\n\n$ grep -v \"^#\" $gtf | \\\n      awk '$5 - $4 > 999' | \\\n      wc -l\n\n#> 8773\nFirst, we again exclude the header like in exercise 1 (this can also be done using awk: see below).\nNext we subtract column 4 ($4) from column 5 ($5) and use as a condition that this difference should be larger than 999.\n# Alternative that excludes the header in awk:\n$ awk '!/^#/ && $5 - $4 > 999' $gtf | \\\n      wc -l\n\n#> 8773\n\nHow many genes on the + strand are at least 1,000 bp long?\n\n\n$ grep -v \"^#\" $gtf | \\\n      awk '$3 == \"gene\" && $7 == \"+\" && $5 - $4 > 999' | \\\n      wc -l\n\n#> 725\nWe chain together different conditions using &&.\nThe second line uses awk to match + in column 7, and to select lines where the difference between column 5 and 4 (i.e., the feature length) is larger than 999 bp.\nThe third line simply counts the number of lines we have left.\nWhat’s the combined size of all genes?\n\n\n$ grep -v \"^#\" $gtf | \\\n      grep -w \"gene\" | \\\n      awk 'BEGIN { s=0 }; { s += $5 - $4 }; END { print s }'\n  \n#> 78935729    # 78,935,729 => ~79 Mbp\nThe first two lines are the same as for the previous question (see the solution there for further details).\nIn the third line, we use a variable in awk to compute a sum of the feature lengths (i.e., column 5 minus column 4). With the BEGIN statement, we initialize the variable s before we start processing lines. Then, for each line, we add the feature length to the value using the += operator (shorthand for s = s +). Finally, with the END statement, we report the final value of the variable s after we have processed all lines.\nWhat is the mean UTR length?\n\n\n$ grep -v \"^#\" $gtf | \\\n      grep -w \"UTR\" | \\\n      awk 'BEGIN { s=0 }; { s += $5 - $4 }; END { print s/NR }'\n      \n#> 472.665      \nThis code is very similar to that for the previous question (see that solution for more details), but now we divide the sum (variable s) by NR, an automatic variable in awk that holds the current record (line) number. Since the END statement is executed after all lines have been processed, the record number will be the total number of lines in the file.\n\nExercise 3: Replacements with sed\nIn genotypes.txt, remove all underscores.\n\n\n$ sed 's/_//g' genotypes.txt > genotypes2.txt\n\n$ cat genotypes2.txt\n#> id      indA    indB    indC    indD\n#> S000    G/G     A/G     A/A     A/G\n#> S001    A/T     A/T     T/T     T/T\n#> S002    C/T     T/T     C/C     C/T\n#> S003    C/T     C/T     C/T     C/C\n#> S004    C/G     G/G     C/C     C/G\n#> S005    A/T     A/T     A/T     T/T\n#> S006    C/G     C/C     C/G     C/G\n#> S007    A/G     G/G     A/A     G/G\n#> S008    G/T     G/T     T/T     G/T\n#> S009    C/C     C/C     A/A     A/C\nSince there are multiple underscores on line 1, you need the g modifier.\n\nIn genotypes.txt, replace the / delimiter by a colon (:).\n\n\n$ sed 's#/#:#g' genotypes.txt > genotypes3.txt\n\n$ cat genotypes3.txt\n#> id      ind_A   ind_B   ind_C   ind_D\n#> S_000   G:G     A:G     A:A     A:G\n#> S_001   A:T     A:T     T:T     T:T\n#> S_002   C:T     T:T     C:C     C:T\n#> S_003   C:T     C:T     C:T     C:C\n#> S_004   C:G     G:G     C:C     C:G\n#> S_005   A:T     A:T     A:T     T:T\n#> S_006   C:G     C:C     C:G     C:G\n#> S_007   A:G     G:G     A:A     G:G\n#> S_008   G:T     G:T     T:T     G:T\n#> S_009   C:C     C:C     A:A     A:C\nBecause the pattern we want to match is a /, we use a different symbol in the sed syntax: here, I chose a #.\n\nExercise 4: FASTA\nWrite sequences with AGATCGGAAGA into a new file.\n\n\n$ grep -B 1 --no-group-separator \"AGATCGGAAGA\" contaminated.fasta \\\n      > contaminated_sel.fasta\n\n$ wc -l < contaminated_sel.fasta\n\n#> 12\n\n# Alternative to `--no-group-separator`;\n# remove the lines with `--` afterwards:\n$ grep -B 1 \"AGATCGGAAGA\" contaminated.fasta | \\\n      grep -v \"--\" > contaminated_sel.fasta\nUse B 1 to get the sequence ID line for each match.\nUse --no-group-separator (top solution) or pipe into grep -v \"--\" to avoid having a group separator, which grep inserts for multiline output like this, in the output.\nCheck how many sequences have one ore more undetermined bases (N).\n\n\nMake sure you first exclude the header lines, since you only want to match “N” in the sequences themselves:\n$ grep -v \"^>\" contaminated.fasta | grep -c \"N\"\n\n#> 0\nThere are no Ns in the sequence.\n\nGet the frequency of each base for position 50 in the fasta file.\n\n\n$ grep -v \"^>\" contaminated.fasta | cut -c 50 | sort | uniq -c \n\n#> 2 C\n#> 3 G\n#> 3 T\n\nExercise 5: Exons\nCreate a tab-delimited file with “exon_id” and “gene_id”\n\n\n$ grep \"gene_id.*exon_id\" $gtf | \\\n      sed -n -E 's/.*gene_id \"(\\w+)\".*exon_id \"(\\w+).*/\\1\\t\\2/p' | \\\n      sort | uniq > exons.txt\nIn the first line of code, we select only lines that contain both “gene_id” and “exon_id”.\nIn the second line, we capture the values for “gene_id” and “exon_id” using backreferences in sed.\nFinally, we sort (since we want to sort by the first column, just sort will work) and then take only unique rows using uniq, and redirect to a new file.\nOn average, how many exons does each gene have?\n\n\n$ n_genes=$(wc -l < exons.txt)\n$ n_exons=$(cut -f 1 exons.txt | uniq | wc -l)\n\n# Divide using `expr`:\n$ expr $(wc -l < exons.txt) / $(cut -f 1 exons.txt | uniq | wc -l)\n\n#> 11\n\n# Alternatively, do the division with Python:\n$ python -c \"print($n_genes / $n_exons)\"\n\n#> 11.627\nFirst, we count the number of genes, then the number of exons, and we assign each value to a variable.\nThen, we divide using expr (or Python to get decimals).\nWhat is the highest number of exons for one gene?\n\n\n$ cut -f 1 exons.txt | uniq -c | sort -rn | head\n\n#>    134 ENSMUSG00000026131\n#>    116 ENSMUSG00000066842\n#>    112 ENSMUSG00000026207\n#>    108 ENSMUSG00000026609\n#>    102 ENSMUSG00000006005\n#>     97 ENSMUSG00000037470\n#>     93 ENSMUSG00000026141\n#>     92 ENSMUSG00000026490\n#>     91 ENSMUSG00000048000\n#>     82 ENSMUSG00000073664\n“ENSMUSG00000026131” has as many as 134 exons!!\n\nCreate a count table for the number of exons per gene.\n\n\n$ cut -f 1 exons.txt | uniq -c | \\\n      sed 's/^ *//' | cut -d \" \" -f 1 | \\\n      sort -rn | uniq -c | \\\n      sort -rn > exon_count_table.txt\n\n$ head exon_count_table.txt     \n#>    646 1   # 646 genes with 1 exon\n#>    186 2   # 186 genes with 2 exons\n#>     94 3   # etc...\n#>     71 5\n#>     71 4\n#>     53 8\n#>     51 7\n#>     51 6\n#>     44 11\n#>     41 10\nIn the first line, we get the number of exons for each gene.\nIn the second line, we clean up the uniq -c output.\nIn the third line, we create the count table of the number of exons.\nFinally we sort this and redirect the output to a file.\nExercise 6: Miscellaneous\nCount the number of elements on each strand.\n\n\n$ grep -v \"^#\" $gtf | cut -f 7 | sort | uniq -c\n\n#> 40574 +\n#> 40652 -\n\nCount the number of elements for each of the three values for “gene_source”.\n\n\n# Capture the gene_source with sed:\n$ sed -n -E 's/.*gene_source \"(\\w+)\".*/\\1/p' $gtf | \\\n      sort | uniq -c\n\n#> 18209 ensembl\n#> 61089 ensembl_havana\n#> 1928 havana\n\n# Alternatively, capture the gene_source with `grep -o`:\n$ grep -E -o 'gene_source \"\\w+\"' $gtf | \\\n      cut -f2 -d\" \" | sed 's/\"//g' | \\\n      sort | uniq -c\n      \n#> 18209 ensembl\n#> 61089 ensembl_havana\n#> 1928 havana\n\nHow many genes have a UTR? Create a list of gene_ids for genes with a UTR.\n\n\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n   grep -E -o 'gene_id \"\\w+\"' | sort | uniq | wc -l\n\n#> 1179\n\n# Create the list of genes:\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n      grep -E -o 'gene_id \"\\w+\"' | \\\n      cut -f2 -d\" \" | sed 's/\"//g' | \\\n      sort | uniq > genes_with_UTR.txt\n\n# Alternative with sed:\n$ grep -v \"^#\" $gtf | grep -w \"UTR\" | \\\n      sed -n -E 's/.*gene_id \"(\\w+)\".*/\\1/p' | \\\n      sort | uniq | wc -l > genes_with_UTR.txt\n\n\n\n\n",
      "last_modified": "2021-04-25T17:52:05-04:00"
    },
    {
      "path": "w04_readings.html",
      "title": "Week 4 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, we will continue learning about working in the Unix shell, using Buffalo chapter 7, with a focus on data tools. New tools will include the less pager and the advanced commands sed and awk (and bioawk).\nWe’ll also revisit and expand our knowledge of several tools we have already seen in week 1, such as grep, sort, and the use of Unix “pipelines”. The examples will focus on working with sequence data in several formats.\nSome of the things you will learn this week:\nHow clever usage of Unix pipelines can be a large time-saver when needing to extract, convert and summarize data.\nUsing less to view files and quickly find and view text matches.\nUsing grep to find, count, and extract occurrences of strings and patterns.\nUsing sort together with cut and uniq / uniq -c to summarize columns of tabular data.\nUsing awk for many tasks on tabular data, including filtering rows and adding columns with arithmetic, and calculating means.\nUse sed to substitute text and reformat data.\nOptionally, understand subshells and process substitution.\nOptionally, use join to merge files by shared columns.\nReadings\nThis week, you will read chapter 7 of Buffalo. It’s a pretty dense chapter with lots of examples. The examples all involve sequence data but if you don’t work with sequence data, it should be fairly easy to see how these tools could be applied any kind of text and tabular data.\nRequired readings\nBuffalo Chapter 7: “Unix Data Tools”\nThe following sections can be skimmed or skipped, depending on your bandwidth and interest in these topics:\n“Decoding Plain-Text Data: hexdump”\n“Bioawk: an Awk for Biological Formats” (skim or skip if you’re not working with sequence data)\n“Advanced Shell Tricks”\n\n\n\n\n",
      "last_modified": "2021-04-25T17:52:06-04:00"
    },
    {
      "path": "w05_exercises.html",
      "title": "Week 5 exercises",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nBackground\nExercise 1: Setting up\nExercise 2: Create a script to compute stats for a FASTQ file\nExercise 3: Modify the looping script\nExercise 4: Bells and whistles\n\nBonus exercises\nExercise 5: Find the longest file\nExercise 6: Plant-pollinator networks\nExercise 7: Data explorer\n\nSolutions\nExercise 1\nExercise 2\nExercise 3\nExercise 4\nExercise 5\nExercise 6\nExercise 7\n\n\nThe main exercises will work with some FASTQ files. If you don’t care much for DNA sequence files, and perhaps start to get lost in the technicalities, make sure to carry on to the three bonus exercises.\nMain exercises\nBackground\nThese exercises will work with 6 FASTQ files with sequences from the V4 region of 16S rRNA, generated in a metabarcoding experiment.\nThe FASTQ files come in pairs: for every sample, there is a FASTQ file with forward reads (or “read 1” reads) that contains _R1_ in its filename, and a FASTQ file with corresponding reverse reads (or “read 2” reads) that contain _R2_ in its filename. So, our 6 FASTQ files consist of 3 pairs of files with forward and reverse reads for 3 different biological samples.\nThe sequences were generated by first amplifying environmental samples with a pair of universal 16S primers, and these primer sequences are expected to be present in the FASTQ sequences. You will search for these primer sequences below, and there are two things to be aware of:\nA primer can also be present in the FASTQ sequence as its reverse complement, so we will search for reverse complements too.\nThe primers contain a few variable sites, which are indicated using ambiguity codes. For instance, an R means that the site can be either an A or a G, and an N means that the site can be any of the four bases. See here for a complete overview of these ambiguity codes.\nHere are the primer sequences and their reverse complements1:\nForward primer (“515F”): GAGTGYCAGCMGCCGCGGTAA / TTACCGCGGCKGCTGRCACTC.\nReverse primer (“806R”): ACGGACTACNVGGGTWTCTAAT / ATTAGAWACCCBNGTAGTCCGT.\nExercise 1: Setting up\nCreate a directory for this week’s exercises and move into it.\nIn these exercises, you will be working with and modifying one of the scripts in Buffalo’s Chapter 12, which is printed below. Save this script as fastq_stat_loop.sh.\n#!/bin/bash\nset -e -u -o pipefail\n\n# Specify the input samples file (3rd column = path to FASTQ file):\nsample_info=samples.txt\n\n# Create a Bash array from the third column of \"$sample_info\":\nsample_files=($(cut -f 3 \"$sample_info\"))\n\n# Loop through the array:\nfor fastq_file in ${sample_files[@]}; do\n\n  # Strip .fastq from each FASTQ file, and add suffix:\n  results_file=\"$(basename $fastq_file .fastq)-stats.txt\"\n\n  # Run \"fastq_stat\" on a file:\n  fastq_stat \"$fastq_file\" > stats/$results_file\n\ndone\nThe FASTQ files you’ll work with are inside the directory /fs/ess/PAS1855/data/week05/fastq. Copy these files into an appropriate directory inside your own directory for these exercises, like data/.\n\nExercise 2: Create a script to compute stats for a FASTQ file\nUnfortunately, the fastq_stat program referenced in Buffalo’s script is an imaginary program… So, let’s create a script fastqc_stat.sh that actually produces a few descriptive statistics for a a FASTQC file.\nSet up a script skeleton with the header lines we’ve been discussing: a shebang line and the various set settings for robust scripts.\nThe script should process one command-line argument, the input file. Assign the automatic placeholder variable for the first argument to a variable with a descriptive name, like fastq_file.\n\nHints\n\nThe automatic placeholder variable for the first argument is $1.\n\n\nIn the sections below, you can print all your output to standard out, i.e. simply use echo with no redirection.\nAlso, just for the purpose of testing the commands while developing them below, it will be convenient to assign one of the FASTQ files to a variable fastq_file.\n\nHave the script report its own name as well as the name of the FASTQ file that is being processed.\n\nHints\n\nRecall that the name of the script is automatically stored in $0.\n\nCompute and report the number of sequences in the FASTQ file.\n\nHints\n\nThe number of sequences can be assumed to be the total number of lines divided by 4 (or alternatively, the number of lines consisting only of a +). Recall that FASTQ files have 4 lines per sequence: a header, the sequence, a + divider, and the quality scores.\nTo make the division, you can use syntax like expr 12 / 4 – and you can use command substitution, $(wc -l ...), to insert the number of lines in the division.\n\nAs mentioned in the Background section, the primer sequences should be present in the FASTQ files. Prior to sequence analyses, these are usually removed with a specialized program like cutadapt, but we can use our grep skills to quickly search for them.\nWe will search for the forward primers only in the forwards reads, and for the reverse primers only in the reverse reads. Therefore, start with an if statement that tests whether the file name contains forward (_R1_) or reverse (_R2_) reads.If it contains forward reads, you should be counting occurrences of the forward primer or its reverse complement. Similarly, if it contains reverse reads you should be counting occurrences of the reverse primer or its reverse complement.\nYou’ll also have to replace the ambiguity codes, like R, with character classes that enumerate the possible alternative bases.\n\nHints\n\nThe test in the if statement should be a grep command that examines whether the filename contains _R1_ (pipe echo output into grep). The grep standard output should be redirected to /dev/null, since we’re only interested in the exit status: if grep finds a match, the commands following then will be executed; if it doesn’t, the commands following else will be executed.\nYou can assume that matches will only be made in the sequences themselves (and not the headers or quality scores), so you can grep the file as is: you don’t need to first select the lines with sequences.\nTo replace ambiguity codes with character classes in the grep regular expression: an R in the primers becomes [AG] in the grep expression, e.g. the partial sequence ATRG would become AT[AG]G in your regular expression.\nYou’ll need both the primer and its reverse complement in a single grep regular expression: separate them with an logical “or” (|) and to enable this, use grep -E for extended regular expressions.\n\nBonus: Print a count table of sequence lengths.\n\nHints\n\nYou’ll have to select only the lines with the actual DNA sequences, and the best way of doing that is using awk like so (see the Solution for an explanation of why this works):\nawk '{ if(NR%4 == 2) }'\nNext, you need to count the number of characters in each line, which is best done in the same awk command using print length($0), which will print the number of characters in the line.\nAfter that, it’s the familiar sort | uniq -c idiom to create a count table.\n\nMake the script executable.\n\nHints\n\nUse the chmod command.\n\nRun the script for a single FASTQ file by calling it from the command line.\n\nExercise 3: Modify the looping script\nNow, let’s modify Buffalo’s script fastq_stat_loop.sh.\nCurrently, the metadata file that contains the list of files to process is hard-coded as samples.txt. Also, the file names have to be extracted from a specific column from the metadata file. This is not a very flexible setup, and is sensitive to minor changes in a file like samples.txt.\nChange the script to let it accept a list of FASTQ file names as an argument.\n\nHints\n\nRecall that the placeholder variable for the first argument that is passed to a script on the command line is $1.\nInside the command substitution ($()) that populates the array, you can simply use cat instead of cut on the file that contains the list of file names, since this file will no longer have multiple columns.\n\nCurrently, the output directory is also hard-coded, as stats – let’s instead add the output directory as a second argument to the script.\nMoreover, add code that creates this output directory if it doesn’t already exist.\n\nHints\n\nYou can write an explicit test to see if the output dir exists first, but simply using mkdir -p will also work: with the -p option, mkdir doesn’t complain when a dir already exists (and can also make multiple levels of directories at once).\n\nChange the line that runs the imaginary program fastq_stat to let it run your fastq_stat.sh script instead. Make sure the path to your script and the path to the output file is correct.\nIn each iteration of the loop, let the script report which FASTQ file will be analyzed.\nNow that the script takes arguments, we need another file or script to create the list of filenames and to submit the script with the appropriate arguments. Specifically, in this file, we need:\nA line that creates a new file just containing the FASTQ file names (akin to column 3 from samples.txt – but with the paths to our actual FASTQ files).\nA line that runs the fastq_stat_loop.sh script. The file that contains the list of FASTQ files should be passed to the script as the first argument, and the output directory as the second argument.\nAs for the actual path to the output dir, you can use whatever (relative!) path makes sense to you.\n\nCreate the file containing the code outlined above. You can save this file either as a .sh script (e.g. fastqc_runner.sh), or put these lines in a Markdown file inside a code block. Either option is reasonable because these lines would likely be run interactively, as opposed to the fastq_stat_loop.sh script which will be run non-interactively. It’s still important to save these interactive commands in a file, so you know what you did and can easily reproduce it.\nMake fastq_stat_loop.sh executable and run it.\nThe loop script should run fastq_stat.sh on all your FASTQ files. Check the output, which should be in one file per FASTQ file in the output dir you designated. You should be seeing that the vast majority of reads contain the primer sequences. Be proud – with a quick script that only runs for a couple of seconds, and no specialized software, you have queried hundreds of thousands of sequences!\n\nExercise 4: Bells and whistles\nIn this exercise, you will touch up your fastqc_stat.sh script to include tests for robustness, and to report what the script is doing.\nFor the tests, check whether they work (…)! For instance, to check the test for the number of arguments, try running the script with no arguments, and also with two arguments, and see if the script produces the error messages you wrote.\nWrite an if statement to check whether the FASTQ file exists / is a regular file and whether it can be read. If not, give an error and exit the script with exit code 1.\n\nHints\n\nGo back to this week’s slides for an example of testing whether a file is a regular file (-f) and whether it can be read (-r).\nTo exit with exit code 1, simply use: exit 1. This should be done after printing any error messages you, or those won’t actually be printed.\n\nTest whether exactly 1 argument was passed to the script on the command line. If not, return an error, succinctly report how to use the script (“usage: …”), and exit with exit code 1.\n\nHints\n\nGo back to this week’s slides for an example of a very similar test.\nThe number of arguments that were passed to a script are automatically available in $#.\nYou can test for equality using <integer> -eq <integer> (e.g. 10 -eq 10 which will evaluate to true) and you can negate a test (\"number of argument is not equal to 1) using a ! before the comparison expression.\n\nAdd date commands at the start and the end of the script, so you’ll be able to tell how long it took the script to complete.\n\nBonus exercises\nExercise 5: Find the longest file2\nWrite a shell script called longest.sh that takes two arguments: the name of a directory and a file extension (like txt). The script should print the name of the file that has the most lines among all files with with that extension in that directory.\nMake sure the script has the shebang and set headers, and make the script executable.\nThen, run your script to learn which FASTQ file has the most lines (and sequences):\n$ ./longest.sh data/fastq fastq\n… should print the name of the .fastq file in data/fastq with the highest number of lines and therefore sequences.\n\nHints\n\nYou can count lines for many files at once using wc -l: simply provide it with a globbing pattern.\n\nExercise 6: Plant-pollinator networks\nThis exercise is slightly modified after 1.10.3 from the CSB book. The Saavedra2013 directory can be found inside the CSB repository at CSB/unix/data/Saavedra2013, and the following code assumes you are in the directory CSB/unix/sandbox. (If you no longer have the repository, download it again using git clone https://github.com/CSB-book/CSB.git.)\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files as an argument, and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces. Don’t forget to make your script executable and to add the standard header lines. Your script should return:\n$ ./netsize.sh ../data/Saavedra2013/n1.txt\n\n#> Filename: ../data/Saavedra2013/n1.txt\n#> Number of rows: 97\n#> Number of columns: 80\nWrite a script that prints the numbers of rows and columns for each network, taking the directory containing all the files as an argument:\n$ ./netsize_all.sh ../data/Saavedra2013\n\n#> ../data/Saavedra2013/n10.txt     14      20\n#> ../data/Saavedra2013/n11.txt     270     91\n#> ../data/Saavedra2013/n12.txt     7       72\n#> ../data/Saavedra2013/n13.txt     61      17\n#> …\n\nHints\n\nTo find the number of columns, use awk and recall awk’s NF (number of fields => number of columns) keyword.\nTo combine them in a script, use command substitution to assign the result of a command to a variable. (For example: mytxtfiles=$(ls *.txt) stores the list of .txt files in the variable $mytxtfiles.)\nNext, you need to write a for loop.\nYou can now use the script you’ve just written in combination with sort to answer the questions (remember the option -k to choose a column and -r to reverse the sorting order).\nYou can use echo -e to print tabs using \\t: echo -e \"column1 \\t column2\".\n\nExercise 7: Data explorer\nThis is slightly modified after exercise 1.10.4 from the CSB book. The Buzzard2015_data.csv file can be found inside the CSB repository at CSB/unix/data/Buzzard2015_data.csv.\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\n1. Write a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nDon’t forget to make your script executable and add the standard header lines.\nFor example, running the script with:\n$ ./explore.sh ../data/Buzzard2015_data.csv 7\n…should return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\n\nHints\n\nYou can select a given column from a csv file using the command cut. Then,\nThe column name is going to be in the first line (header); access it with head.\nFor the next few commands, you’ll need to remove the header line – the tail trick to do so is tail -n +2.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail).\nRename the placeholders $1 and $2 for the command-line arguments to named variables for the file name and column number, respectively.\n\nSolutions\nExercise 1\n\n(1.) Create a directory for these exercises.\n\nmkdir /fs/ess/PAS1855/users/$USER/week05/exercises/\n\n\n(3.) Copy the FASTQ files into your own dir.\n\nmkdir -p data/fastq/\ncp /fs/ess/PAS1855/data/week05/fastq/* data/fastq/\n\nExercise 2\n\n(1.) Create a new script with header lines.\n\nThe first two lines of the script:\n#!/bin/bash\nset -u -e -o pipefail\nSave the file as fastq_stat.sh.\n\n\n(2.) Assign the first argument to a named variable.\n\nAdd a line like this to your fastq_stat.sh script:\nfastq_file=$1\n\n\n(3.) Let the script report its name and the name of the FASTQ file.\n\nFor testing the code below, we first assign one of the FASTQ filenames to the variable $fastq_file:\nfastq_file=201-S4-V4-V5_S53_L001_R1_001.fastq\nAdd lines like these to your fastq_stat.sh script:\necho \"$0: A script to compute basic summary stats for a FASTQ file.\"\necho \"FASTQ file to be analyzed: $fastq_file\"\n\n\n(4.) Compute and report the number of sequences in the FASTQ file.\n\nAdd lines like these to your fastq_stat.sh script:\n# We save the output of our commands using command substitution, $().\n# In the wc -l command, use input redirection so the filename is not in the output.\nn_lines=$(wc -l < \"$fastq_file\")\nn_seqs=$(expr \"$n_lines\" / 4)     # Use expr for arithmetics\n\necho \"Number of sequences: $n_seqs\"\nAlternatively, you can use the (( )) syntax for arithmetics – just take care that in this case, there can be no spaces between the mathematical operator and the numbers:\nn_seqs=$((\"$n_lines\"/4))\nTo get the number of sequences, you can also count the number of lines that only have a + symbol:\nn_seqs=$(grep -c \"^+$\" $fastq_file)\nRecall that + is also a regular expression symbol, but only so in the extended regex set. Therefore, without the -E flag to grep, we are matching a literal + when we use one in our expression.\n\n(5.) Search for adapter sequences.\n\nAdd lines like these to your fastq_stat.sh script:\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelse\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\nTo initiate the if statement:\nWe redirect grep’s output to /dev/null, since we’re only interested in the exit status.\nIf grep finds a match, this evaluates to “true”, and the next code block will be executed. If no match is found, the block after else will be executed.\nInside the if statement:\nWe use the grep’s -E option to search for either of the two primer sequences at once with |.\nWe use character classes like [CT] in place of each ambiguity code.\nWe use grep’s -c option to count the matches.\nOr, to explicitly check the file contains _R2 in its name, rather than assuming this must be the case if it doesn’t contain _R1, you can use elif (short for “else-if”) to add another test:\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelif echo \"$fastq_file\" | grep \"_R2_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\nWhen we test this code by itself, we get:\n#> FASTQ file contains forward (R1) reads, checking for primer 1...\n#> Number of forward primer sequences found: 44687\nThis looks good!\n\n\n(6.) Bonus: Print a count table of sequence lengths.\n\nAdd lines like these to your fastq_stat.sh script:\necho \"Count table of sequence lengths:\"\nawk '{ if(NR%4 == 2) print length($0) }' \"$fastq_file\" | sort | uniq -c\nTo select only actual sequences, and not other lines, from the FASTQ file, we use the following trick. We know that in the FASTQ file, every fourth line, starting from line number 2, contains the actual sequence (i.e.: line 2, line 6, line 10, etc). To select these lines we can use the “modulo” operator to select only line numbers (NR in awk) for which, after dividing the line number by 4, we have 2 left: NR%4 == 2.\nNext we print the number of characters on the entire line using awk’s length function: print length($0).\nFinally, we pass the sequence lengths on to the sort | uniq -c idiom, which will give us a count table.\n\n(7.) Make the script executable.\n\nAssuming it is in the working dir:\n$ chmod u+x fastq_stat.sh \n\n# Or for all scripts at once:\n$ chmod u+x *sh\n\n\n(8.) Run the script for a single FASTQ file by calling it from the command line.\n\n# Assuming you have assigned a FASTQ file to $fastq_file for testing:\n./fastq_stat.sh $fastq_file\n\nExercise 3\n\n(1.) Change the script to let it accept a list of FASTQ file names as an argument.\n\nAdd this line to the script:\nfile_list=\"$1\"\nNow, replace the following line:\n# Old line:\n# sample_files=($(cut -f 3 \"$sample_info\"))\n\n# New line:\nsample_files=($(cat \"$file_list\"))\n\n\n(2.) Add the output directory as a second argument to the script, and create the output dir if necessary.\n\noutput_dir=\"$2\"\n\n# Create the output dir, if necessary:\nmkdir -p \"$output_dir\"\nmkdir -p will not complain if the directory already exists, and it can make multiple levels of directories at once.\n\n(3.) Modify the line in the script that calls fastq_stat to call your script.\n\n# Old line:\n# fastq_stat \"$fastq_file\" > stats/$results_file\n\n# New line:\nscripts/fastq_stat.sh \"$fastq_file\" > \"$output_dir\"/\"$results_file\"\nThe results_file line can remain the same:\nresults_file=\"$(basename $fastq_file .fastq)-stats.txt\"\n\n\n(4.) In each iteration of the loop, let the script report which FASTQ will be analyzed.\n\nAdd the following line inside the loop:\necho \"Running fastq_stat for FASTQ file $fastq_file\"\n\n\n(5.) Create a second file/script to create a list of FASTQ files and to run the loop script.\n\nTo create a list of FASTQ files:\nfile_list=fastq_file_list.txt\n\nls data/*fastq >\"$file_list\"\nTo run the loop script:\noutput_dir=results/fastq_stats\n\n./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\n\n(6.) Make fastq_stat_loop.sh executable and run it.\n\nRun these lines:\n$ chmod u+x ./fastq_stat_loop.sh\n\n$ ./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\nThe final fastq_stat_loop.sh script.\n\n#!/bin/bash\nset -e -u -o pipefail\n\nfile_list=\"$1\"\noutput_dir=\"$2\"\n\n# Create the output dir, if necessary:\nmkdir -p \"$output_dir\"\n\n# Create an array with FASTQ files\nfastq_files=($(cat \"$file_list\"))\n\n# Report:\necho \"Number of fastq files: ${#fastq_files[@]}\"\n\n# Loop through the array:\nfor fastq_file in \"${fastq_files[@]}\"; do\n\n  echo \"Running fastq_stat for FASTQ file $fastq_file\"\n\n  # Strip .fastq from each FASTQ file, and add suffix:\n  results_file=\"$(basename \"$fastq_file\" .fastq)-stats.txt\"\n\n  # Run \"fastq_stat\" on a file:\n  scripts/fastq_stat.sh \"$fastq_file\" >\"$output_dir\"/\"$results_file\"\n\ndone\n\nThe final lines in the runner script / Markdown code block.\n\nfile_list=fastq_file_list.txt\noutput_dir=results/fastq_stats\n\nls data/*fastq >\"$file_list\"\n\n./fastq_stat_loop.sh \"$file_list\" \"$output_dir\"\n\nExercise 4\n\n(1.) Check whether the FASTQ file is a regular file than can be read.\n\nAdd these lines to your script:\n! -f will be true if the file is not a regular/existing file\n! -r will be true if the file is not readable\nWe use || to separate the two conditions with a logical or.\nWith exit 1, we terminate the script with an exit code that indicates failure.\n\nif [ ! -f \"$fastq_file\" ] || [ ! -r \"$fastq_file\" ]; then\n  echo \"Error: can't open file\"\n  echo \"Second argument should be a readable file\"\n  echo \"You provided: $fastq_file\"\n  exit 1\nfi\nTo test your test:\n$ ./fastq_stat.sh blabla \n\n\n(2.) Check whether only one argument was provided.\n\nAdd these lines to your script:\n$# is the number of command-line arguments passed to the script\n-eq will test whether the numbers to the left and right of it are the same, and will return true if they are.\nWe negate this with !: if the number of arguments is NOT 1, then error out.\nexit 1 will exit with exit code 1, which signifies an error / failure.\n\nif [ ! \"$#\" -eq 1 ]; then   # If the number of args does NOT equal 1, then\n  echo \"Error: wrong number of arguments\"\n  echo \"You provided $# arguments, while 1 is required.\"\n  echo \"Usage: fastq_stat.sh <file-name>\"\n  exit 1\nfi\nTo test your test:\n$ ./fastq_stat.sh                      # No args\n$ ./fastq_stat.sh $fastq_file blabla   # Two args\n\n\n(3.) Add date commands.\n\nSimply include two lines with:\ndate\n… in the script, one before file processing, and one after.\n\nThe final fastq_stat.sh script.\n\n#!/bin/bash\nset -u -e -o pipefail\n\necho \"$0: A script to compute basic summary stats for a FASTQ file.\"\ndate\necho\n\n# Test number of args -------------------------------------------------\nif [ ! \"$#\" -eq 1 ]; then\n  echo \"Error: wrong number of arguments\"\n  echo \"You provided $# arguments, while 1 is required.\"\n  echo \"Usage: fastq_stat.sh <file-name>\"\n  exit 1\nfi\n\n# Process command-line args and report ------------------------------------\n\nfastq_file=\"$1\"\n\necho \"FASTQ file to be analyzed: $fastq_file\"\necho\n\nif [ ! -f \"$fastq_file\" ] || [ ! -r \"$fastq_file\" ]; then\n  echo \"Error: can't open file\"\n  echo \"Second argument should be a readable file\"\n  echo \"You provided: $fastq_file\"\n  exit 1\nfi\n\n# Count primer sequences ------------------------------------------------\n\nn_lines=$(wc -l <\"$fastq_file\")\nn_seqs=$(expr \"$n_lines\" / 4)\n\necho \"Number of sequences: $n_seqs\"\n\n# Count primer sequences ------------------------------------------------\n\nif echo \"$fastq_file\" | grep \"_R1_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R1) reads, checking for primer 1...\"\n\n  n_primerF=$(grep -Ec \"GAGTG[CT]CAGC[AC]GCCGCGGTAA|TTACCGCGGC[GT]GCTG[AG]CACTC\" \"$fastq_file\")\n  echo \"Number of forward primer sequences found: $n_primerF\"\n\nelif echo \"$fastq_file\" | grep \"_R2_\" >/dev/null; then\n\n  echo \"FASTQ file contains forward (R2) reads, checking for primer 2...\"\n\n  n_primerR=$(grep -Ec \"ACGGACTAC[ACTG][ACG]GGGT[AT]TCTAAT|ATTAGA[AT]ACCCB[ACTG]GTAGTCCGT\" \"$fastq_file\")\n  echo \"Number of reverse primer sequences found: $n_primerR\"\n\nfi\n\n# Bonus: Count table of sequence lengths ---------------------------------\necho \"Count table of sequence lengths:\"\nawk '{ if(NR%4 == 2) print length($0) }' \"$fastq_file\" | sort | uniq -c\n\n# Report ----------------------------------------------------------------\necho\necho \"Done with $0 for $fastq_file.\"\ndate\n\nExercise 5\n\nSolution\n\nThere are several ways to do this, here’s one example:\n#!/bin/bash\nset -u -e -o pipefail\n\ndir=\"$1\"\nextension=\"$2\"\n\nwc -l \"$dir\"/*.\"$extension\" | sort -rn | sed -n '2p'\nYou have to print the second rather than the the first line: the first line will be a total line number count across all files, which wc automatically computes.\nInstead of using sed, you could also use head -n 2 | tail -n 1 to print the second line:\nwc -l \"$dir\"/*.\"$extension\" | sort -rn | head -n 2 | tail -n 1\n(Note also that there are two files with the same number of lines: R1 and R2 files for the same sample always have the same number of reads.)\n\nExercise 6\n\n(1.) Write a script that takes one file and determines the number of rows and columns.\n\n#!/bin/bash\nset -u -e -o pipefail\n\nfile=\"$1\"\n\necho \"Filename:\"\necho \"$file\"\n\n# We can get the number of rows simply by counting the number of lines:\n# To avoid printing the filename we redirect the input like below\n# (Or we could have done: \"cat ../data/Saavedra2013/n10.txt | wc -l\")\necho \"Number of rows:\"\nwc -l < \"$file\"\n\n# To count the number of columns, we use awk - recall that NF is the number\n# of fields (columns), and to print that number only for a single line, we exit:\necho \"Number of columns:\"\nawk '{ print NF; exit }' \"$file\"\n\n# head -n 1 \"$file\" | awk '{ print NF }' # Also works\nWe can save this script as netsize.sh and make it executable using chmod u+x netsize.sh.\n\n\n(2.) Write a script that prints the number of rows and columns for each network.\n\n#!/bin/bash\nset -u -e -o pipefail\n\ndir=\"$1\"\n\n# We can loop over the files using globbing:\nfor file in \"$dir\"/*.txt; do\n\n    # Next, we can save the number of rows and columns in variables:\n    n_row=$(wc -l < \"$file\")\n    n_col=$(awk '{ print NF; exit }' \"$file\")\n    \n    # And print them all on one line:\n    echo -e \"$file \\t $n_row \\t $n_col\"\ndone\nWe can save this script as netsize_all.sh and run it as follows:\n./netsize_all.sh ../data/Saavedra2013\n\n\n(3.) Which network has the largest number of rows and which the largest number of columns?\n\n# Having written the script netsize_all.sh,\n# you can take its output and order it according to rows or columns.\n\n# Sorting by column 2 gives you the file with the largest number of rows:\n$ ./netsize_all.sh | sort -n -r -k 2 | head -n 1\n#> ../data/Saavedra2013/n58.txt 678 90\n\n# Sorting by column 3 gives you the file with the largest number of columns:\n\n$ ./netsize_all.sh | sort -n -r -k 3 | head -n 1\n#>  ../data/Saavedra2013/n56.txt 110 207\n\nExercise 7\n\nSolution\n\n#!/bin/bash\n\nset -u -e -o pipefail\n\nfile=$1    # $1 is the file name\ncolumn=$2  # $2 is the column of interest\n\necho \"Column name:\"\ncut -d ',' -f \"$column\" \"$file\" | head -n 1\n\n# In the next lines, we need to skip the header, which we can do using\n# tail -n +2\necho \"Number of distinct values:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort | uniq | wc -l\n\necho \"Minimum value:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort -n | head -n 1\n\necho \"Maximum value:\"\ncut -d ',' -f \"$column\" \"$file\" | tail -n +2 | sort -n | tail -n 1\nIf we save the script as explore.sh, and make it executable, we can run it using:\n./explore.sh ../data/Buzzard2015_data.csv 6\n\n# Column name\n# Abund.n\n# Number of distinct values:\n# 46\n# Minimum value:\n# 1\n# Maximum value:\n# 157\n\n# This works well also for alphabetical order:\n\n./explore.sh ../data/Buzzard2015_data.csv 3\n\n# Column name\n# genus\n# Number of distinct values:\n# 85\n# Minimum value:\n# Acacia\n# Maximum value:\n# Zanthoxylum\n\nThere initially was an error in these primer sequences (one sequence repeated twice), which has been corrected on Friday, Feb 12.↩︎\nThis exercise was slightly modified from Software Carpentry’s Shell Novice tutorial.↩︎\n",
      "last_modified": "2021-04-25T17:52:06-04:00"
    },
    {
      "path": "w05_readings.html",
      "title": "Week 5 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, we will talk about shell scripting. Last week, we focused on more advanced shell commands that can help to summarize and reformat data – but we issued these commands interactively in the shell, one line at a time. When you need to repeat a certain sequence of commands regularly, or run a program that may take a while, it becomes useful to put your shell commands in a script. Such a script can be easily and quickly (re-)executed, or submitted to a queue on a cluster (next week’s topic!).\nSince shell scripts are essentially sequences of shell commands with some added bells and whistles, it is relatively straightforward to start using them with what we have learned so far. That said, we will start by further setting the stage for scripts by talking about loops, conditionals, and variables on Tuesday. On Thursday, we will talk about shell scripts themselves.\nSome of the things you will learn this week:\nWhy it is useful to collect your commands into scripts that can be rerun easily.\nfor and while loops.\nif statements and true/false tests.\nShell variables (and arrays) and how to use them.\nWhy and how to start shell scripts with a “shebang” line and set settings.\nWhy and how to dress up your scripts with tests and echo statements.\nReadings\nThis week’s reading is Chapter 12 from the Buffalo book.\nThe latter part of this chapter is about using find, xargs, and Makefiles. These are somewhat tangential to the week’s topic of scripts: in class, I will likely only touch briefly on find.\nIf you can, do read these sections, but I recommend you focus on the first part of the chapter, especially if run out of time or bandwidth.\nAs for Makefiles specifically, it will be good to understand the principle behind them. However, there is no need to fully understand the syntax, since we will learn about Snakemake, an alternative approach to workflow management, later in the course.\nRequired readings\nBuffalo Chapter 12: “Bioinformatics Shell Scripting, Writing Pipelines, and Parallelizing Tasks”\n\n\n\n",
      "last_modified": "2021-04-25T17:52:07-04:00"
    },
    {
      "path": "w06_GA_scripts.html",
      "title": "Graded Assignment II: Shell scripts at OSC",
      "author": [],
      "contents": "\n\nContents\nIntroduction\nGeneral instructions\nGrading information\n\nGetting set up\nCutadapt script for one sample\nRunning the script and finishing up\nOptional (ungraded) - Loop over all samples\n\n\nIntroduction\nIf you did last week’s exercises, much of the following introduction will be familiar to you, as we will be working with the same FASTQ files and looking at the same primers. This time around, you will actually remove the primer sequences using the software Cutadapt.\nThis assignment will work with 6 FASTQ files with sequences from the V4 region of 16S rRNA, generated in a metabarcoding experiment.\nThe FASTQ files come in pairs: for every sample, there is a FASTQ file with forward reads (or “read 1” reads) that contains _R1_ in its file name, and a FASTQ file with corresponding reverse reads (or “read 2” reads) that contains _R2_ in its file name. So, our 6 FASTQ files consist of 3 pairs of R1-R2 files for 3 biological samples.\nThe sequences were generated by first amplifying environmental samples with a pair of universal 16S primers, and these primer sequences are expected to be present in the FASTQ sequences. You will remove these primer sequences with the program Cutadapt, and there are two things to be aware of:\nA primer can also be present in the FASTQ sequence as its reverse complement, so we will search for reverse complements too.\nThe primers contain a few variable sites for which ambiguity codes are used. For instance, an R means that the site can be either an A or a G, and an N means that the site can be any of the four bases. See here for a complete overview of these ambiguity codes.\nThese are the primer sequences1:\nForward primer (“515F”): GAGTGYCAGCMGCCGCGGTAA.\nReverse primer (“806R”): ACGGACTACNVGGGTWTCTAAT.\nGeneral instructions\nFor each numbered step below, you should create at least one Git commit.\nAll files should be added to your repository, unless mentioned otherwise.\nGrading information\nThe number of points (if any) that you can earn for each step are denoted between square brackets (e.g. [0.5]). In total, you can earn 10 points with this assignment, which is 10% of your grade for the class.\n\nGetting set up\nCreate a new directory for this assignment, and inside it, initialize a Git repository. Add a very brief README.md describing that this is a repository for such-and-such assignment. (You can also use this README to further document your workflow, but you don’t have to.) [0.5]\nCopy the FASTQ files from /fs/ess/PAS1855/data/week05/fastq into a directory data/fastq/ inside your assignment’s directory. [0.5]\nCreate a .gitignore file and add a line to make Git ignore all .fastq files. [0.5]\nLoad the Conda module at OSC and create a Conda environment for Cutadapt following these instructions (i.e. just the section “Installation with Conda”). [1]\nHints\n\nTo be able to install Cutadapt with the command provided in that link, you first need to do the following general Conda setup steps, which we will (have) do(ne) together in class:\n$ conda config --add channels defaults\n$ conda config --add channels bioconda\n$ conda config --add channels conda-forge \n(Adding the above lines in that order will result in the different channels having the proper priority, with conda-forge having the highest priority.)\nIf you run into installation problems that you can’t seem to solve, you can can also use my Conda environment in Step 7, below, as follows:\n$ source activate users/PAS0471/jelmer/.conda/envs/cutadaptenv\nBut do this only if needed, and if you do, include a description of your errors in your README file.\n\n\nExport the environment description for your Cutadapt environment to a YAML (.yml) file. [0.5]\n\nCutadapt script for one sample\nNow, you will write a script called cutadapt_single.sh that runs Cutadapt for one pair of FASTQ files: a file with forward (R1) reads and a file with reverse (R2) reads for the same sample.\nThe following instructions all refer to what you should write inside the script:\nStart with the shebang line followed by SLURM directives. Specify at least the following SLURM directives [0.5]\nThe class’s OSC project number, PAS1855.\nA 20-minute wall-time limit.\nExplicitly ask for one node, one process (task), and one core (these are three separate directives).\n\nNext, include the familiar set settings for robust bash scripts, load OSC’s Conda module and then activate your own Cutadapt Conda environment. [0.5]\nHints\nUse source activate and not conda activate to activate the Conda environment, otherwise Conda will complain about your shell not being properly set up.\n\nLet the script take 4 arguments that can be passed to it on the command-line: [1]\nThe path to a FASTQ file with forward reads (whose value, when passed to the script from the shell will e.g. be data/fastq/201-S4-V4-V5_S53_L001_R1_001.fastq).\nThe name of the output directory for trimmed FASTQ files (whose value, when passed to the script from the shell will be whatever you pick, e.g. results/trim).\nThe sequence of the forward primer (whose value when passed to the script from the shell, will be GAGTGYCAGCMGCCGCGGTAA in this case).\nThe sequence of the reverse primer (whose value when passed to the script from the shell, will be TTACCGCGGCKGCTGRCACTC in this case).\nGive each of these variables a descriptive name rather than directly using the placeholder variables ($1, etc) – that will make your life easier when writing the rest of the script.\nCompute the reverse complement for each primer. The hint below in fact has the answer, but please take a moment to think how you might do this with tr and a new command called rev that simply reverses a string.\nHints\n\nAdjust your variable names as necessary!\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBVDH TAGCRYMKVBHD | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBVDH TAGCRYMKVBHD | rev)\nNote that we needed to not only translate ACTG but also all IUPAC ambiguity codes.\n\n\nFrom the file name of the input FASTQ file with forward reads (which is one of the arguments to the script), infer the name of the corresponding FASTQ file with reverse reads, which will have an identical name except that _R1_ is replaced by _R2_. [0.5]\nAssign output file paths (output dir + file name) for the R1 and R2 output file, inserting _trimmed before the .fastq file extension (that is, the output file paths should be along the lines of <output-dir>/<old-file-basename>_trimmed.fastq). [1]\nHints\n\nYou’ll do yourself a favor by having the script echo the file names that you have assigned, so you can easily check if you’re doing this right.\nMoreover, I recommend that you test interactively whether you’re getting all the file names right: assign one of the paths to the actual FASTQ files to the variable name you’re using for that, and ditto with an output dir. Just take care that these assignments don’t end up in your (final) script.\n\n\nCreate the output directory if it doesn’t already exist. [0.5]\nThe actual call to the Cutadapt program should be as follows – just change any variable names as needed:\n$ cutadapt -a \"$primer_f\"...\"$primer_r_revcomp\" \\\n    -A \"$primer_r\"...\"$primer_f_revcomp\" \\\n    --discard-untrimmed --pair-filter=any \\\n    -o \"$R1_out\" -p \"$R2_out\" \"$R1_in\" \"$R2_in\"\nOptional (ungraded): Touch up the scripts with additional echo statements, date commands, and tests such as whether 4 arguments were provided.\n\nRunning the script and finishing up\nSubmit the script as a SLURM job for one pair of FASTQ files – don’t forget to provide it with the appropriate arguments. Check the SLURM log file and the output files. If it didn’t work, troubleshoot until you get it working. [2]\nDo any necessary cleaning up of files, e.g. move your SLURM log file to an appropriate place, and make sure everything is committed to the Git repository. (I’ll need to see the SLURM log file in your repository to see if the script worked.) [0.5]\nCreate a GitHub repository and push your local Git repository to GitHub. Like last time, start an issue and in the issue, tag @jelmerp. [0.5]\n\nOptional (ungraded) - Loop over all samples\nCreating a script like we did above is worth the trouble mostly if we plan to run it for multiple/many samples. Now, you will create a second script cutadapt_submit.sh that loops over all FASTQ files in a specified directory. It doesn’t need to be a “proper” script with a robust header and so on, and shouldn’t contain any SLURM directives: this script merely functions to submit SLURM jobs and can be run interactively.\nLoop over a globbing pattern that accepts all .fastq files with R1 in the name in the input directory (recall: we don’t want to loop over the R2 files explicitly, because they will be automatically included in our previous script).\nInside the loop, the cutadapt_single.sh script should be submitted as a SLURM job, similar to your single submission of the script above.\nRun the loop.\nCheck the SLURM log files and the output directory. If it didn’t work, remove all these files, troubleshoot, and try again until it is working.\n\nInitially, there was an error in the primer sequences provided in last week’s exercises, which has been corrected on Friday, Feb 12.↩︎\n",
      "last_modified": "2021-04-25T17:52:07-04:00"
    },
    {
      "path": "w06_readings.html",
      "title": "Week 6 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, we’ll start with an introduction to OSC and supercomputers. Then, we’ll see how we can submit shell scripts to OSC’s queue for compute jobs with the widely-used SLURM resource manager. Finally, we will learn about installing and managing software with Conda, and loading pre-installed software with the “module” system.\nSome of the things you will learn this week:\nWhen and why we need to run our analyses on supercomputers.\nKey terminology around supercomputers.\nDifferent ways to access and transfer data to and from OSC.\nDifferent ways to start “compute jobs”: via OnDemand, with interactive jobs, and with scripts.\nSome strategies around requesting appropriate resources for your compute jobs.\nThe SLURM/sbatch syntax to request specific resources for your compute jobs.\nHow to monitor SLURM jobs.\nHow to load pre-installed software at OSC with the “module” system.\nHow to install and manage software with Conda.\nReadings\nThis week’s required reading is a little unusual, as it is not a book chapter. Instead, you will read part of the OSC documentation. There are some pointers below, but you are welcome to use your own judgment in how much you want to read. You are also encouraged to simply look around on OSC’s website a bit to get an idea of what’s there, and bookmark pages that you think will be useful for you later on.\nRequired readings\nPart of the OSC documentation:\nYour first starting point should be the New User Resource Guide At least read or carefully look at the first few pages including “HPC Basics” and “Getting Connected”.\nYour second starting point should be the Batch Processing at OSC pages Again, at least read or carefully look at the first few pages including “Batch System Concepts”, “Batch Execution Environment”, and “Job Scripts” (in that last section, you can stop when you reach “Considerations for parallel jobs”, unless this is something that interests you).\nOptional readings\nBuffalo Chapter 4: “Working with Remote Machines”\n\n\n\n",
      "last_modified": "2021-04-25T17:52:07-04:00"
    },
    {
      "path": "w06_UA_ssh.html",
      "title": "Optional Ungraded Assignment: SSH setup",
      "author": [],
      "contents": "\n\nContents\nIntroduction\n1. Avoid being prompted for password\n2. Use a shortcut for your SSH connection\n3. Set up your local VS Code to SSH tunnel into OSC\n\n\nIntroduction\nYou can connect to OSC with SSH if you have a bash shell on your machine, either natively with a Mac or Linux machine, or via WSL, Git Bash, or another application on Windows.\nThe regular way to do this is typing ssh <username>@pitzer.osc.edu and then providing your password, which can get tedious. There are two setup steps you can take to make this quicker:\nAvoid being prompted for your password.\nSet up a shortcut for your SSH connection name.\nA third thing you can do below, assuming you have VS Code installed locally, is to get it to SSH tunnel to OSC: this will be as if you have VS Code open in OnDemand, but then not in your browser!\n1. Avoid being prompted for password\nThese steps are similar to what you did for your SSH Github authentication.\nOn your own computer, generate a public-private SSH key-pair:\n$ ssh-keygen -t rsa\nWhen you’re prompted for a passphrase, you can just press enter if you are okay with not having a passphrase.\n\nIf you do want a passphrase, you have to take an extra step to not be prompted, after step 3 below:\n$ ssh-add\n# (Then here, don't enter a passphrase in any case!)\n\nFrom your own computer, transfer the public key to the remote computer:\n# Replace <user> by your username, e.g. \"jelmer@owens.osc.edu\"\n$ cat ~/.ssh/id_rsa.pub | ssh <user>@owens.osc.edu 'mkdir -p .ssh && cat >> .ssh/authorized_keys'\nLog in to the remote computer (OSC) and once there, set appropriate permissions:\n$ chmod 700 .ssh; chmod 640 .ssh/authorized_keys\n\nSee also this Tecmint post in case you’re struggling, and Buffalo Chapter 4 page 59-60.\n\n\n2. Use a shortcut for your SSH connection\nThese two steps should both be done on your local machine.\nCreate a file called ~/.ssh/config:\n$ touch ~/.ssh/config\nOpen the file in a text editor and add your alias(es) in the following format:\nHost <arbitrary-alias-name>    \n     HostName <remote-name>\n     User <user-name>\nFor instance, I have something along these lines for Pitzer and Owens:\nHost op\n    HostName pitzer.osc.edu\n    User jelmer\n\nHost oo\n    HostName owens.osc.edu\n    User jelmer\nNow, you just need to use your, preferably very short, alias to log in:\n$ ssh op\nThis shortcut will also work with scp and rsync!\n$ rsync ~/scripts op:/fs/ess/PAS1855/scripts\n\n3. Set up your local VS Code to SSH tunnel into OSC\nIf you want to use VS Code to write code, have a shell, and interact with files at OSC directly, you don’t need to use the VS Code Server from OSC OnDemand. You can also “SSH tunnel” in with your local installation. This is a more convenient way of working because it’s quicker to start up and you are not working inside a browser (which, among other things, takes away screen space and interferes with some keyboard shortcuts).\nThe set-up is pretty simple (see also these instructions if you get stuck), just recall to do this in your local installation of VS Code, assuming you have one.\nInstall the VS Code “Remote Development extension pack” by opening up the Extensions side bar, searching for it there, and then clicking “Install”.\nOpen up the Command Palette (F1 or Ctrl+ShiftP) and start typing “Remote SSH”. Then, select Remote-SSH: Connect to Host… and specify your SSH connection: e.g. ssh jelmer@pitzer.osc.edu (you’ll have to do this separately for Pitzer and Owens if you want to be able to connect to either).\nWith the above setup, you shouldn’t be prompted for a password and VS Code will connect to OSC!\nIf you’re asked about the operating system of the host, select Linux, which is the operating system of the OSC clusters.\nNow, if you open a shell or try to open a file, you are interacting with the OSC file system and not your local computer.\n\n\n\n\n",
      "last_modified": "2021-04-25T17:52:08-04:00"
    },
    {
      "path": "w07_exercises.html",
      "title": "Exercises: Week 7",
      "author": [],
      "contents": "\n\nContents\nExercises\nExercise 1: Download genome assembly files\nExercise 2: Checking out the downloaded files\nExercise 3: File transfer\n\nSolutions\nExercise 1\nExercise 2\nExercise 3\n\n\nExercises\nYou can do exercises 1 and 2 in a shell either at OSC or on your local computer: whichever you prefer. In exercise 3, you will be transferring files to or from OSC, which should be done from a local shell.\nExercise 1: Download genome assembly files\nIn this exercise, you’ll download a set of files associated with a genome assembly of one organism from NCBI. As an example, I’m using the genome assembly files for Phytophthora nicotianae var. parasitica (also known simply as P. parasitica), one of several species of fungus that causes buckeye rot of tomato.\nNote, if you are more interested in getting the equivalent files for another organism, you should be able to follow the same steps.\nGo to https://www.ncbi.nlm.nih.gov. NCBI has a number of different databases, including “SRA” for short next-generation sequencing reads, and “PubMed” for publications. In the drop-down menu that says All Databases next to the Search box, select Assembly and type Phytophthora parasitica: we want to search for whole-genome assemblies for this species.\nClick the assembly in the prominent box at the top that says Phyt_para_CJ02B3_V1.\nHints\n\nHere is the link to the assembly, in case things look different for you: https://www.ncbi.nlm.nih.gov/assembly/GCA_000509465.1. (But again, you can also download files from a different organism or assembly.)\n\n\nNow, you should be on the assembly page. There is a big blue button Download Assembly, but if you click on it, you can see that you can only download one file at a time. Since we want all 20 or so files associated with the assembly, we will use wget instead. Note that this would scale easily even if we wanted files for multiple assemblies (for instance, there are 9 different assemblies for P. parasitica, as the page reports).\nClick on FTP directory for GenBank assembly in the top list on the right hand side.\nUse a wget command to download all the files you see on the assembly page, using the URL in the address bar.\nIn addition to the option for downloading multiple files at once (and optionally, other options to optimize the download), make sure you use the following options:\n--no-parent — Stop wget from moving up via the Parent Directory link at the top of the page, which would lead it to start downloading files from other assemblies…\n-e robots=off — Ignore robots.txt to enable the download (as specified in NCBI’s download FAQ).\n\nIf you see errors saying that filenames are too long, you’ll have to use the --no-host-directories and --cut-dirs=6 options as further detailed in the Hints, because all the nested directories that are being created may end up creating paths that are too long.\n\nHints\n\nYour command should be structured as follows:\n$ wget --no-parent -e robots=off <other-options> <link-to-page>\nThe link to the page is: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/509/465/GCA_000509465.1_Phyt_para_CJ02B3_V1/\nUse the --recursive option to download more than one file at once.\nBy default, the files will be placed inside several levels of directories:\n# ftp.ncbi.nlm.nih.gov/\n# └── genomes\n#     └── genbank\n#         └── protozoa\n#             └── Phytophthora_parasitica\n#                 └── latest_assembly_versions\n#                     └── GCA_000509525.1_Phyt_para_IAC_01_95_V1\nTo avoid this, you can use the options --no-host-directories and --cut-dirs=6 as suggested in the NCBI’s download FAQ. And if you do so, you’ll also want to specify an arbitrary target dir name with -P, e.g. -P ncbi_Pparasiticus.\nIf you want to play around with different options, I suggest you use the --accept option to only select one or two small files to be downloaded, e.g. --accept \"*assembly_stats*\".\nDon’t hesitate to look at the solution if your first attempts fail. The minutiae of wget commands aren’t terribly important – the next exercise, while easier, is more so.\n\nExercise 2: Checking out the downloaded files\nMove inside the directory that you’ve downloaded where all the assembly files are. You should be seeing over 20 files, including GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_report.txt and GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz.\nOne of the files you’ve downloaded has md5 checksums for all the other files: md5checksums.txt. Use this file in a single md5sum command to verify that all the downloaded files are identical to the source, i.e. that they have the correct checksums.\nHints\n\nUse the -c (check) option to md5sum to cross-check the checksums of the files in your directory with those in the file you specify.\n\n\nCount the number of lines in the genome sequence (GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.fna.gz) that contain CAGCAGCAG, without unzipping the file.\n(Side note in case you are wondering about the fna extension: this explicitly denotes a FASTA file with nucleotide sequences, as opposed to one with amino acid sequences, .faa.)\nHints\n\nzgrep will allow you to search in a gzipped file.\nRecall that (z)grep’s -c option will count matches.\n\nCheck the size of the same zipped genome sequence FASTA file. Then, unzip the file, and check the file size again. Finally, gzip it back up.\nHints\n\nTo unzip, use gunzip <filename>.\nTo zip, use gzip <filename>.\nCheck the file size with ls or du.\n\nDoes the checksum still match for the “rezipped” FASTA file?\nHints\n\nYou can simply use the same command you used in the first step of this exercise.\n\n\nExercise 3: File transfer\nTransfer the entire directory you downloaded above to OSC (in case you have been working locally so far) or to your local computer (in case you have been working at OSC so far).\nYou can use rsync and/or SFTP, whichever you prefer to get some practice with.\nEdit Fri, Feb 26: We did not end up discussing SFTP in class. But if you are interested in getting experience with this (at OSC, it is recommended for larger transfers since the transfer does not use a login node like scp andrsync do), have a look at this tutorial. The hint below will tell you how you can log in to the OSC SFTP server.\n\nHints\n\nrsync\nRecall that using a trailing slash in the source dir will make a difference. In this case (and usually) you will most likely not want to include a trailing slash.\nsftp\nLog in using:\nsftp sftp.osc.edu\nLike many commands, the put and get functions need the -r option to work recursively.\nIn sftp, the ~ shortcut for your home dir does not work.\n\nSolutions\nExercise 1\nUse a wget command to download all these files at once.\n\n\nThe wget command with the fewest options that works for our goal is:\n$ wget --recursive -e robots=off --no-parent \\\n    https://ftp.ncbi.nlm.nih.gov/genomes/genbank/protozoa/Phytophthora_parasitica/latest_assembly_versions/GCA_000509525.1_Phyt_para_IAC_01_95_V1/\nThese options (--recursive -e robots=off --no-parent) were all discussed in the question and the hints.\nIf you wanted to place these files in a dir with a name of your choice, and no unnecessarily nest directories (see the hint above), you could use:\n--no-host-directories in combination with --cut-dirs=6 will remove the 6 levels of nested directories you got earlier (ftp.ncbi.nlm.nih.gov/genomes/genbank/protozoa/Phytophthora_parasitica/latest_assembly_versions).\n-P ncbi_Pparasiticus/ to name your directory.\nFurthermore, since we are not interested in index.html (the actual HTML we are looking at on the web), we can exclude it using --reject \"index.html\".\nAll in all, this would result in the following command:\n$ wget --recursive -e robots=off --no-parent \\\n       --reject \"index.html\" \\\n       --no-host-directories --cut-dirs=6 \\\n       -P ncbi_Pparasiticus/ \\\n       https://ftp.ncbi.nlm.nih.gov/genomes/genbank/protozoa/Phytophthora_parasitica/latest_assembly_versions/GCA_000509525.1_Phyt_para_IAC_01_95_V1/ \nThis should result in the following set of files being downloaded (one is directory, in fact):\n$ ls ncbi_Pparasiticus # Or whatever dir your downloads are in\n#> annotation_hashes.txt\n#> assembly_status.txt\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_report.txt\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_stats.txt\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_feature_count.txt.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_feature_table.txt.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.fna.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic_gaps.txt.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gbff.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gff.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gtf.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_protein.faa.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_protein.gpff.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_rm.out.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_rm.run\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_rna_from_genomic.fna.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_translated_cds.faa.gz\n#> GCA_000509525.1_Phyt_para_IAC_01_95_V1_wgsmaster.gbff.gz\n#> index.html.tmp\n#> md5checksums.txt\n#> README.txt\n\nExercise 2\nCheck the file integrity with md5sum.\n\n\nThe -c option, when provided with a filename, will check the checksums of the files in your current directory against those in the file:\n$ md5sum -c md5checksums.txt\n#> ./annotation_hashes.txt: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_report.txt: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_stats.txt: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure/Primary_Assembly/component_localID2acc: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure/Primary_Assembly/scaffold_localID2acc: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure/Primary_Assembly/unplaced_scaffolds/AGP/unplaced.scaf.agp.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_assembly_structure/Primary_Assembly/unplaced_scaffolds/FASTA/unplaced.scaf.fna.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_feature_count.txt.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_feature_table.txt.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.fna.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic_gaps.txt.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gbff.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gff.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_genomic.gtf.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_protein.faa.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_protein.gpff.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_rm.out.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_rm.run: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_rna_from_genomic.fna.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_translated_cds.faa.gz: OK\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_wgsmaster.gbff.gz: OK\nIf all went well, all files should say OK.\nIf there was a mismatch somewhere, you should also see a separate warning at the end of the output, like:\n#> md5sum: WARNING: 1 computed checksum did NOT match\n\nCount the number of lines in the genome sequence that contain CAGCAGCAG.\n\n\n$ zgrep -c \"CAGCAGCAG\" GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n#> 1426\n\nCheck file size, unzip and zip.\n\n\n$ ls -lh GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n#> -rw-rw-r-- 1 jelmer jelmer 8.8M Dec 21  2017 GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n\n$ gunzip GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz\n\n$ ls -lh GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna\n#> -rw-rw-r-- 1 jelmer jelmer 38M Dec 21  2017 GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna\n\n$ gzip GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna\nThe zipped file is less than a quarter of the size of the unzipped file.\n\nDoes the checksum still match for the “rezipped” FASTA file?\n\n\n$ md5sum -c md5checksums.txt\n#> ... (showing the pertinent part of the output)\n#> ./GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz: FAILED\n#> ... \n#> md5sum: WARNING: 1 computed checksum did NOT match\nNo, it no longer matched! This is merely due to the fact that the gzipped file contains metadata about when it was zipped.\nHowever, this is another reason not to modify any original files. If you needed an unzipped file, it would be better to unzip it to a separate file using the -c option:\n$ gunzip -c GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna.gz > \\\n      GCA_000509525.1_Phyt_para_IAC_01_95_V1_cds_from_genomic.fna\n\nExercise 3\n\nTransfer with rsync\n\nRegardless of the direction of the transfer, these commands should be executed in a local shell.\nFrom local to OSC, assuming that your files were downloaded in a dir called ncbi_Pparasiticus in your home dir, your username is me, and you made a dir week07/exercises/ for these exercises at OSC (note: rsync will not create “missing” directories!):\n$ rsync -avz --progress \\\n      ~/ncbi_Pparasiticus \\\n      me@pitzer.osc.edu:/fs/ess/PAS1855/users/me/week07/exercises/\nThe above will result in you having an OSC dir /fs/ess/PAS1855/users/me/week07/exercises/ncbi_Pparasiticus. If you would have included a trailing slash in ~/ncbi_Pparasiticus above, you would have copied the contents of ncbi_Pparasiticus directly into the exercises dir, which is probably not what you wanted.\nFrom OSC to local, assuming that your files were downloaded in a dir called ncbi_Pparasiticus in /fs/ess/PAS1855/users/me/week07/exercises/ your username is me, and you copy the downloaded dir straight into your home dir:\n$ rsync -avz --progress \\\n      me@pitzer.osc.edu:/fs/ess/PAS1855/users/me/week07/exercises/ncbi_Pparasiticus \\\n      ~\nThe above will result in you having a local dir ~/ncbi_Pparasiticus. If you would have included a trailing slash in ~/ncbi_Pparasiticus above, you would have copied the contents of ncbi_Pparasiticus directly into your home dir, which is probably not what you wanted.\n\nTransfer with SFTP\n\nFrom local to OSC:\nAssuming that your local home ($HOME / ~) directory is /home/me, and your OSC username is me:\n$ sftp sftp.osc.edu\nsftp> put -r /home/me/ncbi_Pparasiticus/ /fs/ess/PAS1855/users/me/week07/exercises/\nThe above will result in you having an OSC dir /fs/ess/PAS1855/users/me/week07/exercises/ncbi_Pparasiticus.\nFrom OSC to local\nAssuming that your local home ($HOME / ~) directory is /home/me, and your OSC username is me:\n$ sftp sftp.osc.edu\nsftp> get -r /fs/ess/PAS1855/users/me/week07/exercises/ncbi_Pparasiticus/ /home/me/\nThe above will result in you having a local dir ~/ncbi_Pparasiticus.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:08-04:00"
    },
    {
      "path": "w07_readings.html",
      "title": "Week 7 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week is a “half-module” as it overlaps with one of the two “instructional breaks” this semester. We will only meet on Thursday and the chapter to be read (Buffalo Chapter 6) is a very short one.\nWe will talk about downloading and transferring data in the shell, checking file integrity, and working with compressed data. We will also do a recap of loops and Bash scripting.\nSome of the things you will learn this week:\nHow to download data using the shell tools wget (and curl).\nHow to transfer data (in particular to and from OSC) using rsync and sftp.\nHow to check file integrity using “checksums”, to ensure that you have downloaded/transferred files completely.\nHow to compress and uncompress data using gzip, and how to work with gzipped data.\nOptional: Basics of process management – in particular, how to send processes to the background.\nReadings\nThis chapter deals with commands for downloading data, remote copying, checking data integrity after download or transfer, and working with compressed data. Despite the title “Bioinformatics Data”, there is little in the chapter that applies only to bioinformatics data.\nIf you have additional time to spend on the course this week, I recommend you review the material we’ve covered so far, as we will switch gears next week and will be covering Python for much of the rest of the course.\nRequired readings\nBuffalo Chapter 6: “Bioinformatics Data”\n\n\n\n",
      "last_modified": "2021-04-25T17:52:09-04:00"
    },
    {
      "path": "w08_exercises.html",
      "title": "Exercises: Week 8",
      "author": [],
      "contents": "\n\nContents\nSetup\nExercise 1: Variable types and strings\nExercise 2: Lists\n\n\n\n\n\n Edit March 4: I have moved part of the exercises on lists and all those on dictionaries and sets to the page with exercises for week 9. Though you are welcome to work ahead to those already, the exercises that remain here cover the material that we ended up discussing in class this week. \nSetup\nOpen a new file and save it as week08_exercises.py or something along those lines.\nType your commands in the script and send them to the prompt in the Python interactive window by pressing Shift+Enter.\nProblems with the keyboard shortcut?\n\nIf this doesn’t work, check your keyboard shortcut by right-clicking in the script and looking for “Run Selection/Line In Python Interactive Window”.\nAlso, you can open the Command Palette (Ctrl+Shift+P) and look for that shortcut there, and change it if you want.\n\nBecause these exercises have many small steps, I put the solutions right below the question, so you don’t have to scroll back-and-forth all the time. However, make sure you actually try to do the exercises!\n\nExercise 1: Variable types and strings\nPrint the type of the value 4.88.\nSolution\nWe can use the function type():\n\ntype(4.88)\n<class 'float'>\n\nThe result would be the same if we first assigned it as a variable:\n\nnum = 4.88\ntype(num)\n<class 'float'>\n\n\nAssign the variable n_samples = 658, and then extract the third character from n_samples.\nHints\n\nYou can’t index a number like n_samples[index], so you’ll first have to convert n_samples to a string. Also, recall that Python starts counting from 0!\nSolution\n\nn_samples = 658\nstr(n_samples)[2]\n'8'\n\n\nAssign the string ‘CTTATGGAAT’ to a variable called adapter. Print the number of characters in adapter.\nSolution\n\nadapter = 'CTTATGGAAT'\nlen(adapter)\n10\n\n\nReplace all As by Ns in adapter and assign the resulting string to a new variable. Print the new variable.\nHints\n\nUse the string method replace(), and recall that methods are called using the <object_name>.<method_name>() syntax.\nSolution\n\nbad_seq = adapter.replace('A', 'N')\nbad_seq\n'CTTNTGGNNT'\n\n\nFind out what the third argument to the replace() method does by using the built-in help.\nHints\n\nIf you are typing your commands in a script rather than straight in the console, you will get some more information already when typing the opening parenthesis of the method (briefly pause if necessary).\nTo get more help, you can use a notation with a ?, or help(object.method).\nSolution\n\nhelp(adapter.replace)\n# Or: \"adapter.replace?\"\n# Or: \"?adapter.replace\"\nHelp on built-in function replace:\n\nreplace(old, new, count=-1, /) method of builtins.str instance\n    Return a copy with all occurrences of substring old replaced by new.\n\n      count\n        Maximum number of occurrences to replace.\n        -1 (the default value) means replace all occurrences.\n\n    If the optional argument count is given, only the first count occurrences are\n    replaced.\n\nAs it turns out, the third argument, count, determines how many instances of the substring will be replaced.\n\nUsing what you found out in the previous steps, replace just the first two As in adapter by Ns.\nSolution\n\nWe specify 2 as the third argument, which is the number of instances of the substring that will be replaced:\n\nadapter.replace('A', 'N', 2)\n'CTTNTGGNAT'\n\n\nConvert the following strings and numbers to a Boolean value to see what the resulting Boolean is (True or False): \"False\" (with quotes), 0, 1, -1, \"\", None, and see if you can make sense of these results.\nSolution\n\nbool(\"False\")\nTrue\n\n\nbool(1)\nTrue\n\n\nbool(0)\nFalse\n\n\nbool(-1)\nTrue\n\nAs it turns out, among numbers and strings, only 0 is interpreted as False, whereas anything else is interpreted as True.\n\nbool(\"\")\nFalse\n\n\nbool()\nFalse\n\n\nbool(None)\nFalse\n\nBut an empty string, nothing at all between parenthesis, and None (Python’s keyword to define a null value or the lack of a value), are also interpreted as False.\nNote that as soon as you quote \"None\", it is a string again and will be interpreted as True:\n\nbool(\"None\")\nTrue\n\n\nHave a look at the names of the methods that appear when you type adapter. (note the .). Can you find a method that will print the last occurrence of a T in adapter?\nHints\n\nThe method rfind will search from the right-hand side (hence r), and will therefore print the last occurrence of the specified substring.\nSolution\n\nadapter.rfind(\"T\")\n9\n\n\nSplit the sequence by GAGTCCCTNNNAGCAACGTTNNTTCGTCATTAN by Ns.\nHints\n\nUse the split() method for strings.\nSolution\n\nseq = \"GAGTCCCTNNNAGCAACGTTNNTTCGTCATTAN\"\nsplit_seq = seq.split('N')\nsplit_seq\n['GAGTCCCT', '', '', 'AGCAACGTT', '', 'TTCGTCATTA', '']\n\n\n\nExercise 2: Lists\nAssign a list plant_diseases that contains the items fruit_rot, leaf_blight, leaf_spots, stem_blight, canker, wilt, root_knot and root_rot.\nSolution\n\ndiseases = ['fruit_rot', 'leaf_blight', 'leaf_spots', 'stem_blight',\n            'canker', 'wilt', 'root_knot', 'root_rot']\n\n\nExtract stem_blight from diseases by its index (position).\nSolution\nstem_blight is the fourth item and because Python starts counting at 0, this is index number 3.\n\ndiseases[3]\n'stem_blight'\n\n\nExtract the first 5 items from diseases.\nHints\n\nRecall that when using ranges, Python does not include the item corresponding to the last index.\nSolution\n\nWhile index 5 is the sixth item, it is not included, so we specify 0:5 or :5 to extract elements up to and including the fifth one:\n\ndiseases[0:5]\n['fruit_rot', 'leaf_blight', 'leaf_spots', 'stem_blight', 'canker']\n\nOr:\n\ndiseases[:5]\n['fruit_rot', 'leaf_blight', 'leaf_spots', 'stem_blight', 'canker']\n\n\nExtract the last item from diseases.\nHints\n\nRecall that you can use negative numbers to start counting from the end. Also, while 0 is the first index, “-0” (or something along those lines) is not the last index.\nSolution\n\ndiseases[-1]\n'root_rot'\n\n\nExtract the last 3 items from diseases.\nSolution\n\nNote that you’ll have to omit a number after the colon in this case, because [-3:-1] would not include the last number, and [-3:0] does not work either.\n\ndiseases[-3:]\n['wilt', 'root_knot', 'root_rot']\n\n\n",
      "last_modified": "2021-04-25T17:52:11-04:00"
    },
    {
      "path": "w08_python-resources.html",
      "title": "Python resources",
      "author": [],
      "contents": "\n\nContents\nGeneral resources for learning Python\nAvailable online via the OSU library\nFree online resources\nNot free\n\nOther resources\nBest practices\nMiscellaneous\n\nCoding infrastructure\nVS Code\nJupyter Notebooks / JupyterLab\n\n\nGeneral resources for learning Python\nHere is a list of recommended resources for learning Python. Most are geared towards beginners (in Python and programming alike). Also, the bioinformatics-specific books are all quite practical, describing “applied bioinformatics” – i.e. more how to analyze your data than how to write your own algorithms.\nAvailable online via the OSU library\nBook: Python for the life sciences: a gentle introduction to python for life scientists (Alexandar Lancestar, 2019). This book is very explicitly geared towards biologists with no or little programming experience, and takes a very practical and project-oriented approach. From what I’ve seen of the book, I can highly recommended it!\nBook: Python for Bioinformatics (Sebastian Bassi, 2018). This book also starts with an introduction to Python and then has chapters that each describe practical problems/projects for Python, like “Calculating melting temperature from a set of primers”.\n(Associated GitHub repository.)\nBook: Python programming for biology, bioinformatics, and beyond (Tim Stevens, 2015). This book starts with an introduction to Python and then has chapters on topics like “Pairwise sequence alignments”, “Sequence variation and evolution”, and “High-throughput sequences”.\nBook: Reproducible Bioinformatics with Python (Ken Youens-Clark, 2021). This is a slightly more advanced book that does not start with an introduction to Python, but you should be able to follow the book with what you’ll learn over the next couple of weeks in the course.\nFree online resources\nVideos\nA YouTube playlist of Microsoft videos introducing Python\nVideos from the MIT course “Introduction to Computer Science and Programming in Python”\nCourses\nPython for everybody – Includes course materials and lectures, and is also available at Coursera and edX.\nProgramming for Biology – This is the Cold Spring Harbor course that your TA Zach took, and the materials are available online.\nNot free\nBook: Bioinformatics with Python Cookbook (Tiago Antao, 2018).\n\nOther resources\nBest practices\nPEP 8 — the Style Guide for Python Code\nTen simple rules for writing and sharing computational analyses in Jupyter Notebooks – Rule et al. 2019, PLoS Computational Biology\nMiscellaneous\nStack Overflow: “The Incredible Growth of Python” (2017)\nStatista: “Python Remains Most Popular Programming Language” (2020)\nThe Economist: “Python is becoming the world’s most popular coding language” (2018)\nCoding infrastructure\nVS Code\nVS Code documentation on its “Python Interactive Window”.\nVS Code documentation on its Jupyter Notebook implementation.\nJupyter Notebooks / JupyterLab\nYou can also use the Jupyter Notebooks / JupyterLab as an Interactive App at OSC OnDemand. If you’re interested in using this, I would recommend trying JupyterLab which can run Jupyter Notebooks but also regular Python scripts, a shell, and so forth.\n\nTo do so at OSC OnDemand, click on Interactive Apps (top blue bar) and then Jupyter (Owens and Pitzer) near the bottom, and check the box Use JupyterLab instead of Jupyter Notebook?.\n\n6-minute video: How to use JupyterLab.\nThis JupyterLab documentation provides a nice introduction to JupyterLab features (the link goes to documentation for a version close to the one at OSC).\nFor a general introduction to Jupyter Notebooks, see also How to Use Jupyter Notebook in 2020: A Beginner’s Tutorial.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:11-04:00"
    },
    {
      "path": "w08_readings.html",
      "title": "Week 8 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis is the first of 5 consecutive weeks focused entirely on Python, which will hopefully get you a solid introduction to this very useful language. We will start by working our way through the first part of CSB Chapter 3 (3.1-3.4) to get acquainted with Python basics.\nSome of the things you will learn this week:\nUnderstand why we will be learning Python in this course.\nLearn how to use Python in VS Code.\nLearn the basics of Python syntax.\nLearn how to work with different variable types: strings, integers, floats, and Booleans.\nLearn about the four basic data structures in Python: lists, dictionaries, tuples, and sets.\nReadings\nAssuming that you are completely new to Python, I recommend that you at least quickly read CSB 3.1-3.4 before class and that you also go through it once more after class, preferably interactively with an open Python prompt. Before we move on, it will be important to get some Python fundamentals under your fingers.\nRequired readings\nCSB Chapter 3.1 - 3.4: “Basic Programming”\nFurther Resources\nSee the Python resources page on this site for a list of general resources for learning Python!\nThe Economist, 2018: “Python is becoming the world’s most popular coding language”\nStack Overflow, 2017: “The Incredible Growth of Python”\n\n\n\n",
      "last_modified": "2021-04-25T17:52:12-04:00"
    },
    {
      "path": "w09_exercises.html",
      "title": "Exercises: Week 9",
      "author": [],
      "contents": "\n\nContents\nSetup\nExercise 1: Dictionaries\nExercise 2: Sets\nIntro to CSB exercises\nExercise CSB-1: Measles time series\nBonus: Exercise CSB-2: Red queen in fruit flies\nCSB Solutions\n\n\n\n\n\nSetup\nOpen a new file and save it as week09_exercises.py or something along those lines.\nType your commands in the script and send them to the prompt in the Python interactive window by pressing Shift+Enter.\nProblems with the keyboard shortcut?\n\nIf this doesn’t work, check your keyboard shortcut by right-clicking in the script and looking for “Run Selection/Line In Python Interactive Window”.\nAlso, you can open the Command Palette (Ctrl+Shift+P) and look for that shortcut there, and change it if you want.\n\nBecause these first two exercises have many small steps, I put the solutions right below the question, so you don’t have to scroll back-and-forth all the time. However, make sure you actually try to do the exercises!\nExercise 1: Dictionaries\nCreate and print a dictionary called yield_current with the following items:\n{\"plotA_1\": 12, \"plotA_2\": 18, \"plotA_3\": 2,\n \"plotB_1\": 33, \"plotB_2\": 28, \"plotB_3\": 57}\nSolution\n\nyield_current = {\"plotA_1\": 12, \"plotA_2\": 18, \"plotA_3\": 2,\n                 \"plotB_1\": 33, \"plotB_2\": 28, \"plotB_3\": 57}\n\nyield_current                 \n{'plotA_1': 12, 'plotA_2': 18, 'plotA_3': 2, 'plotB_1': 33, 'plotB_2': 28, 'plotB_3': 57}\n\n\nPrint just the value for key plotA_3.\nSolution\n\nWe can get the value for a specific key using the <dict>[<key>] notation:\n\nyield_current[\"plotA_3\"]\n2\n\n\nUpdate the value for key plotB_2 to be 31 and check whether this worked.\nSolution\n\nWe can simply assign a new value using =:\n\nyield_current[\"plotB_2\"] = 31\nyield_current[\"plotB_2\"]\n31\n\n\nCount the number of items (i.e. entries, key-value pairs) in your dictionary.\nHints\n\nUse the len() function.\nSolution\n\nlen(yield_current)\n6\n\n\nBonus: Create a dictionary obs_20210305 with keys plotA_3 and plotC_1, and values 18 and 3, respectively. Then, update the yield_current dictionary with the obs_20210305 dictionary, and check whether this worked.\nSolution\n\nobs_20210305 = {\"plotA_3\": 18, \"plotC_1\": 3}\n\nWe use the update() method as follows:\n\nyield_current.update(obs_20210305)\n\nyield_current\n{'plotA_1': 12, 'plotA_2': 18, 'plotA_3': 18, 'plotB_1': 33, 'plotB_2': 31, 'plotB_3': 57, 'plotC_1': 3}\n\nNow, our dictionary has an updated value for key “plotA_3”, and an entirely new item with key “plotC_1”.\n\nBonus: Get and count the number of unique values in your dictionary.\nHints\n\nExtract the values with the values() method. Next, turn these values into a set to get the unique values. Finally, count the unique values with the len() function.\nSolution\n\nlen(set(yield_current.values()))\n6\n\n\n\nExercise 2: Sets\nAssign a set named dna with 4 items: each of the 4 bases (single-letter abbreviations) in DNA.\nHints\n\nRecall the use of curly braces to assign a set.\nThe order of the bases doesn’t matter, because sets are unordered.\n\nSolution\n\ndna = {'A', 'G', 'C', 'T'}\n\n\nAssign a set named rna with 4 items: each of the 4 bases (single-letter abbreviations) in RNA.\nSolution\n\nrna = {'A', 'G', 'C', 'U'}\n\n\nFind the 3 bases that are shared between DNA and in RNA (try both with an operator and a method, if you want).\nSolution\n\ndna & rna\n{'A', 'C', 'G'}\n\nOr:\n\ndna.intersection(rna)\n{'A', 'C', 'G'}\n\n\nFind all 5 bases that are collectively found among DNA and RNA.\n\nSolution\n\ndna | rna\n{'T', 'G', 'A', 'U', 'C'}\n\nOr:\n\ndna.union(rna)\n{'T', 'G', 'A', 'U', 'C'}\n\n\nFind the base that only occurs in DNA.\nSolution\n\ndna - rna\n{'T'}\n\nOr:\n\ndna.difference(rna)\n{'T'}\n\n\nAssign a set named purines with the two purine bases and a set named pyrimidines with the three pyrimidine bases.\nSolution\n\npurines = {'A', 'G'}\npyrimidines = {'C', 'T', 'U'}\n\n\nFind the pyrimidine that occurs both in RNA and DNA.\nSolution\n\nYou can combine more than two sets either by chaining methods or adding another operator.\nSolution\n\npyrimidines & dna & rna\n{'C'}\n\nOr:\n\npyrimidines.intersection(dna).intersection(rna)\n{'C'}\n\n\nBonus: Find the pyrimidine that occurs in RNA but not DNA.\nSolution\n\n(rna - dna) & pyrimidines\n{'U'}\n\nOr:\n\nrna - dna & pyrimidines\n{'U'}\n\nOr:\n\nrna.difference(dna).intersection(pyrimidines)\n{'U'}\n\n\n\nIntro to CSB exercises\n Edit March 12: We did not get to the section on reading tabular files with the csv module in class. Please read CSB 3.7.2 (p. 115-116) before attempting to do these exercises. \nFrom the CSB Chapter 3 preface to the exercises:\n\nHere are some practical tips on how to approach the Python exercises (or any programming task):\nThink through the problem before starting to write code: Which data structure would be more convenient to use (e.g., sets, dictionaries, lists)?\nBreak the task down into small steps (e.g., read file input, create and fill data structure, output).\nFor each step, describe in plain English what you are trying to do— leave these notes as comments within your program to document your code.\nWhen working with large files, initially use only a small subset of the data; once you have tested your code thoroughly you can run it on the whole data set.\nConsider using specific modules (e.g., use the csv module to parse each line into a dictionary or a list).\nSkim through appropriate sections above to refresh your memory on data-type-specific methods.\nUse the documentation and help forums.\n\n\nExercise CSB-1: Measles time series\nIn their article, Dalziel et al. (2016) provide a long time series reporting the numbers of cases of measles before mass vaccination, for many US cities. The data consist of cases in a given US city for a given year, and a given “biweek” of the year (i.e., first two weeks, second two weeks, etc.). The time series is contained in the file Dalziel2016_data.csv.\nWrite a program that extracts the names of all the cities in the database (one entry per city).\nHints\nWhile you could try to parse the file from scratch (you have learnt the building blocks to do so), using the DictReader from the csv module, as we did in class, will make this easier.\nThe city name is in the column loc.\nBecause each city is reported multiple times, the main task here is to remove duplicates. Using a set will be the easiest way to do so, since sets cannot contain duplicates.\nYou don’t need to write to a new file here, just print the set after you are done processing the file.\nPseudocode:\nimport csv\ncities = an empty set\nopen data for reading\ncreate dictionary reader\nfor each row in the file\n    add the city to the set\n\nWrite a program that creates a dictionary where the keys are the cities and the values are the number of records (rows) for that city in the data.\nHints\nInitialize an empty dictionary before you start looping over the lines.\nFor every line, extract the city name and add 1 to the value for that city in your dictionary, since you are counting rows.\nYou don’t need to prepopulate the dictionary with all cities: when you provide a default value with the get() method, a key that is not yet present will be added to the dictionary with said default value.\nFor example, we can build up a dictionary using get() like so:\n\ndd = {} # empty dictionary\nmy_list = ['a', 'b', 'a', 'c', 'd', 'b', 'a']\nfor element in my_list:\n    dd[element] = dd.get(element, 0) + 1\n\nprint(dd)\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\nPseudocode:\nimport csv library\ncitycount = an empty dictionary\nopen file for reading\nset up dictionary reader\n  for each line in data\n      my_city = extract the city\n      citycount[my_city] = use get to update value\n\nWrite a program that calculates the mean population for each city, obtained by averaging the values of pop.\nHints\nNote that for some reason, the population sizes have decimal values.\nAgain, use a dictionary that you keep adding to for each row of the data set. This time, though, each value in the dictionary should be a list of two items: the total population, and the number of occurences.\nIn your get() call, you can initialize the values to be a list of two items as follows (here assuming the dictionary is called citypop and the city’s name has been extracted as mycity):\ncitypop[mycity] = citypop.get(mycity, [0, 0])\nThen, you can refer to each item in the dictionary’s values by chaining indices, e.g. citypop[mycity][0].\nPseudocode:\nimport csv\ncitypop = an empty dictionary\nopen data file reading\nset up dictionary reader\nfor each line in data\n  my_city = extract the city\n  my_pop = extract population\n  if this is the first time you see this city, initialize:\n      citypop[my_city] = [0.0, 0]\n  citypop[my_city][0] = what it was before + my_pop\n  citypop[my_city][1] = what it was before + 1\n\nfor each city\n  divide the first element by the second to obtain the mean\n\nWrite a program that calculates the mean population for each city and year.\nHints\nYou can do this in (at least) two ways with a dictionary:\nBy creating a nested dictionary: each city is a dictionary, which itself contains a dictionary for each year.\nBy using a (city, year) tuple as the keys for the dictionary.\nNote that the worked-out solution in the link below uses the first strategy.\n\n\nBonus: Exercise CSB-2: Red queen in fruit flies\nSingh et al. (2015) show that, when infected with a parasite, the four genetic lines of D. melanogaster respond by increasing the production of recombinant offspring (arguably, trying to produce new recombinants able to escape the parasite). They show that the same outcome is not achieved by artificially wounding the flies. The data needed to replicate the main claim (figure 2 of the original article) is contained in the file Singh2015_data.csv.\nOpen the file, and compute the mean RecombinantFraction for each Drosophila genetic line, and InfectionStatus (W for wounded and I for infected).\nPrint the results in the following form:\nLine 45 Average Recombination Rate:\nW : 0.187\nI : 0.191\n\nHints\nFor each Dropsophila genetic line, you need to keep track of all the recombination rates for W (wounded) and I (infected).\nFor example, you could build a dictionary of dictionaries in which the first (outer) dictionary has a key for each line, and the inner dictionary has a key for each status (W or I) and a list of recombination rates as each value.\nThen, you would calculate averages for each list at the end.\n\nCSB Solutions\nSolutions for exercise CSB-1.\nSolutions for exercise CSB-2.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:12-04:00"
    },
    {
      "path": "w09_readings.html",
      "title": "Week 9 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, you will learn about control flow and working with files in Python.\nSome of the things you will learn this week:\nLearn when and how to use if statements, for loops, and while loops in Python.\nLearn how you can read from and write to files in Python.\nReadings\nRequired readings\nCSB Chapter 3.5 - 3.9: “Basic Programming”\nNote: Where the CSB text says to save a file as a Jupyter Notebook (with the .ipynb extension), you can just create a regular Python script (.py extension) – nothing in the code is specific to Jupyter Notebooks.\nIf you want to play around with Jupyter Notebooks, you can actually do this in VS Code: simply save a file with the .ipynb extension and see what happens. Here is some VS Code documentation on its Jupyter support. If you would want to try Jupyter Notebooks in their native environment, you can do so via OSC OnDemand: see this section of this site’s Python resources page.\n\n",
      "last_modified": "2021-04-25T17:52:13-04:00"
    },
    {
      "path": "w10_exercises.html",
      "title": "Exercises for week 10: Writing good code",
      "author": [],
      "contents": "\n\nContents\nExercise 1: More lists\nExercise CSB-1: Assortative mating\nBonus: Exercise CSB-2: Human intestinal ecosystems\n\n\n\n\n\nExercise 1: More lists\nStart with a same list as you created in the exercises for week 8:\n\ndiseases = ['fruit_rot', 'leaf_blight', 'leaf_spots', 'stem_blight',\n            'canker', 'wilt', 'root_knot', 'root_rot']\n\nSort diseases in place.\nSolution\n\nThe sort() method sorts a list in place:\n\ndiseases.sort()\n\ndiseases\n['canker', 'fruit_rot', 'leaf_blight', 'leaf_spots', 'root_knot', 'root_rot', 'stem_blight', 'wilt']\n\n\nInstead of sorting in place with the sort() method like in the previous step, you can also use the sorted() function, which will not sort in place but return a new, sorted list.\nFind out how to use sorted() to sort in reverse order, and apply this to diseases to create a new list diseases_sorted.\nSolution\n\nWe can use the reverse argument to sorted() to sort in reverse order:\n\ndiseases_sorted = sorted(diseases, reverse=True)\n\n\nIf you would run fewer_diseases = diseases.remove(\"root_rot\"), what would fewer_diseases contain? Think about what the answer should be, and then check if you were right. Does simply running fewer_diseases versus running print(fewer_diseases) make a difference?\nSolution\nBecause remove() operates in place, it doesn’t return anything:\n\nfewer_diseases = diseases.remove(\"root_rot\")\n\nfewer_diseases\n\nWell, it actualy returns None, which you can see by explicitly calling the print() function:\n\nprint(fewer_diseases)\nNone\n\n\nIf you would run:\n\nmore_diseases = diseases\n\nmore_diseases.append(\"crown_galls\")\n\nWould the list diseases also contain crown_galls? Think about what the answer should be, and then check if you were right.\nSolution\n\nYes, diseases will contain the item crown_galls that was added to more_diseases, because more_diseases in not an independent list but is merely a second pointer to the same list that diseases points to.\n\nmore_diseases = diseases\n\nmore_diseases.append(\"crown_galls\")\n\ndiseases\n['canker', 'fruit_rot', 'leaf_blight', 'leaf_spots', 'root_knot', 'stem_blight', 'wilt', 'crown_galls']\n\n\nCopy diseases to a new list with a name of your choice – the new list should not simply be a pointed to the old one, but a different object in memory. Then, remove all items from the new list. Check if diseases still contains its items – if not, you’ll have to try again!\nHints\n\nTo create a new list, use the copy() method or the [:] notation.\nSolution\n\nTo create a copy, use the copy() method:\n\ndiseases_copy = diseases.copy()\n\nOr the [:] notation.\n\ndiseases_copy = diseases[:]\n\nThen, to remove all elements in the copy of the list:\n\ndiseases_copy.clear()\n\ndiseases\n['canker', 'fruit_rot', 'leaf_blight', 'leaf_spots', 'root_knot', 'stem_blight', 'wilt', 'crown_galls']\n\n\nWhat fundamental difference between lists and strings makes it so that newstring = oldstring creates a new string, whereas newlist = oldlist simply creates a new pointer to the same list?\nSolution\n\nThe fact that strings are immutable, whereas lists are mutable.\n\nBonus: Get all unique characters (not items) present in diseases.\nHints\n\nRemember how we can turn a list into a string with join()? If you specify \"\" as the separator, it will simply concatenate all the items in the list.\nNext, note that applying set() to a string will extract the unique characters.\nSolution\n\nFirst, turn the list into a string using \"\".join. Then, call set() on the string to get a list of unique items (= characters).\n\nset(\"\".join(diseases))\n{'b', 'l', 'p', 'a', 's', 'r', 't', 'c', 'm', 'k', 'g', 'o', 'h', '_', 'e', 'i', 'w', 'u', 'n', 'f'}\n\n\n\nExercise CSB-1: Assortative mating\nJiang et al. (2013) studied assortative mating in animals. They compiled a large database, reporting the results of many experiments on mating. In particular, for several taxa they provide the value of correlation among the sizes of the mates. A positive value of r stands for assortative mating (large animals tend to mate with large animals), and a negative value for disassortative mating.\nYou can find the data in CSB/good_code/data/Jiang2013_data.csv1. Write a function that takes as input the desired Taxon and returns the mean value of r. Then, apply that function to all taxa in the file.\nHints\nHave a look at the file in the shell before you start.\nTo parse the file, DictReader from the csv module is again a good option, but note that you’ll have the specify the delimiter argument since the file is tab-separated.\nThere are several ways of going about here, but the one in the solutions is to first read in the file and create two lists: one with taxa names and one with the corresponding values for r: (These lists will also be re-used in parts 2 and 3 of the exercise, so it is recommended to follow this approach.)\nimport csv \ntaxa = []\nr_values = []\n\nopen the file and set up dictionary reader\nfor each row:\n    append to taxa\n    append to r_values\nThen, the actual function will take these two lists and a taxon name as input.\ndef compute_avg_r(taxa, r_values, target_taxon = \"Fish\"):\navg_taxon = 0.0\nnum_occurrences = 0\ncycle through the values of taxa\n    every time you find the right taxon, add its r value to avg_taxon\n    and increment num_occurrences\nat the end, divide avg_taxon by num_occurrences and return the average\nTo apply the function to all taxa, use a set to get the unique taxa, and loop over the taxa.\n\nYou should have seen that fish have a positive value of r, but that this is also true for other taxa. Is the mean value of r especially high for fish? To test this, compute a p-value by repeatedly sampling 37 values of r (37 experiments on fish are reported in the database) at random, and calculating the probability of observing a higher mean value of r. To get an accurate estimate of the p-value, use 50,000 randomizations.\nHints\nIn part 3 of this exercise, you will repeat this procedure for other taxa, so it will be a good idea to create a function here.\nYour function should take as input the target taxon name, the lists with taxa and r-values that you created in the first step, and the number of repetitions (randomizations).\nIn the function:\nFirst compute the mean r value for the target taxon.\nThen, iterate over the output of range() to repeat the randomizations.\nIn every iteration, shuffle either the list with r values or the taxon names, using the function scipy.random.shuffle() (don’t forget to import scipy).\nThen, you can simply call the same function you used before to calculate the mean value of r.\nIn every iteration, compare the randomized mean with the observed mean, and keep a tally of the number of times the observed mean is higher.\nThe p-value will simply be the proportion (~= probability) of times you got a higher mean value of r among randomized sets of 37.\nPseudocode:\ndef compute_pvalue(taxa, r_values, target_taxon = \"Fish\", num_rep = 1000):\n    observed_r = compute the mean for the observed average r value\n    count_random_is_higher = 0.0\n    for i in range(num_rep):\n        shuffle the r values\n        random_r = compute the mean using the shuffled values\n        if random_r >= observed_r:\n           increment count_random_is_higher\n    now divide count_random_is_higher by num_rep (= the p-value) and return\n\nRepeat the procedure for all taxa.\nHints\nLoop over all taxa, and call the function you created in the previous part of the exercise in every iteration.\n\n\nSolutions for all steps\nSee this Jupyter Notebook by the authors of the CSB book.\n\nBonus: Exercise CSB-2: Human intestinal ecosystems\nLahti et al. (2014) studied the microbial communities living in the intestines of 1,000 human individuals. They found that bacterial strains tend to be either absent or abundant, and posit that this would reflect bistability in these bacterial assemblages.\nThe data used in this study are contained in the directory CSB/good_code/data/Lahti20142. The directory contains:\nThe file HITChip.tab containing estimates of microbial abundance for each sample, as obtained by HITChip signal.\nThe file Metadata.tab, providing metadata about each of the 1,006 human records.\nREADME, a description of the data by the study’s authors.\nWrite a function that takes as input a dictionary of constraints (i.e., selecting a specific group of records) and returns a dictionary tabulating the values for the column BMI_group for all records matching the constraints.\nFor example, calling:\nget_BMI_count({\"Age\": \"28\", \"Sex\": \"female\"})\nshould return:\n{'NA': 3, 'lean': 8, 'overweight': 2, 'underweight': 1}\nHints\nSeveral strategies are again possible, but the CSB solution linked to below creates a single function will all code, including reading in the file.\nOnce again, DictReader() from the csv module is a useful way to read in the data.\nLoop over the lines (rows) and for each row, you’ll want to check whether all conditions are satisfied.\nYour output dictionary will basically be a count table of the values found for the BMI_group, column, so for each matching row, add 1 to the value for the key that represents the group (lean, etc).\nPseudocode:\ndef get_BMI_count(dict_constr):\n    open the file and set up the csv reader\n   for each row:\n        add_to_count = True\n        for each constrain in dict_constr:\n              if constraint is not met:\n                  add_to_count = False\n        if add_to_count:\n              all the constraints are respected\n              add to the tally\n   return the result\n\nWrite a function that takes as input the constraints (as above) and a bacterial “genus.” The function returns the average abundance (in logarithm base 10) of the genus for each BMI group in the subpopulation.\nFor example, calling:\nget_abundance_by_BMI({\"Time\": \"0\",\n                      \"Nationality\": \"US\"},\n                      \"Clostridium difficile et rel.\")\nshould return:\n------------------------------------------------\nAbundance of Clostridium difficile et rel.\nIn subpopulation:\n------------------------------------------------\nNationality -> US\nTime -> 0\n------------------------------------------------\n3.08\nNA\n3.31\nunderweight\n3.84\nlean\n2.89\noverweight\n3.31\nobese\n3.45\nsevereobese\n------------------------------------------------\nHints\nTo write the function, you need to:\nOpen the file Metadata.tab, and extract the SampleID corresponding to the constraints specified by the user (you can use a list to keep track of all IDs).\nOpen the file HITChip.tab to extract the abundances matching the genus specified by the user (and for the ID stored in step 1).\nTo calculate the log value, you can use the log10 function from the scipy module (though you may get a deprecation warning; this is now supposed to be called from the numpy module, but we haven’t installed that yet.)\nPseudocode:\ndef get_abundance_by_BMI(dict_constraints, genus = 'Aerococcus'):\n    open the file Metadata.tab extract matching IDs using the same \n    approach as in exercise 1\n    these IDs are stored in BMI_IDs\n\n    Now open HITChip.tab, and keep track of the abundance\n    of the genus for each BMI group\n    Calculate means, and print results\n\nRepeat this analysis for all genera, and for the records having Time = 0.\nHints\nThe genera are contained in the header of the file HITChip.tab. Extract them from the file and store them in a list.\nThen, you can call the function get_abundance_by_BMI({'Time': '0'}, g), where g is the genus; cycle through all genera.\n\n\nSolutions for all steps\nSee this Jupyter Notebook by the authors of the CSB book.\n\n\nIf necessary, download the CSB repository again using git clone https://github.com/CSB-book/CSB.git↩︎\nIf necessary, download the CSB repository again using git clone https://github.com/CSB-book/CSB.git↩︎\n",
      "last_modified": "2021-04-25T17:52:13-04:00"
    },
    {
      "path": "w10_readings.html",
      "title": "Week 10 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, we wil talk about several strategies for writing “good” code: code that is clear, modular, flexible, and easy to troubleshoot.\nSome of the things you will learn this week:\nThe basics of Python’s module and package system, and how to import modules into your script or session.\nWhy you should write your own functions, and learn how to write them in Python.\nHow to properly structure your code.\nHow to run Python scripts from the shell.\nHow to interpret and handle errors.\nBetter understand the difference between mutable and immutable Python data structures and the need to explicitly copy mutable data structures.\nAnd optionally:\nPython coding style guidelines for clear code.\nHow to debug Python code with a debugger.\nWhat unit testing and profiling is, and how to perform some basic implementations of this in Python.\nReadings\nRequired readings\nCSB Chapter 4: “Writing Good Code”\nSection 4.6-4.8 on debugging, unit testing, and profiling, respectively, are optional.\nBut don’t forget the last section, 4.9 (“Beyond the Basics”)!\n\n",
      "last_modified": "2021-04-25T17:52:14-04:00"
    },
    {
      "path": "w11_exercises.html",
      "title": "Exercises for week 11: Scientific computing",
      "author": [],
      "contents": "\n\nContents\n1: Tutorial exercise  Querying PubMed (CSB 6.4.4)\n2: Exercise CSB-1:  Lord of the fruit flies\n3: Exercise CSB-2: Rejection rates\n4: Tutorial exercise  Image processing with NumPy\n\n\n\n\n\nThe first and last of these exercises are more short tutorials working through two of the examples in the book.\nThe first two exercises use BioPython, the third uses Pandas, and the last uses NumPy. As we’re getting into more advanced and specialized material, you may be much more interested in some approaches/packages than others, and it is fine if you direct your attention accordingly.\nFor example, if you are interested in one of these three but not so much the other two, feel free to skip the other exercises. And if you have time to spare, I would rather recommend digging a bit deeper into your topic of interest:\nFor more BioPython, see this BioPython workshop (and for much more, the BioPython tutorial).\nFor more NumPy, see the official NumPy tutorial.\nFor more Pandas, see this overview of tutorials.\n\n1: Tutorial exercise  Querying PubMed (CSB 6.4.4)\nUsing the Entrez.esearch() function we used in class, we can search any NCBI database, including PubMed, which is a comprehensive scientific literature database.\nBy way of example, we will search for any papers on Drosophila that mention the gene “spaetzle” anywhere in the title or abstract.1\nAs always, we start by importing the Entrez module and providing our email address:\nfrom Bio import Entrez\nEntrez.email = \"me.999@osu.edu\"  # Replace with your actual email address!\nNow, we can run the search. We will match “spaetzle” only in the title or abstract, and we will match Drosophila anywhere ([ALL]):\nhandle = Entrez.esearch(db = \"pubmed\",\n                        term = (\"spaetzle[Title/Abstract] AND Drosophila[ALL]\"),\n                        usehistory = \"y\")\nrecord = Entrez.read(handle)\nhandle.close()                        \nNote that these search keywords are NCBI’s. For a list of all of them go NCBI’s PubMed help page and search for “Search Field descriptions and tags”.\nWe used usehistory = \"y\", which will allows us to refer back to our search to fetch the titles and abstracts by saving WebEnv and QueryKey:\nwebenv = record[\"WebEnv\"]\nquery_key = record[\"QueryKey\"]\nHow many hits did we get?\nrecord[\"Count\"]\n#> '15'\nWe found 15 records (up from 13 in the book) that contained the words “spaetzle” and “Drosophila”.\nWe can now fetch the titles and abstracts:\nhandle = Entrez.efetch(db = \"pubmed\",\n                         rettype = \"medline\", retmode = \"text\",\n                         webenv = webenv, query_key = query_key)\ndata = handle.read()\nhandle.close() \nFinally, we write the results to file:\nout_handle = open(\"Spaetzle_abstracts.txt\", \"w\")\nout_handle.write(data)\nout_handle.close()\nLet’s have a look at the results – here, I am using ! to execite shell commands from within Python (alternatively, you can open a shell):\n!cat Spaetzle_abstracts.txt\n\n#> [Output not shown.]\nWith a simple grep command, we can select just those lines that contain the word “Spaetzle”. We’ll use -C 1 to also see 1 line before and 1 lines after each match, to get a bit more context:\n!grep -i \"spaetzle\" -C 1 Spaetzle_abstracts.txt\n\n#>     leading to ventrally-restricted expression of the sulfotransferase Pipe. These\n#>    events promote the ventral processing of Spaetzle, a ligand for Toll, which\n#>    ultimately sets up the embryonic dorsal-ventral axis. We then describe the\n#> --\n#> DP  - 2019 Nov 12\n#> TI  - Dynamics of Spaetzle morphogen shuttling in the Drosophila embryo shapes\n#>    gastrulation patterning.\n#> --\n#>    The dynamics indicate that a sharp extracellular gradient is formed through\n#>    diffusion-based shuttling of the Spaetzle (Spz) morphogen that progresses through\n#>    several nuclear divisions. Perturbed shuttling in wntD mutant embryos results in\n\n#> [And so on, first three matches shown.]\n\nBonus using regular expressions in Python\nAlternatively, we could use regular expressions in Python to nicely retrieve each sentence (rather than each line) that contains “Spaetzle”, and also print the PubMedID (“PMID”) for the publication. You will learn how to use regular expressions like this in Python next week!\nimport re   # re is Python'r regular expression module\n\nwith open(\"Spaetzle_abstracts.txt\") as datafile:\n\npubmed_input = datafile.read()\n\n# To get titles and abstracts on one line: delete newlines + 6 spaces:\npubmed_input = re.sub(r\"\\n\\s{6}\", \" \", pubmed_input)\n\nfor line in pubmed_input.split(\"\\n\"):\n\n    # re.match()'s output will be interpreted as True only if a match is found:\n    # (Here we just use a literal search from \"PMID\")\n    if re.match(\"PMID\", line):\n\n         # We find and extract the PubMed ID by matching one or more digits;\n         # group() will return the match:\n         PMID = re.search(r\"\\d+\", line).group()\n\n         if re.match(\"AB\", line):\n\n              # We look for *all* sententces that contain \"Spaetzle\" using findall():\n              if re.findall(r\"([^.]*?Spaetzle[^.]*\\.)\", line):\n                    print(\"PubMedID: \", PMID, \" \", spaetzle)\n\n#> PubMedID:  32591083   [' These events promote the ventral processing of Spaetzle, a ligand for Toll,   which ultimately sets up the embryonic dorsal-ventral axis.']\n#> PubMedID:  31719046   ['  The dynamics indicate that a sharp extracellular gradient is formed through   diffusion-based shuttling of the Spaetzle (Spz) morphogen that progresses through several nuclear   divisions.']\n#> PubMedID:  27314646   [' While cytokines activating immune responses,  such as Spaetzle or Unpaired-3,   have been identified and\n\nWhile this PubMed search was relatively trivial, you may need to do this kind of search for a dozen or more genes, and possibly repeat the search periodically. In that case, using Python to do the searches and parse the results can be a huge time-saver!\n2: Exercise CSB-1:  Lord of the fruit flies\n(You will need to have run through the previous tutorial exercise to be able to do this one.)\nSuppose you need information on how to breed Drosophila virilis in your laboratory and you would like to contact an expert. Conduct a PubMed query on who has published most contributions on D. virilis. This person might be a good researcher to contact.\nIdentify how many papers in the PubMed database have the words Drosophila virilis in their title or abstract.\n\nHints\n\nUse the Entrez.esearch() function and search the “pubmed” database, and then save the results from the search handle in an object called record.\nInclude usehistory=y in your Entrez.esearch() call, and then assign the “WebEnv” and “QueryKey” to variables. If you saved you results in the variable record, you could get these using record[\"WebEnv\"] and record[\"QueryKey\"], respectively. Then, you can refer to this search in the next step.\nUse [Title/Abstract] after the search term to search only in the title and abstract.\nPseudocode:\nimport the Entrez module\nEntrez.email = \"your email here\"\n\n# Create a handle:\nhandle = Entrez.esearch(your code here)\n\n# Retrieve the records\nrecord = Entrez.read(handle)\nclose the handle\n\nsave WebEnv from your record\nsave QueryKey from your records\n\nSolutions\n\n# Import the Entrez module:\nfrom Bio import Entrez\n\n# Tell NCBI who you are:\nEntrez.email = \"me.1@osu.edu\" # EDIT TO YOUR ADDRESS\n\n# Perform the search:\nhandle = Entrez.esearch(db = \"pubmed\",\n                        term = \"Drosophila virilis[Title/Abstract]\",\n                        usehistory = \"y\")\n# Save the results from the search handle in \"record\" and close the handle:                 \nrecord = Entrez.read(handle)\nhandle.close()\n\n# Print the number of matching papers:\nprint(record[\"Count\"])\n\n# Save the WebEnv and QueryKey for later use:\nwebenv = record[\"WebEnv\"]\nquery_key = record[\"QueryKey\"]\nFor the similar CSB solutions to all steps in this exercise, see here.\n\nRetrieve the PubMed entries that were identified in step 1, and write them to a new file called D_virilis_pubs.txt.\n\nHints\n\nUse the Entrez.efetch() function to retrieve the records. Use similar options to those we used in class, but set retmax to be at least as high as the number of papers that you found in the previous step, or you will not get all papers.\nRead the results from your search handle to an object, and then write your object to the file D_virilis_pubs.txt.\nPseudocode:\nhandle = fetch records from \"pubmed\" db with \"medline\" rettype, \"text\" retmode,\n              600 retmax, and the saved webenv and query_key\ndata = read from handle\nclose the handle\n\nwith open output file as handle\n    write the data\n\nSolutions\n\n# Perform the search:\nshandle = Entrez.efetch(db = \"pubmed\",\n                        rettype = \"medline\",\n                        retmode = \"text\",\n                        retstart = 0, retmax = 600,\n                        webenv = webenv, query_key = query_key)\n\n# Save results from the search handle in \"data\" and close the handle:\ndata = shandle.read()\nshandle.close(\nwith open(\"D_virilis_pubs.txt\", \"w\") as fhandle:\n    fhandle.write(data)\n\nCount the number of contributions per author.\nStart by taking a look at your D_virilis_pubs.txt file to see how you can match lines that contain author names.\n\nHints\n\nCreate an empty dictionary for authors and their publication counts, and then loop through each line in D_virilis_pubs.txt to fill in the dictionary.\nTo find lines with authors, use the find() methods and search for AU or even better AU  -, since these are the lines with authors.\nYou’ll want to only process lines for which you found AU  - as above. Because find() returns -1 when nothing if found, use an if statement to test what the output of find() is.\nPseudocode:\nwith open D_virilis_pubs.txt\n     initialize empty dict\n     for line in fhandle\n          if line contains \"AU -\"\n              split the line by \"-\" and take the 2nd element\n              strip whitespace\n              initialize author if not in dict and add 1 to keep count\n\nSolutions\n\nwith open(\"D_virilis_pubs.txt\") as fhandle:\n\n# Initialize an empty dictionary:\nauthor_dict = {}\n\n# Loop through each line:\n      for line in fhandle:\n\n      # Search for lines with \"AU  -\", which contain authors:\n      if line.find(\"AU  -\") != -1:\n\n      # The author name is after the \"-\",\n      # so we split by \"-\" and take the 2nd element:\n      author = line.split(\"-\", 1)[1]\n\n      # Then, we remove any leading and trailing whitespace:\n      author = author.strip()\n\n      # If key (=author) is present, add 1, otherwise, initialize at 1:\n      author_dict[author] = author_dict.get(author, 0) + 1\n\nFor the the five authors with the most papers on D. virilis, print each name and the corresponding number of papers.\nYou’ll have to use the sorted() in a way we have not seen yet, see the Hints below for more details.\n\nHints\n\nTo get the top authors, you can use the sorted() function on your dictionary. You’ll have two provide two additional arguments:\nkey = author_dict.get, which will sort by the values of the dictionary.\nreverse = TRUE, so you get the highest number of papers first.\nThen, take the first 5 items in the resulting list, which will be the top 5 authors.\nFinally, loop through your list of top authors and print their names (keys) and number of papers (values).\nPseudocode:\nsorted_authors = sort dict by value in reverse order\ntop_authors = take top 5 from sorted_authors\nfor author in top authors\n    print the name and the number of papers\n\nSolutions\n\nsorted_authors = sorted(author_dict, key = author_dict.get, reverse = True)\ntop_authors = sorted_authors[:5]\n\nfor author in top_authors:\n    print(author, \":\", author_dict[author])\n\n#> Gruntenko NE : 36\n#> Evgen'ev MB : 31\n#> Hoikkala A : 24\n#> Raushenbakh IIu : 24\n#> Korochkin LI : 22\n\n3: Exercise CSB-2: Rejection rates\nFox et al. (2016) studied the effects on the outcome of papers of the genders of the handling editors and reviewers. For the study, they compiled a database including all the submissions to the journal Functional Ecology from 2004 to 2014. Their data are reported in CSB/scientific/data/Fox2015_data.csv.2\nBesides the effects of gender and bias in journals, the data can be used to investigate whether manuscripts having more reviewers are more likely to be rejected. Note that this hypothesis should be tested for reviewed manuscripts, that is, excluding “desk rejections” without review.\nImport the data using Pandas, and count the number of reviewers (by summing ReviewerAgreed) for each manuscript (i.e., unique MsID). The column FinalDecision contains 1 for rejection, and 0 for acceptance.\nCompile a table measuring the probability of rejection given the number of reviewers. Does the probability of being rejected seem to increase with the number of reviewers?\n\nHints\n\nWith an eye on the next step, where you have to do the same thing for each year, it is convenient to write a function that takes the data and a calendar year as input, and prints the probability of rejection given the number of reviewers for that given year.\nWe can set the function to return the general rejection rate if “all” instead of a year is specified as an argument.\nPseudocode:\nimport pandas\nimport numpy as np\n\n# read the data using pandas\n# (assuming you are in 'CSB/scientific/sandbox')\nfox = pandas.read_csv(\"../data/Fox2015_data.csv\")\n\nuse a combination of list and set() to extract the unique `MsID`\n\nnow go through each manuscript and store:\ni) the final decision (reject/accept) in the np.array final_decision\nii) the number of reviewers in the np.array num_reviewers\niii) the submission year in the np.array year\n\ndef get_prob_rejection(my_year = \"all\"):\n    if my_year == \"all\":\n        do not subset the data\n    else:\n        subset the data to use only the specified year\n    for each number of reviewers:\n        compute probability of rejection and produce output\n\nWrite a function to repeat the analysis above for each year represented in the database.\nFor example, for the year 2009, your function should return:\nYear: 2009\nSubmissions: 626\nOverall rejection rate: 0.827\nNumRev    NumMs   rejection rate\n0   306   0.977\n1   2     0.5\n2   228   0.68\n3   86    0.698\n4   4     0.75\n\nHints\n\nIf your function doesn’t already take a calendar year as as an argument, modify your function to do so.\nThen loop through the years of 2004-2014 (inclusive) and run your function for each year.\n\n\nSolutions for both steps\n See the CSB notebook with the solutions.\nThe notebooks sometimes don’t manage to load on GitHub; if not, try refreshing and otherwise go here and look at the PDF version instead.\nNote: The solution uses some “plain Python” approaches where specialized Pandas functions are also available to do the same thing more succinctly. That is fine, and makes sense given that we have only had a quick introduction to Pandas and did not learn about these more advanced functions — but it may be good to be aware of this.\nIf you want to try out specialized Pandas approaches for this exercise, look into the groupby method in particular.\n\n4: Tutorial exercise  Image processing with NumPy\nTo get started, you will need to install scikit-image into your interactive Python Conda environment, which you can do as follows:\nsinteractive -A PAS1855 -t 60      # Start an interactive job\nmodule load python/3.6-conda5.2    # Load the OSC Conda module\nsource activate ipy-env            # Activate your existing Conda environment\nconda install -y scikit-image      # Install scikit-image\nWe start by loading NumPy and Scikit-image:\nimport numpy as np\nimport skimage.io as io\nNext, let’s save the path to the image we want to read in as a variable:\nimport os\n\n# Change \"<user>\" by your username:\n# (and adjust the path otherwise if necessary)\nCSB_dir = '/fs/ess/PAS1855/users/<user>/CSB/scientific/data'\n\nimage_file = 'Kacsoh2013_Drosobrain.png'\nimage_path = os.path.join(CSB_dir, image_file)\n\nPlotting works out of the Box in the VS Code interactive window.\nIf you’re using a Jupyter Notebook, you would also need to make Matplotlib image plotting available using the following IPython “magic function”:\n%matplotlib inline\n\nLet’s view the image:\nimage = io.imread(image_path)\nio.imshow(image)\n\n\n\nIn what format is our image stored?\ntype(image)\n#> numpy.ndarray\nAs a NumPy array!\nTherefore, we can use the NumPy methods we have learned about. Let’s check the dimensions:\nimage.shape\n#> (1024, 1024, 4)\nThe image is 1024 x 1024 pixels, with 4 the four RGBA channels along the 3rd dimension:\nRed (index 0)\nGreen (index 1)\nBlue (index 2)\nAlpha (opaqueness; index 3).\n\nThe first channel is the red channel, which should be the one of interest, given what our image looks like… Let’s extract it and explore a bit:\nred = image[:, :, 0]          # All rows, all columns, red channel\n\nprint(red.mean())\n#> 23.181024551391602\nprint(red.std())\n#> 30.446740821993011\nprint(red.min(), red.max())\n#> 0 255\nIt looks like we indeed only have variation in the red channel:\n# Loop over the channel indices in the 3rd dimension: image.shape[3]\nfor channel_index in range(0, image.shape[2]):\n\n      # Compute the standard deviation in the 3rd dimension:\n      print(image[:, :, channel_index].std())\n\n#> 30.44674082199301\n#> 0.0\n#> 0.0\n#> 0.0\nSay that we are interested in examining what area in the image express the NPF and NPFR1 genes that were targeted by this assay.\nWe’ll here take an overly simple approach, by setting a threshold of 100 and determining which pixels have a higher value than that.\nLet’s check what a value of 100 looks like by modifying a copy of the image:\nimg_copy = image.copy()\nimg_copy[:, 480:500, 0] = 100  # This should create a thick vertical red line\nio.imshow(img_copy)\n\n\n\nNext, we’ll “mask” our array: with a conditional expression, we produce a Boolean (False/True) array and then multiply that by 1 to get 0 (for False) for pixels that do not pass the threshold and 1 (for True) for pixels that do pass the threshold.\nthreshold = 100\nmask = (red > threshold) * 1 # Recall: \"red\" is the 2d-array for the red channel\nHow many pixels passed this mask (filter)?\nmask.sum()\n#> 37037\nPlot the mask:\nio.imshow(mask)\n\n\n\nFinally, we’ll use a little trick to get the actual pixel values back for those pixels that passed the threshold – we multiply the original array by the 0/1 mask:\nmask2 = red * mask\nio.imshow(mask2)\n\n\n\n\nSee here for PubMed search options.↩︎\nIf necessary, download the CSB repository again using git clone https://github.com/CSB-book/CSB.git↩︎\n",
      "last_modified": "2021-04-25T17:52:14-04:00"
    },
    {
      "path": "w11_readings.html",
      "title": "Week 11 content overview and readings",
      "author": [],
      "contents": "\nContent overview for this week\nThis week, you’ll first learn about using NumPy and Pandas, two large and popular Python packages for data analysis. Next, we’ll talk about BioPython, which is an ecosystem of packages in the field of bioinformatics, primarily to deal with DNA sequence data.\nSome of the things you will learn this week:\nThe key NumPy and Pandas data structures: the (n-dimensional) array and the DataFrame.\nVectorized operations and how they can be done with arrays and DataFrames.\nHow to index, slice, and manipulate arrays and DataFrames to analyze data.\nUse various BioPython modules to download sequence data from the NCBI, parse and subset FASTA files, and run BLAST.\nReadings\nRequired readings\nCSB Chapter 6: “Scientific Computing”\nSections 6.2.3-6.2.5 (“Linear algebra”, “Integration”, and “Optimization”, respectively) are optional reading and won’t be discussed in class.\nFurther resources\nThese resources are mostly from CSB 6.7 (“References and Reading”), with some additions and replacements:\nNumPy and Pandas\nFor more NumPy, I would recommend the official NumPy tutorial over the one mentioned in the book.\nFor more Pandas, I would similarly look first at the tutorials on the Pandas website.\nThe “Python for Data Analysis” book recommended by CBS is available online through OSU’s library. The book covers NumPy, Pandas, and plotting with Python, among other things.\nA similar, excellent book is the “Python Data Science Handbook” by Jake VanderPlas, which is freely available online.\nBioPython\nPerhaps the best place to start is this workshop tutorial by Peter Cock.\nFor more, the BioPython Tutorial and Cookbook has a comprehensive overview of BioPython functionality (HTML / PDF.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:15-04:00"
    },
    {
      "path": "w12_exercises.html",
      "title": "Exercises: Week 12 -- Regular expressions",
      "author": [],
      "contents": "\n\nContents\nExercise CSB-1 (5.9.1): Bee checklist\nBonus – Exercise CSB-2 (5.9.2): A map of science\n\n\nExercise CSB-1 (5.9.1): Bee checklist\nMichael Ruggiero of the Integrated Taxonomic Information System has led the World Bee Checklist project, aiming to collect taxonomic information on all the bee species in the world.\nIn the file CSB/regex/data/bee_list.txt1, you can find a list of about 20,000 species (!), along with their TSN (the identifier in the ITIS database), and a column detailing the authors and year of publication of documents describing the species.\nWhat is the name of the author with the most entries in the database? To find out, you’ll need to parse the citations in the file.\nNote that you need to account for different citation formats that occur in the file, such as:\n(Morawitz, 1877)\nCockerell, 1901\n(W. F. Kirby, 1900)\nMeade-Waldo, 1914\nEardley & Brooks, 1989\nLepeletier & Audinet-Serville, 1828\nMichener, LaBerge & Moure, 1955\n\nHints\nThe difficult part of the exercise is to write a regular expression capable of capturing a wide variety of styles used to store the authors’ names: there may or may not be a parenthesis at the beginning/end of the string; the initials for the authors may or may not be reported, author names can be split using commas or ampersands; some names are hyphenated.\nAlternatively, it is also possible to match the author names quite generically (non-specifically), and instead make use of the fact that author names are consistently followed by a comma and a 4-digit year.\nIn your regular expression, capture the author name(s) and the year as separate groups (with ()). Then, you can retrieve the author names and year from your match using the group() method.\nStart by testing your regular expression on one or a few citation strings.\nTo make sure you’ve captured all authors, check that the number of names matches the number of records.\nBecause we want to count publications for individual authors and not for combinations of authors, you will need to write a second regular expression that splits the list of authors using commas or &. Also be sure to include any necessary spaces in the splitting pattern!\nNext, you want to create a dictionary and count the number of occurrences of each author.\nTo get the author with the most occurrences, the CSB solution finds the maximum value with max_value_author = max(dict_authors.values()), then turns the dictionary values into a list and gets the index for the maximum value, and finally turns the dictionary keys into a list and extracts the key associated with the maximum value.\nHowever, in last week’s exercises, we saw that we can use the sorted() function to do this a little more easily: get a sorted list of keys using sorted(author_dict, key=author_dict.get, reverse=True) and then print the first value in this sorted list. (Note that in the sorted function, the key argument expects a function to be applied: author_dict.get gets the values, and then sorted() will sort the keys by those values.)\nPseudocode:\nopen the file\n   extract species name\n   extract author/date string\n   use re.match to extract i) author list, ii) date\n   now go through each author list, and split the authors\n   create a dictionary for authors to count the number of occurences for each author \n   get the author with the highest number of occurences   \n\nWhich year of publication occurs most often in the database?\n\nHints\nYou should be able to extract the year for each publication with group() just like you did above for the authors, provided you created a group with () that captures the year in your regular expression.\nThen, you should build a dictionary with counts for each year, just like you did for authors above – though here, you don’t need to preprocess the regular expression match since there is only year in each citation.\nThe last step is also very similar to what you did above: you’ll need to extract the year with the highest number of occurrences from your dictionary.\n\n\nSolutions for both steps\nSee the CSB notebook with the solutions.\n\nBonus – Exercise CSB-2 (5.9.2): A map of science\nWhere does science come from? This question has fascinated researchers for decades, and has even led to the birth of the field of the “science of science,” where researchers use the same tools they invented to investigate nature to gain insights into the development of science itself. In this exercise, you will build a “map of Science,” showing where articles published in the magazine Science have originated.\nYou will find two files in the directory CSB/regex/data/MapOfScience2. The first, pubmed_results.txt, is the output of a query to PubMed, listing all the papers published in Science in 2015. You will extract the US ZIP codes from this file, and then use the file zipcodes_coordinates.txt to extract the geographic coordinates for each ZIP code.\nRead the file pubmed_results.txt, and extract all the US ZIP codes (5-digit numbers).\n\nHints\nWhile you may be able to match only ZIP codes by creating a pattern that just matches 5 digits, it will be safer to match them using a more extensive pattern: two-letter state code => space => 5 digits => comma and space => USA.\nTo be able to match the pattern above, you need to account for the fact that the pattern may be split across two lines. Therefore, it’s best to read the entire file into a single string using the read() function (<my_file_handle>.read()), and then replacing newlines (\\n) followed by six spaces by a single space using the re.sub() function.\nNext, you will need a re function that returns all matches and not just a single match, since the entire file is now a single string. Recall that re.findall() will simply return a list of matches and not a match object.\nNote also that with re.findall(), if you use parentheses to group a match (i.e. the 5 digits of the ZIP code), it will return the group(s) only, which is exactly what you want!\nPseudocode:\nopen the file and read all of the text using f.read()\nmy_text = re.sub(your regex here, ' ', my_text)\nuse re.findall to extract all ZIP codes\nzipcodes = re.findall(another regex here, my_text)\n\nCreate the lists zip_code, zip_long, zip_lat, and zip_count, containing the unique ZIP codes, their longitudes, latitudes, and counts (number of occurrences in Science), respectively.\n\nHints\nYou need to first save and count the number of unique ZIP codes, and then extract the corresponding longitude and latitude from the file for each ZIP code from the file zipcodes_coordinates.txt.\nPseudocode:\n# list of distinct zipcodes\nunique_zipcodes = list(set(zipcodes))\nfor each zipcode:\n    extract number of occurrences\n    extract latitude and longitude from zipcodes_coordinates.txt\n\nTo visualize the data you’ve generated, use the code below.\n\nCode for the plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(zip_long, zip_lat, s = zip_count, c= zip_count)\nplt.colorbar()\n\n# Only plot the continental US without Alaska:\nplt.xlim(-125,-65)\nplt.ylim(23, 50)\n\n# Add a few cities for reference (optional):\nard = dict(arrowstyle=\"->\")\nplt.annotate('Los Angeles', xy = (-118.25, 34.05), \n               xytext = (-108.25, 34.05), arrowprops = ard)\nplt.annotate('Palo Alto', xy = (-122.1381, 37.4292), \n               xytext = (-112.1381, 37.4292), arrowprops= ard)\nplt.annotate('Cambridge', xy = (-71.1106, 42.3736), \n               xytext = (-73.1106, 48.3736), arrowprops= ard)\nplt.annotate('Chicago', xy = (-87.6847, 41.8369), \n               xytext = (-87.6847, 46.8369), arrowprops= ard)\nplt.annotate('Seattle', xy = (-122.33, 47.61), \n               xytext = (-116.33, 47.61), arrowprops= ard)\nplt.annotate('Miami', xy = (-80.21, 25.7753), \n               xytext = (-80.21, 30.7753), arrowprops= ard)\n\nparams = plt.gcf()\nplSize = params.get_size_inches()\nparams.set_size_inches( (plSize[0] * 3, plSize[1] * 3) )\nplt.show()\n\nSolutions for all steps\nSee the CSB notebook with the solutions.\n\n\nIf necessary, download the CSB repository again using git clone https://github.com/CSB-book/CSB.git↩︎\nIf necessary, download the CSB repository again using git clone https://github.com/CSB-book/CSB.git↩︎\n",
      "last_modified": "2021-04-25T17:52:15-04:00"
    },
    {
      "path": "w12_readings.html",
      "title": "Week 12 content overview and readings",
      "author": [],
      "date": "2021-04-12",
      "contents": "\nContent overview for this week\nThere is another instructional break this week, so we will only meet on Tuesday.\nThis week, you will learn about using regular expressions in Python. We already talked a fair bit about regular expressions in week 4 and used them with grep and sed in the Shell. Regular expressions are shared across all programming languages but there are slightly different “dialects” in use by different languages. We’ll see that Python’s syntax, as exposed by the re module, is similar to but a bit easier to use than that in Bash (e.g., no need to turn on extended regex!), and that there is some additional functionality too.\nSome of the things you will learn this week:\nAll common regular expression constructs and their syntax in Python: metacharacters, character classes (AKA sets), quantifiers, anchors, and alternations.\nWhy and how we need to define regular expressions as “raw strings”.\nDifferent functions in Python’s re module that can be used to match and replace text.\nHow you can use and refer back to “groupings” in regular expressions for fine-grained matching and extraction of sub-matches within larger matches.\nReadings\nThis week’s chapter, CSB’s “Regular Expressions”, is relatively short and straightforward. Yet learning how to use regular expressions can be extremely useful!\nRequired readings\nCSB Chapter 5: “Regular Expressions”\nFurther resources\nDive Into Python is a nice and free online book on Python, and its chapter on regular expressions is particularly good.\nRegex101 is a useful website to translate regular expressions into plain English and to test your code on example text.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:16-04:00"
    },
    {
      "path": "w13_exercises-bonus.html",
      "title": "Bonus Exercise: Week 13 -- Snakemake",
      "author": [],
      "contents": "\n\nContents\n1. Set-up\n2. Our initial workflow\n2. A config file with workflow settings\n3. A FastQC rule\n4. A MultiQC rule\n5. A Cutadapt rule\n6. A DADA2 rule to filter FASTQs\n7. A DADA2 rule to infers ASVs\nOur final Snakefile\n\n\n\nWe did not work with real data in the Snakemake workflows we created so far. Therefore, in this bonus exercise, you’ll create a workflow that processes actual FASTQ files, which should be useful practice if you intend to use Snakemake for your final project or your own research.\nWe’ll use the same 6 FASTQ files that we explored in the exercises for week 5 and the second graded assignment. These 6 FASTQ files consist of 3 pairs of R1 (forward reads) and R2 (reverse reads) files for 3 biological samples that contain sequences from the V4 region of 16S rRNA, generated in a metabarcoding experiment.\nIn the workflow, we will perform QC with FastQC and MultiQC, remove primers with Cutadapt, and filter FASTQ files and infer Amplicon Sequence Variants (ASVs) with the R package DADA2.\n1. Set-up\nYou can copy the data and a project skeleton from /fs/ess/PAS1855/exercises/:\ncd /fs/ess/PAS1855/users/$USER/week13\ncp -r /fs/ess/PAS1855/exercises/week13/bonus/initial_workflow bonus_exercise\n\n# Go into the exercise dir:\ncd bonus_exercise\n\n# Make sure all the scripts are executable:\nchmod +x scripts/*\nTake a look at the files we have:\n$ tree\n#> .\n#> ├── data\n#> │   ├── 201_R1.fastq.gz\n#> │   ├── 201_R2.fastq.gz\n#> │   ├── 202_R1.fastq.gz\n#> │   ├── 202_R2.fastq.gz\n#> │   ├── 203_R1.fastq.gz\n#> │   └── 203_R2.fastq.gz\n#> ├── log\n#> ├── scripts\n#> │   ├── cutadapt.sh\n#> │   ├── fastq_filter.R\n#> │   ├── infer_ASVs.R\n#> │   └── multiqc.sh\n#> ├── slurm_profile\n#> │   └── config.yaml\n#> └── workflow\n#>     ├── config.yaml\n#>     ├── envs\n#>     └── Snakefile\nFinally, load the Conda environment in which you have Snakemake installed:\nmodule load python/3.6-conda5.2\nsource activate ipy-env\n\n# Check if Snakemake can be found and is inside your ipy-env:\nwhich snakemake\n#> ~/.conda/envs/ipy-env/bin/snakemake\nHere is a DAG (Directed Acyclic Graph) of the full workflow that we will build:\n\n\n\n2. Our initial workflow\n\nFull Snakefile #1\n\"\"\"\nA workflow to perform QC and basic analyses on a set of FASTQ files from\nmetagenomic amplicon sequencing.\nThe following steps are performed:\n1. FastQC on the raw FASTQ files\n2. MultiQC on the output of FastQC\n3. Cutadapt to remove primers on the raw FASTQ files\n4. DADA2 FASTQ filtering\n5. DADA2 ASV inference, producing a single ASV output table\n\"\"\"\n\n# We'll use the `join` function to create paths\nfrom os.path import join\n\n# Settings:\nIN_DIR=\"data\"\nOUT_DIR=\"results\"\nLOG_DIR=\"log\"\n\nPRIMER_F: \"GAGTGYCAGCMGCCGCGGTAA\"\nPRIMER_R: \"ACGGACTACNVGGGTWTCTAAT\"\n\n# Samples and reads:\nSAMPLES = glob_wildcards(join(IN_DIR, \"{sample}_R1.fastq.gz\")).sample\nREADS = [\"R1\", \"R2\"]\n\n# All output dirs:\nFASTQC_DIR=join(OUT_DIR, \"fastqc\")\nMULTIQC_DIR=join(OUT_DIR, \"multiqc\")\nCUTADAPT_DIR=join(OUT_DIR, \"cutadapt\")\nFILTER_DIR=join(OUT_DIR, \"fastq_filter\")\nASV_DIR=join(OUT_DIR, \"ASV_infer\")\n\n# Define local rules:\nlocalrules: all, clean  \n\n# Rules:\nrule all:\n    \"\"\"\n    A pseudo-rule that triggers execution of the entire workflow.\n    It's input files include all final output files.\n    \"\"\"\n    input:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n        join(ASV_DIR, \"ASV_table.tsv\")\n\nrule clean:\n    \"\"\"\n    A rule to remove all results from the workflow.\n    \"\"\"\n    shell:\n        \"rm -rf results;\"\n        \"rm -rf log/*\"\nCurrent Snakefile\nOur current Snakefile has a couple of features we haven’t seen before:\nWe use generic Python code directly in the Snakefile: we import the join() function from the os.path module, and further down, use this function to build file paths: 1\nfrom os.path import join\n\nFASTQC_DIR=join(OUT_DIR, \"fastqc\")\nWe have a rule clean, which is a common convenience rule to remove all output and log files from the workflow. As you are developing and testing a workflow, you may want to do this regularly.\nIn the shell directive of rule clean, we have two lines of shell code, which works because we use Python triple-quotes.\nAlso of note:\nIn rule all, the output files of the final workflow are already specified, so we won’t have to change this rule in the exercises below.\nWe will run the workflow on the cluster in the sense that each Snakemake job will be a SLURM job. Because rule all and rule clean they are not computationally intensive at all, we specify them as “local rules” (i.e., rules not to be submitted as SLURM jobs) using thelocalrules directive. (The overhead of submitting a SLURM job and waiting for it start would just slow thing down for those rules.)\nOur “profile” config.yaml file\nWe use a “profile” slurm_profile/config.yaml file very similar to the one in a previous Snakemake exercise:\ncluster: \"sbatch --account={resources.account}\n                 --time={resources.time_min}\n                 --mem={resources.mem_mb}\n                 --cpus-per-task={resources.cpus}\n                 --output=log/slurm-{rule}_{wildcards}.out\"\ndefault-resources: [cpus=1, mem_mb=4000, time_min=60, account=PAS1855]\nlatency-wait: 30\njobs: 100\nuse-conda: true\nThis file tells Snakemake how to submit jobs to the cluster, and provides some default resource values that we would be able to modify for individual rules (as we will eventually do for rule ASV_infer).\nRecall that this file simply contains options that we could also pass at the command line. We can therefore also take the opportunity to specify the maximum number of jobs (which we normally do with the -j option) and ensure that Snakemake creates and uses any Conda environments specified in the Snakefile (use-conda: true; corresponds to the --use-conda option).\nThen, we don’t have to specify this at the command line when we call Snakemake, and the only option we need to pass to Snakemake is --profile. For this option, we specify the directory (not the file name, which is assumed to be config.yaml by Snakemake) in which our YAML file is found: snakemake --profile slurm_profile.\n\n2. A config file with workflow settings\nInstead of specifying settings like input and output directories as well as primer sequences in our Snakefile, it would be useful to separate such settings from the workflow itself. Snakemake supports the use of a configuration file to do so.\nLet’s create a configuration file workflow/config.yaml with the following settings:\nin_dir: \"data\"\nout_dir: \"results\"\nlog_dir: \"log\"\n\nprimer_f: \"GAGTGYCAGCMGCCGCGGTAA\"\nprimer_r: \"ACGGACTACNVGGGTWTCTAAT\"\nThe format of the file is YAML just like the slurm_profile/config.yaml above and the files we have used to specify and save Conda environments.\nNow, we need a way to tell Snakemake about this config file. We can do so using the configfile directive in the Snakefile. After we do that, all key-value pairs from the config file will be available in the Snakefile in a Python dictionary called config:\n# Workflow configuration file:\nconfigfile: \"workflow/config.yaml\"\n\n# Get settings from the config file:\nIN_DIR=config[\"in_dir\"]\nOUT_DIR=config[\"out_dir\"]\nLOG_DIR=config[\"log_dir\"]\n\nReplace the 5 lines of code in the Snakefile under the # Settings: line with the lines printed directly above this box.\nThen, assign the primer sequences in config.yaml to the variables PRIMER_F and PRIMER_R, respectively.\n\nSolution\n\nAssign the primer sequences to variables PRIMER_F and PRIMER_R:\nPRIMER_F=config[\"primer_f\"]\nPRIMER_R=config[\"primer_r\"]\n\n\nFull Snakefile #2\n\"\"\"\nA workflow to perform QC and basic analyses on a set of FASTQ files from\nmetagenomic amplicon sequencing.\nThe following steps are performed:\n1. FastQC on the raw FASTQ files\n2. MultiQC on the output of FastQC\n3. Cutadapt to remove primers on the raw FASTQ files\n4. DADA2 FASTQ filtering\n5. DADA2 ASV inference, producing a single ASV output table\n\"\"\"\n\n# We'll use the `join` function to create paths\nfrom os.path import join\n\n# Workflow configuration file:\nconfigfile: \"workflow/config.yaml\"\n\n# Get settings from config file:\nIN_DIR=config[\"in_dir\"]\nOUT_DIR=config[\"out_dir\"]\nLOG_DIR=config[\"log_dir\"]\n\nPRIMER_F=config[\"primer_f\"]\nPRIMER_R=config[\"primer_r\"]\n\n# Other constants, mostly based on config file:\n# Samples and reads:\nSAMPLES = glob_wildcards(join(IN_DIR, \"{sample}_R1.fastq.gz\")).sample\nREADS = [\"R1\", \"R2\"]\n# All output dirs:\nFASTQC_DIR=join(OUT_DIR, \"fastqc\")\nMULTIQC_DIR=join(OUT_DIR, \"multiqc\")\nCUTADAPT_DIR=join(OUT_DIR, \"cutadapt\")\nFILTER_DIR=join(OUT_DIR, \"fastq_filter\")\nASV_DIR=join(OUT_DIR, \"ASV_infer\")\n\n# Define local rules:\nlocalrules: all, clean  \n\n# Rules:\nrule all:\n    \"\"\"\n    A pseudo-rule that triggers execution of the entire workflow.\n    It's input files include all final output files.\n    \"\"\"\n    input:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n        join(ASV_DIR, \"ASV_table.tsv\")\n\nrule clean:\n    \"\"\"\n    A rule to remove all results from the workflow.\n    \"\"\"\n    shell:\n        \"rm -rf results;\"\n        \"rm -rf log/*\"\n\n3. A FastQC rule\nNow, let’s add a rule to run FastQC:\nrule fastqc:\n    \"\"\"\n    A rule to run FastQC to QC the FASTQ files.\n    \"\"\"\n    input:\n        join(IN_DIR, \"{sample}_{read}.fastq.gz\"),\n    output:\n        join(FASTQC_DIR, \"{sample}_{read}_fastqc.html\"),\n    log:\n        join(LOG_DIR, \"fastqc/{sample}_{read}.log\"),\n    shell:\n        \"\"\"\n        module load fastqc/0.11.8\n        fastqc {input} -o {FASTQC_DIR} &> {log}\n        \"\"\"\nSome things to note about this rule:\nPreviously we have always run scripts in the shell directive, but in rule fastqc, we directly run the program FastQC in the shell directive.\nBecause FastQC doesn’t allow us to specify the output file, we just provide an output directory. Note that in the shell directive, we can access any Snakefile variable using {} (not just values of directives like {input} and {log}). Therefore, we can retrieve the FastQC output dir with {FASTQC_DIR}.\nIn the input and output directives, we are using two wildcards: {sample} and {read}.\nTo avoid excessive installations, we load the OSC module for FastQC, here. If we had wanted increased portability, we could have specified a Conda environment using a YAML file like workflow/envs/fastqc.yaml with the following contents:\nchannels:\n  - bioconda\ndependencies:\n  - fastqc=0.11.8\nWhen we have Snakemake submit jobs to the cluster, and we also specify log files with the log directive, we will have two log files for every job:\nThe log file that we define in the Snakefile will contain standard output and/or standard error from our actual shell command.\nThe SLURM log file will contain’s Snakemake’s standard output and standard error.\n\nAdd rule fastqc, with the contents as specified above, to the Snakefile.\nGet Snakemake to run rule fastqc for at least one of the FASTQ files.\nCheck for the presence of output files and look at the log files.\n\nHints\n\nOur rule all input does not match the output of rule fastqc, so trying to run Snakemake without speciying a rule or output file(s) won’t work (recall that by default, Snakemake will run the first rule, which is rule all). We could add this to rule all, but since we want to add a rule MultiQC in the next step, which uses the output of FastQC, that would only be temporary.\nMoreover, rule fastqc contains wildcards in its {input} and {output} directives, so trying to run it using snakemake [...] fastqc would result in an error: Snakemake is not able to figure out what it should run (you can most conveniently try this with a dry-run using snakemake -n fastqc).\nsnakemake -n fastqc\n#> WorkflowError:\n#> Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards.\nTo test whether rule functions, it is therefore most convenient to just call Snakemake with an actual output file.\n\nSolution\n$ snakemake --profile slurm_profile \\\n    results/fastqc/201_R1_fastqc.html\n$ ls -lh results/fastqc/\n#> total 1.9M\n#> -rw-r--r-- 1 jelmer PAS0471 1007K Apr 21 09:19 201_R1_fastqc.html\n#> -rw-r--r-- 1 jelmer PAS0471  847K Apr 21 09:19 201_R1_fastqc.zip\n\n# This file will show FastQC's output:  \n$ cat log/fastqc_201_R1.log\n#> Started analysis of 201_R1.fastq.gz\n#> Approx 5% complete for 201_R1.fastq.gz\n#> Approx 10% complete for 201_R1.fastq.gz\n#> Approx 15% complete for 201_R1.fastq.gz\n#> Approx 20% complete for 201_R1.fastq.gz\n#> Approx 25% complete for 201_R1.fastq.gz\n#> Approx 30% complete for 201_R1.fastq.gz\n#> Approx 35% complete for 201_R1.fastq.gz\n#> Approx 40% complete for 201_R1.fastq.gz\n#> Approx 45% complete for 201_R1.fastq.gz\n#> Approx 50% complete for 201_R1.fastq.gz\n#> Approx 55% complete for 201_R1.fastq.gz\n#> Approx 60% complete for 201_R1.fastq.gz\n#> Approx 65% complete for 201_R1.fastq.gz\n#> Approx 70% complete for 201_R1.fastq.gz\n#> Approx 75% complete for 201_R1.fastq.gz\n#> Approx 80% complete for 201_R1.fastq.gz\n#> Approx 85% complete for 201_R1.fastq.gz\n#> Approx 90% complete for 201_R1.fastq.gz\n#> Approx 95% complete for 201_R1.fastq.gz\n#> Analysis complete for 201_R1.fastq.gz\n\n$ cat log/fastqc_slurm-read\\=R1\\,sample\\=201.out \n#> Building DAG of jobs...\n#> Using shell: /usr/bin/bash\n#> Provided cores: 1 (use --cores to define parallelism)\n#> Rules claiming more threads will be scaled down.\n#> Job counts:\n#>         count   jobs\n#>         1       fastqc\n#>         1\n#> \n#> [Wed Apr 21 09:19:25 2021]\n#> rule fastqc:\n#>     input: data/201_R1.fastq.gz\n#>     output: results/fastqc/201_R1_fastqc.html\n#>     log: log/fastqc_201_R1.log\n#>     jobid: 0\n#>     wildcards: sample=201, read=R1\n#>     resources: cpus=1, mem_mb=4000, time_min=60, account=PAS1855\n#> \n#> Activating conda environment: #> /fs/ess/PAS1855/data/week13/exercise_bonus/step2/.snakemake/conda/7e57eac489af13d2bb58773ba0fd9#> c38\n#> [Wed Apr 21 09:19:36 2021]\n#> Finished job 0.\n#> 1 of 1 steps (100%) done\n\n\nFull Snakefile #3\n\"\"\"\nA workflow to perform QC and basic analyses on a set of FASTQ files from\nmetagenomic amplicon sequencing.\nThe following steps are performed:\n1. FastQC on the raw FASTQ files\n2. MultiQC on the output of FastQC\n3. Cutadapt to remove primers on the raw FASTQ files\n4. DADA2 FASTQ filtering\n5. DADA2 ASV inference, producing a single ASV output table\n\"\"\"\n\n# We'll use the `join` function to create paths\nfrom os.path import join\n\n# Workflow configuration file:\nconfigfile: \"workflow/config.yaml\"\n\n# Get settings from config file:\nIN_DIR=config[\"in_dir\"]\nOUT_DIR=config[\"out_dir\"]\nLOG_DIR=config[\"log_dir\"]\n\nPRIMER_F=config[\"primer_f\"]\nPRIMER_R=config[\"primer_r\"]\n\n# Other constants, mostly based on config file:\n# Samples and reads:\nSAMPLES = glob_wildcards(join(IN_DIR, \"{sample}_R1.fastq.gz\")).sample\nREADS = [\"R1\", \"R2\"]\n# All output dirs:\nFASTQC_DIR=join(OUT_DIR, \"fastqc\")\nMULTIQC_DIR=join(OUT_DIR, \"multiqc\")\nCUTADAPT_DIR=join(OUT_DIR, \"cutadapt\")\nFILTER_DIR=join(OUT_DIR, \"fastq_filter\")\nASV_DIR=join(OUT_DIR, \"ASV_infer\")\n\n# Define local rules:\nlocalrules: all, clean  \n\n# Rules:\nrule all:\n    \"\"\"\n    A pseudo-rule that triggers execution of the entire workflow.\n    It's input files include all final output files.\n    \"\"\"\n    input:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n        join(ASV_DIR, \"ASV_table.tsv\")\n\nrule clean:\n    \"\"\"\n    A rule to remove all results from the workflow.\n    \"\"\"\n    shell:\n        \"rm -rf results;\"\n        \"rm -rf log/*\"\n\nrule fastqc:\n    \"\"\"\n    A rule to run FastQC to QC the FASTQ files.\n    \"\"\"\n    conda:\n        \"envs/fastqc.yaml\"\n    input:\n        join(IN_DIR, \"{sample}_{read}.fastq.gz\"),\n    output:\n        join(FASTQC_DIR, \"{sample}_{read}_fastqc.html\"),\n    log:\n        join(LOG_DIR, \"fastqc_{sample}_{read}.log\"),\n    shell:\n        \"fastqc {input} -o {FASTQC_DIR} &> {log}\"\n\n4. A MultiQC rule\nWe will add a rule that runs MultiQC, which summarizes multiple FastQC reports into a single HTML file.\nWe will run a shell script that in turn runs MultiQC, rather than running MultiQC directly in the shell directive like we did for FastQC.\nThe main reason for this is an annoyance with installing MultiQC through Conda2: when using a YAML file or attempting a one-step installation, Conda will fail. Instead, we need to do a two-step installation: first, we install Python, and then, we install MultiQC. The scripts/multiqc.sh script that rule multiqc runs will create this environment if it doesn’t already exist, before running MultiQC.\n\nShow the script scripts/multiqc.sh\n#!/bin/bash\n\nset -e -u -o pipefail\n\nmodule load python/3.6-conda5.2\n\n# Process command-line args:\nin_dir=$1\nout_dir=$2\n\n# Report:\necho \"# This script will run MultiQC.\"\necho \"# Input directory: $in_dir\"\necho -e \"# Output directory: $out_dir \\n\" \n\n# Activate/install and activate MultiQC conda environment:\nif [[ $(conda env list) = *\"multiqc-env\"* ]]; then\n    echo \"# Activating existing Conda environment multiqc-env\"\n    source activate multiqc-env\nelse\n    echo \"# Installing MultiQC in Conda environment multiqc-env\"\n    conda create -y -n multiqc-env python=3.7\n    source activate multiqc-env\n    conda install -y -c bioconda -c conda-forge multiqc\nfi\n\n# Run MultiQC:\necho -e \"\\n# Running MultiQC...\"\nmultiqc \"$in_dir\" -o \"$out_dir\"\n\necho -e \"\\n# Done with script.\"\n\n\nBuild a rule multiqc to run MultiQC with the script scripts/multiqc.sh.\nRecall that MultiQC will run once with all FastQC output files as its input. The output file will be automatically called multiqc_report.html so the output directive should be:\noutput:\n    join(MULTIQC_DIR, \"multiqc_report.html\"),\nYou’ll still need to come up with input, log, and shell directives for the rule.\n\nHints\n\nUse the expand() function in the input directive. Besides expanding the possible values for SAMPLES and READS, you will also need to include the FastQC output dir here, which is saved in the variable FASTQC_DIR.\nIn the shell directive, you will call the script scripts/multiqc.sh. Take a look at the script to see what arguments it takes, and specify those.\nLike in rule fastqc, send standard output and standard error from the script to {log}.\n\nSolutions\nrule multiqc:\n    \"\"\"\n    A rule to run MultiQC to collate the FastQC results.\n    \"\"\"\n    input:\n        expand(\n            \"{fastqc_dir}/{sample}_{read}_fastqc.html\",\n            fastqc_dir=FASTQC_DIR,\n            sample=SAMPLES,\n            read=READS,\n        ),\n    output:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n    log:\n        join(LOG_DIR, \"multiqc.log\"),\n    shell:\n        \"scripts/multiqc.sh {FASTQC_DIR} {MULTIQC_DIR} &> {log}\"\n\nRun rule MultiQC and check the output. (What else will be run automatically?)\n\nSolutions\nrule fastqc will also be run for the remaining 5 FASTQ files, since the input of rule multiqc is the output of rule fastqc.\n$ snakemake --profile slurm_profile multiqc\n$ ls -lh results/multiqc/\n#> total 1.2M\n#> drwxr-xr-x 2 jelmer PAS0471 4.0K Apr 21 10:38 multiqc_data\n#> -rw-r--r-- 1 jelmer PAS0471 1.2M Apr 21 10:38 multiqc_report.html\n\n$ cat log/multiqc.log\n#> # This script will run MultiQC.\n#> # Input directory: results/fastqc\n#> # Output directory: results/multiqc \n#> \n#> # Activating existing Conda environment multiqc-env\n#> \n#> # Running MultiQC...\n#> [WARNING]         multiqc : MultiQC Version v1.10.1 now available!\n#> [INFO   ]         multiqc : This is MultiQC v1.10\n#> [INFO   ]         multiqc : Template    : default\n#> [INFO   ]         multiqc : Searching   : #> /fs/project/PAS0471/teach/courses/pracs-sp21/week13/exercise_bonus/step3_multiqc/results/fastqc\n#> Searching   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 12/12  [INFO   ]          fastqc : #> Found 6 reports\n#> [INFO   ]         multiqc : Compressing plot data\n#> [INFO   ]         multiqc : Report      : results/multiqc/multiqc_report.html\n#> [INFO   ]         multiqc : Data        : results/multiqc/multiqc_data\n#> [INFO   ]         multiqc : MultiQC complete\n#> \n#> # Done with script.\n\n\nFull Snakefile #4\n\"\"\"\nA workflow to perform QC and basic analyses on a set of FASTQ files from\nmetagenomic amplicon sequencing.\nThe following steps are performed:\n1. FastQC on the raw FASTQ files\n2. MultiQC on the output of FastQC\n3. Cutadapt to remove primers on the raw FASTQ files\n4. DADA2 FASTQ filtering\n5. DADA2 ASV inference, producing a single ASV output table\n\"\"\"\n\n# We'll use the `join` function to create paths\nfrom os.path import join\n\n# Workflow configuration file:\nconfigfile: \"workflow/config.yaml\"\n\n# Get settings from config file:\nIN_DIR=config[\"in_dir\"]\nOUT_DIR=config[\"out_dir\"]\nLOG_DIR=config[\"log_dir\"]\n\nPRIMER_F=config[\"primer_f\"]\nPRIMER_R=config[\"primer_r\"]\n\n# Other constants, mostly based on config file:\n# Samples and reads:\nSAMPLES = glob_wildcards(join(IN_DIR, \"{sample}_R1.fastq.gz\")).sample\nREADS = [\"R1\", \"R2\"]\n# All output dirs:\nFASTQC_DIR=join(OUT_DIR, \"fastqc\")\nMULTIQC_DIR=join(OUT_DIR, \"multiqc\")\nCUTADAPT_DIR=join(OUT_DIR, \"cutadapt\")\nFILTER_DIR=join(OUT_DIR, \"fastq_filter\")\nASV_DIR=join(OUT_DIR, \"ASV_infer\")\n\n# Define local rules:\nlocalrules: all, clean  \n\n# Rules:\nrule all:\n    \"\"\"\n    A pseudo-rule that triggers execution of the entire workflow.\n    It's input files include all final output files.\n    \"\"\"\n    input:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n        join(ASV_DIR, \"ASV_table.tsv\")\n\nrule clean:\n    \"\"\"\n    A rule to remove all results from the workflow.\n    \"\"\"\n    shell:\n        \"\"\"\n        rm -rf results\n        rm -rf log/*\n        \"\"\"\n\nrule fastqc:\n    \"\"\"\n    A rule to run FastQC to QC the FASTQ files.\n    \"\"\"\n    input:\n        join(IN_DIR, \"{sample}_{read}.fastq.gz\"),\n    output:\n        join(FASTQC_DIR, \"{sample}_{read}_fastqc.html\"),\n    log:\n        join(LOG_DIR, \"fastqc_{sample}_{read}.log\"),\n    shell:\n        \"\"\"\n        module load fastqc/0.11.8\n        fastqc {input} -o {FASTQC_DIR} &> {log}\n        \"\"\"\n\nrule multiqc:\n    \"\"\"\n    A rule to run MultiQC to collate the FastQC results.\n    \"\"\"\n    input:\n        expand(\n            \"{fastqc_dir}/{sample}_{read}_fastqc.html\",\n            fastqc_dir=FASTQC_DIR,\n            sample=SAMPLES,\n            read=READS,\n        ),\n    output:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n    log:\n        join(LOG_DIR, \"multiqc.log\"),\n    shell:\n        \"scripts/multiqc.sh {FASTQC_DIR} {MULTIQC_DIR} &> {log}\"\n\n5. A Cutadapt rule\nNext, we will add a rule to run Cutadapt, which will remove the primer sequences from our reads.\nrule cutadapt:\n    \"\"\"\n    A rule to run Cutadapt to remove primers from the FASTQ files.\n    \"\"\"\n    conda:\n        \"envs/cutadapt.yaml\"\n    input:\n        fwd=join(IN_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(IN_DIR, \"{sample}_R2.fastq.gz\"),\n    output:\n        fwd=join(CUTADAPT_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(CUTADAPT_DIR, \"{sample}_R2.fastq.gz\"),\n    log:\n        join(LOG_DIR, \"cutadapt_{sample}.log\")\n    shell:\n        \"scripts/cutadapt.sh {input.fwd} {CUTADAPT_DIR} {PRIMER_F} {PRIMER_R} &> {log}\"\nSome things to note about this rule:\nWe are processing one sample and therefore two FASTQ files (forward and reverse) at a time.\nAgain, we run a script (scripts/cutadapt.sh) rather than calling the program directly in the rule. In this case, that’s the most convenient option because we will have the script compute the reverse complements of the primer sequences.\nThe script takes four arguments:\nThe name for the FASTQ file with forward reads (the name for the file with reverse reads will be inferred from that).\nThe output directory.\nThe forward primer sequences.\nThe reverse primer sequences.\nThe output file names are automatically determined by the script.\n\nThe script scripts/cutadapt.sh\n\n#!/bin/bash\nset -e -u -o pipefail\n\n# This script will run Cutadapt on a pair (R1+R2) of FASTQ files,\n# removing primer sequences.\n# Forward and reverse primer sequences are passed as argument to the script,\n# and the script will compute the reverse complements of each.\n\n# SETUP --------------------------------------------------------\nR1_in=\"$1\"          # Path to the file with forward (R1) reads (R1 filename is inferred from  this)\noutdir=\"$2\"         # Output directory\nprimer_f=\"$3\"       # Sequence of the forward primer\nprimer_r=\"$4\"       # Sequence of the reverse primer\n\n# Test if the correct number of args were passed to the script:\nif [ ! \"$#\" = 4 ]; then\n    echo \"Error: must provide 4 arguments; you provided $#\"\n    echo \"Usage: cutadapt_single.sh <input-FASTQ-R1> <outdir> <primer_f> <primer_r>\"\n    exit 1\nfi\n\n# Get reverse primer sequences by reverse complementing:\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\n\n# Infer R2 (reverse) filename: R1 and R2 are assumed to have the same name\n# except for \"_R1_\" vs \"_R1_\": \nR2_in=${R1_in/_R1/_R2}\n\n# Check that the input dir is not the same as the output dir,\n# or the input files will be overwritten:\nindir=$(dirname \"$R1_in\")\nif [ \"$indir\" = \"$outdir\" ]; then\n    echo \"## ERROR: Input dir should not be the same as output dir\" >&2\n    exit 1\nfi\n\n# Output files:\nR1_out=\"$outdir\"/$(basename \"$R1_in\" .fastq.gz).fastq.gz\nR2_out=\"$outdir\"/$(basename \"$R2_in\" .fastq.gz).fastq.gz\n\n# Report:\necho -e \"\\n## Starting cutadapt script.\"\ndate\necho\necho \"## Using the following parameters:\"\necho \"## Input file R1: $R1_in\"\necho \"## Output dir: $outdir\"\necho \"## Forward primer: $primer_f\"\necho \"## Reverse primer: $primer_r\"\necho\necho \"## Input file R2: $R2_in\"\necho \"## Output file R1: $R1_out\"\necho \"## Output file R2: $R2_out\"\necho\necho \"## Reverse complement of forward primer: $primer_f_rc\"\necho \"## Reverse complement of reverse primer: $primer_r_rc\"\necho\n\n# Test:\nif [ ! -f \"$R1_in\" ] || [ ! -r \"$R1_in\" ]; then\n    echo -e \"\\n## $0: ERROR: Input file R1_in, $R1_in, not found\\n\" >&2 && exit 1\nfi\nif [ ! -f \"$R2_in\" ] || [ ! -r \"$R2_in\" ]; then\n    echo -e \"\\n## $0: ERROR: Input file R2_in, $R2_in, not found\\n\" >&2 && exit 1\nfi\n\n# Create output directory if it doesn't already exist:\nmkdir -p \"$outdir\"\n\n# RUN CUTADAPT --------------------------------------------------------\necho -e \"\\n\\n## Running cutadapt...\"\n\ncutadapt -a \"$primer_f\"...\"$primer_r_rc\" -A \"$primer_r\"...\"$primer_f_rc\" \\\n    --discard-untrimmed --pair-filter=any \\\n    -o \"$R1_out\" -p \"$R2_out\" \"$R1_in\" \"$R2_in\"\n\n# Options:\n# \"-a\"/\"-A\": Primers for R1/R2\n# \"--discard-untrimmed\": Remove pairs with no primer found\n# \"--pair-filter=any\": Remove pair if one read is filtered (=Default)\n\n# REPORT AND FINALIZE --------------------------------------------------------\necho -e \"\\n## Listing output files:\"\nls -lh \"$outdir\"\n\necho -e \"\\n## Done with cutadapt script.\"\ndate\n\nThis time, we will provide Snakemake with a YAML file describing a Conda environment for Cutadapt. Here is the contents of the YAML file envs/cutadapt.yaml:\nchannels:\n  - bioconda\ndependencies:\n  - cutadapt=3.4\n\nAdd the rule cutadapt with the contents as specified above.\nGet Snakemake to run rule cutadapt for at least one of the FASTQ files. Note that this will take a little while the first time around, since Snakemake will create a Conda environment with Cutadapt.\nCheck for the presence of output files and look at the log files.\n\nHints\n\nLike with rule fastqc, you’ll need to specify an individual output file.\n\nSolutions\n$ snakemake --profile slurm_profile results/cutadapt/201_R1.fastq.gz\n#> Building DAG of jobs...\n#> Creating conda environment workflow/envs/cutadapt.yaml...\n#> Downloading and installing remote packages.\n#> [...]\n$ ls -lh results/cutadapt/\n#> -rw-r--r-- 1 jelmer PAS0471 6.0M Apr 21 11:25 201_R1.fastq.gz\n#> -rw-r--r-- 1 jelmer PAS0471 7.1M Apr 21 11:25 201_R2.fastq.gz\n\n$ less log/cutadapt_201.log\n#> ## Starting cutadapt script.\n#> Wed Apr 21 11:25:00 EDT 2021\n#> \n#> ## Using the following parameters:\n#> ## Input file R1: data/201_R1.fastq.gz\n#> ## Output dir: results/cutadapt\n#> ## Forward primer: GAGTGYCAGCMGCCGCGGTAA\n#> ## Reverse primer: ACGGACTACNVGGGTWTCTAAT\n#> \n#> ## Input file R2: data/201_R2.fastq.gz\n#> ## Output file R1: results/cutadapt/201_R1.fastq.gz\n#> ## Output file R2: results/cutadapt/201_R2.fastq.gz\n#> \n#> ## Reverse complement of forward primer: TTACCGCGGCKGCTGRCACTC\n#> ## Reverse complement of reverse primer: ATTAGAWACCCBNGTAGTCCGT\n#> \n#> \n#> ## Running cutadapt...\n#> This is cutadapt 3.4 with Python 3.9.2\n#> Command line parameters: -a GAGTGYCAGCMGCCGCGGTAA...ATTAGAWACCCBNGTAGTCCGT -A #> ACGGACTACNVGGGTWTCTAAT...TTACCGCGGCKGCTGRCACTC --discard-untrimmed --pair-filter=any -o #> results/cutadapt/201_R1.fastq.gz -p results/cutadapt/201_R2.fastq.gz #> data/201_R1.fastq.gz data/201_R2.fastq.gz\n#> Processing reads on 1 core in paired-end mode ...\n#> Finished in 4.84 s (103 µs/read; 0.58 M reads/minute).\n#> \n#> === Summary ===\n#> \n#> Total read pairs processed:             47,070\n#>   Read 1 with adapter:                  47,061 (100.0%)\n#>   Read 2 with adapter:                  46,675 (99.2%)\n#> Pairs written (passing filters):        46,671 (99.2%)\n\n\nFull Snakefile #5\n\"\"\"\nA workflow to perform QC and basic analyses on a set of FASTQ files from\nmetagenomic amplicon sequencing.\nThe following steps are performed:\n1. FastQC on the raw FASTQ files\n2. MultiQC on the output of FastQC\n3. Cutadapt to remove primers on the raw FASTQ files\n4. DADA2 FASTQ filtering\n5. DADA2 ASV inference, producing a single ASV output table\n\"\"\"\n\n# We'll use the `join` function to create paths\nfrom os.path import join\n\n# Workflow configuration file:\nconfigfile: \"workflow/config.yaml\"\n\n# Get settings from config file:\nIN_DIR=config[\"in_dir\"]\nOUT_DIR=config[\"out_dir\"]\nLOG_DIR=config[\"log_dir\"]\n\nPRIMER_F=config[\"primer_f\"]\nPRIMER_R=config[\"primer_r\"]\n\n# Other constants, mostly based on config file:\n# Samples and reads:\nSAMPLES = glob_wildcards(join(IN_DIR, \"{sample}_R1.fastq.gz\")).sample\nREADS = [\"R1\", \"R2\"]\n# All output dirs:\nFASTQC_DIR=join(OUT_DIR, \"fastqc\")\nMULTIQC_DIR=join(OUT_DIR, \"multiqc\")\nCUTADAPT_DIR=join(OUT_DIR, \"cutadapt\")\nFILTER_DIR=join(OUT_DIR, \"fastq_filter\")\nASV_DIR=join(OUT_DIR, \"ASV_infer\")\n\n# Define local rules:\nlocalrules: all, clean  \n\n# Rules:\nrule all:\n    \"\"\"\n    A pseudo-rule that triggers execution of the entire workflow.\n    It's input files include all final output files.\n    \"\"\"\n    input:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n        join(ASV_DIR, \"ASV_table.tsv\")\n\nrule clean:\n    \"\"\"\n    A rule to remove all results from the workflow.\n    \"\"\"\n    shell:\n        \"\"\"\n        rm -rf results\n        rm -rf log/*\n        \"\"\"\n\nrule fastqc:\n    \"\"\"\n    A rule to run FastQC to QC the FASTQ files.\n    \"\"\"\n    input:\n        join(IN_DIR, \"{sample}_{read}.fastq.gz\"),\n    output:\n        join(FASTQC_DIR, \"{sample}_{read}_fastqc.html\"),\n    log:\n        join(LOG_DIR, \"fastqc_{sample}_{read}.log\"),\n    shell:\n        \"\"\"\n        module load fastqc/0.11.8\n        fastqc {input} -o {FASTQC_DIR} &> {log}\n        \"\"\"\n\nrule multiqc:\n    \"\"\"\n    A rule to run MultiQC to collate the FastQC results.\n    \"\"\"\n    input:\n        expand(\n            \"{fastqc_dir}/{sample}_{read}_fastqc.html\",\n            fastqc_dir=FASTQC_DIR,\n            sample=SAMPLES,\n            read=READS,\n        ),\n    output:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n    log:\n        join(LOG_DIR, \"multiqc.log\"),\n    shell:\n        \"scripts/multiqc.sh {FASTQC_DIR} {MULTIQC_DIR} &> {log}\"\n\nrule cutadapt:\n    \"\"\"\n    A rule to run Cutadapt to remove primers from the FASTQ files.\n    \"\"\"\n    conda:\n        \"envs/cutadapt.yaml\"\n    input:\n        fwd=join(IN_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(IN_DIR, \"{sample}_R2.fastq.gz\"),\n    output:\n        fwd=join(CUTADAPT_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(CUTADAPT_DIR, \"{sample}_R2.fastq.gz\"),\n    log:\n        join(LOG_DIR, \"cutadapt_{sample}.log\")\n    shell:\n        \"scripts/cutadapt.sh {input.fwd} {CUTADAPT_DIR} {PRIMER_F} {PRIMER_R} &> {log}\"\n\n6. A DADA2 rule to filter FASTQs\nWe will run the script scripts/fastq_filter.R to filter and trim our Cutadapt-processed FASTQ files (this will remove poor-quality reads and bases).\nThis script takes two arguments:\nThe name of the file with forward reads (in our case, as output by rule cutadapt).\nThe output directory.\nThe script processes one pair of FASTQ files, like the Cutadapt rule. It outputs a pair of unzipped FASTQ files with the same names as the input files, just in a different directory (FILTER_DIR, in our case) and with a .fastq instead of a .fastq.gz extension.\n\nThe full script scripts/fastq_filter.R\n\n#!/usr/bin/env Rscript\n\n# This script will run the DADA2 function \"filterAndTrim\" to remove poor-quality\n# reads and bases from FASTQ files by filtering and trimming them.\n# The script has as input a pair (F and R reads for one sample) of,\n# optionally gzipped, FASTQ files, and as output a pair of not-zipped FASTQ files. \n\n# Set-up -----------------------------------------------------------------------\n\n# Install and/or load packages:\nif(!require(pacman)) {\n  install.packages(\"pacman\", repos = \"http://cran.us.r-project.org\")\n  library(pacman)\n}\npackages <- c('BiocManager', 'dada2')\npacman::p_load(char = packages)\n\n# Get command-line args:\nargs <- commandArgs(trailingOnly = TRUE)\nR1_in <- as.character(args[1])\noutdir <- as.character(args[2])\n\n# Determine R2 file name - assumed to be same as R1 but with \"R2\" instead:\nR2_in <- sub(\"_R1\", \"_R2\", R1_in)\n\n# Determine sample ID - file name up until \"_R1\":\nsampleID <- sub(\"_R1.*\", \"\", basename(R1_in))\n\n# Set output file paths:\nR1_out <- file.path(outdir, paste0(sampleID, '_R1.fastq'))\nR2_out <- file.path(outdir, paste0(sampleID, '_R2.fastq'))\n\n# Report:\ncat(\"Output dir:\", outdir, \"\\n\\n\")\ncat(\"R1_in:\", R1_in, \"\\n\")\ncat(\"R2_in:\", R2_in, \"\\n\")\ncat(\"R1_out:\", R1_out, \"\\n\")\ncat(\"R2_out:\", R2_out, \"\\n\")\n\n\n# Filtering and Quality Trimming -----------------------------------------------\n\nprint('Filtering and Trimming...')\nSys.time()  # Print the time to keep track of running time for individual steps\n\n# Run the DADA2 function to filter and trim FASTQ files\nfilter_results <-\n  filterAndTrim(\n      R1_in, R1_out,          # Forward (F) reads input/output\n      R2_in, R2_out,          # Reverse (R) reads input/output\n      truncLen = c(250,210),  # Truncate F at 250 and R at 210 bp (remove last bases)\n      trimLeft = 10,          # Remove first 10 bases\n      maxN = 0,               # Don't allow for any Ns\n      maxEE = c(2,2),         # Max. expected nr of errors of 2 in F and 2 in R\n      truncQ = 2,             # Truncate when base quality score is 2 or less\n      rm.phix = FALSE,        # Don't remove Phi-X\n      multithread = 1,        # No multithreading\n      compress = FALSE,       # Don't compress output files\n      verbose = TRUE          # Verbose runtime info\n  )\n\nprint('...Done!')\nSys.time()\n\nprint('Done with script fastq_filter.R')\n\n\nWrite rule fastq_filter.\nRun rule fastq_filter for one sample and check the output.\n\nSolutions\n\nrule fastq_filter:\n    \"\"\"\n    A rule to run an R script that contains DADA2 functions to filter and trim FASTQ files.\n    \"\"\"\n    input:\n        fwd=join(CUTADAPT_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(CUTADAPT_DIR, \"{sample}_R2.fastq.gz\"),\n    output:\n        fwd=join(FILTER_DIR, \"{sample}_R1.fastq\"),\n        rev=join(FILTER_DIR, \"{sample}_R2.fastq\"),\n    log:\n        join(LOG_DIR, \"fastq_filter_{sample}.log\")\n    shell:\n        \"\"\"\n        module load R/4.0.2-gnu9.1\n        scripts/fastq_filter.R {input.fwd} {FILTER_DIR} &> {log}\n        \"\"\"\n$ snakemake --profile slurm_profile \\\n    results/fastq_filter/201_R1.fastq\n$ ls -lh results/fastq_filter/\n#> total 42M\n#> -rw-r--r-- 1 jelmer PAS0471 23M Apr 21 11:47 201_R1_filt.fastq\n#> -rw-r--r-- 1 jelmer PAS0471 19M Apr 21 11:47 201_R2_filt.fastq\n\n\nFull Snakefile #6\n\"\"\"\nA workflow to perform QC and basic analyses on a set of FASTQ files from\nmetagenomic amplicon sequencing.\nThe following steps are performed:\n1. FastQC on the raw FASTQ files\n2. MultiQC on the output of FastQC\n3. Cutadapt to remove primers on the raw FASTQ files\n4. DADA2 FASTQ filtering\n5. DADA2 ASV inference, producing a single ASV output table\n\"\"\"\n\n# We'll use the `join` function to create paths:\nfrom os.path import join\n\n# Workflow configuration file:\nconfigfile: \"workflow/config.yaml\"\n\n# Get settings from config file:\nIN_DIR=config[\"in_dir\"]\nOUT_DIR=config[\"out_dir\"]\nLOG_DIR=config[\"log_dir\"]\n\nPRIMER_F=config[\"primer_f\"]\nPRIMER_R=config[\"primer_r\"]\n\n# Other constants, mostly based on config file:\n# Samples and reads:\nSAMPLES = glob_wildcards(join(IN_DIR, \"{sample}_R1.fastq.gz\")).sample\nREADS = [\"R1\", \"R2\"]\n# All output dirs:\nFASTQC_DIR=join(OUT_DIR, \"fastqc\")\nMULTIQC_DIR=join(OUT_DIR, \"multiqc\")\nCUTADAPT_DIR=join(OUT_DIR, \"cutadapt\")\nFILTER_DIR=join(OUT_DIR, \"fastq_filter\")\nASV_DIR=join(OUT_DIR, \"ASV_infer\")\n\n# Define local rules:\nlocalrules: all, clean  \n\n# Rules:\nrule all:\n    \"\"\"\n    A pseudo-rule that triggers execution of the entire workflow.\n    It's input files include all final output files.\n    \"\"\"\n    input:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n        join(ASV_DIR, \"ASV_table.tsv\")\n\nrule clean:\n    \"\"\"\n    A rule to remove all results from the workflow.\n    \"\"\"\n    shell:\n        \"\"\"\n        rm -rf results\n        rm -rf log/*\n        \"\"\"\n\nrule fastqc:\n    \"\"\"\n    A rule to run FastQC to QC the FASTQ files.\n    \"\"\"\n    input:\n        join(IN_DIR, \"{sample}_{read}.fastq.gz\"),\n    output:\n        join(FASTQC_DIR, \"{sample}_{read}_fastqc.html\"),\n    log:\n        join(LOG_DIR, \"fastqc_{sample}_{read}.log\"),\n    shell:\n        \"\"\"\n        module load fastqc/0.11.8\n        fastqc {input} -o {FASTQC_DIR} &> {log}\n        \"\"\"\n\nrule multiqc:\n    \"\"\"\n    A rule to run MultiQC to collate the FastQC results.\n    \"\"\"\n    input:\n        expand(\n            \"{fastqc_dir}/{sample}_{read}_fastqc.html\",\n            fastqc_dir=FASTQC_DIR,\n            sample=SAMPLES,\n            read=READS,\n        ),\n    output:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n    log:\n        join(LOG_DIR, \"multiqc.log\"),\n    shell:\n        \"scripts/multiqc.sh {FASTQC_DIR} {MULTIQC_DIR} &> {log}\"\n\nrule cutadapt:\n    \"\"\"\n    A rule to run Cutadapt to remove primers from the FASTQ files.\n    \"\"\"\n    conda:\n        \"envs/cutadapt.yaml\"\n    input:\n        fwd=join(IN_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(IN_DIR, \"{sample}_R2.fastq.gz\"),\n    output:\n        fwd=join(CUTADAPT_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(CUTADAPT_DIR, \"{sample}_R2.fastq.gz\"),\n    log:\n        join(LOG_DIR, \"cutadapt_{sample}.log\")\n    shell:\n        \"scripts/cutadapt.sh {input.fwd} {CUTADAPT_DIR} {PRIMER_F} {PRIMER_R} &> {log}\"\n\nrule fastq_filter:\n    \"\"\"\n    A rule to run an R script that contains DADA2 functions to filter and trim FASTQ files.\n    \"\"\"\n    input:\n        fwd=join(CUTADAPT_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(CUTADAPT_DIR, \"{sample}_R2.fastq.gz\"),\n    output:\n        fwd=join(FILTER_DIR, \"{sample}_R1.fastq\"),\n        rev=join(FILTER_DIR, \"{sample}_R2.fastq\"),\n    log:\n        join(LOG_DIR, \"fastq_filter_{sample}.log\")\n    shell:\n        \"\"\"\n        module load R/4.0.2-gnu9.1\n        scripts/fastq_filter.R {input.fwd} {FILTER_DIR} &> {log}\n        \"\"\"\n\n7. A DADA2 rule to infers ASVs\nFinally, we will create a rule that will run the script scripts/ASV_infer.R, which takes as its input all filtered FASTQ files at once, and outputs a file containing a matrix of counts of ASVs (Amplicon Sequence Variants) for each sample.\n\nThe full script scripts/ASV_infer.R\n#!/usr/bin/env Rscript\n\n# This script will run several steps of the DADA2 pipeline to infer\n# Amplicon Sequence Variants (ASVs) and their abundance in a set of samples.\n# The input is a set of FASTQ files from multiple samples\n# (as automatically detected given an input dir, which is one of the arguments),\n# and the output is a single table with ASV counts per sample.\n\n\n# Set-up ----------------------------------------------------------------------\n\nargs <- commandArgs(trailingOnly = TRUE)\nindir <- as.character(args[1])           # Input dir. Input files will be detected.\noutfile <- as.character(args[2])         # Ouput file: An ASV table.\nn_cores <- as.integer(args[3])           # Number of cores to use.\n\n# Install and/or load packages:\nif(!require(pacman)) {\n  install.packages(\"pacman\", repos = \"http://cran.us.r-project.org\")\n  library(pacman)\n}\npackages <- c('BiocManager', 'dada2')\npacman::p_load(char = packages)\n\n# Detect input files given the input directory passed as an argument:\nfastqs_filt_F <- sort(list.files(indir, pattern = '_R1.fastq', full.names = TRUE))\nfastqs_filt_R <- sort(list.files(indir, pattern = '_R2.fastq', full.names = TRUE))\n\n# Infer Sample IDs from FASTQ file names:\nsampleIDs <- sub(\"_R1.*\", \"\", basename(fastqs_filt_F))\n\n# Report:\ncat(\"Input dir:\", indir, \"\\n\")\ncat(\"Output file:\", outfile, \"\\n\")\ncat(\"Number of threads/cores:\", n_cores, \"\\n\")\ncat(\"Sample IDs:\", sampleIDs, \"\\n\")\n\n\n# Dereplication ----------------------------------------------------------------\n\nprint('Dereplicating FASTQs...')\nSys.time() # Keep track of time to see how how long each step takes\n\nfastqs_derep_F <- derepFastq(fastqs_filt_F, verbose = FALSE)\nfastqs_derep_R <- derepFastq(fastqs_filt_R, verbose = FALSE)\n\nnames(fastqs_derep_F) <- sampleIDs\nnames(fastqs_derep_R) <- sampleIDs\n\nprint('...Done!')\nSys.time()\n\n\n# Error training  -------------------------------------------------------------\n\nprint('Learning errors...')\n\nerr_F <- learnErrors(fastqs_derep_F, multithread = n_cores, verbose = TRUE)\nerr_R <- learnErrors(fastqs_derep_R, multithread = n_cores, verbose = TRUE)\n\nprint('...Done!')\nSys.time()\n\n\n# Infer ASVs  -------------------------------------------------------------\n\nprint('Inferring ASVs (running the dada algorithm)...')\n\ndada_Fs <- dada(fastqs_derep_F, err = err_F,\n                pool = FALSE, multithread = n_cores)\ndada_Rs <- dada(fastqs_derep_R, err = err_R,\n                pool = FALSE, multithread = n_cores)\n\nprint('...Done!')\nSys.time()\n\n\n# Merge read pairs ------------------------------------------------------------\n\nprint('Merging read pairs...')\n\nmergers <- mergePairs(dada_Fs, fastqs_derep_F,\n                      dada_Rs, fastqs_derep_R,\n                      verbose = TRUE)\n\n\n# Construct a sequence table --------------------------------------------------\n\nprint('Constructing a sequence table...')\n\nseqtab_all <- makeSequenceTable(mergers)\n\n\n# Remove chimeras ------------------------------------------------------------\n\nprint('Remove chimeras...')\n\nseqtab <- removeBimeraDenovo(seqtab_all,\n                             method = 'consensus',\n                             multithread = n_cores,\n                             verbose = TRUE)\n\n# Write to file ----------------------------------------------------------------\n\nprint('Write table to file...')\n\nwrite.table(seqtab, outfile,\n            sep = \"\\t\", row.names = TRUE, quote = FALSE)\n\nprint('Done with script ASV_infer.R')\n\nFor this rule, we will tinker a bit with the resource request to SLURM. This script is more computationally intensive that the previous ones, which makes sense because it will process all files at once.\nSpecifically, the DADA2 functions that we run in the script can take an argument multithread, specifying the number of cores/threads that can be used. Therefore, scripts/ASV_infer.R also takes such an argument, which will be passed on the the DADA2 functions.\n\nHow can we make sure Snakemake allocated 8 cores/threads for this job?\n\nSolution.\n\nBy using a threads directive in the rule:\nthreads: 8\nAnd how can we make sure Snakemake passes a request of 8 cores/threads to SLURM for this job?\n\nSolution.\n\nBy using a resources directive in the rule, with a key cpus that matches what we have specified in slurm_profile/config.yaml:\nresources: cpus=8\nHere is the relevant matching part of slurm_profile/config.yaml:\ncluster: \"sbatch --account={resources.account}\n                 --time={resources.time_min}\n                 --mem={resources.mem_mb}\n                 --cpus-per-task={resources.cpus}\n                 --output=log/slurm-{rule}_{wildcards}.out\"\ndefault-resources: [cpus=1, mem_mb=4000, time_min=60, account=PAS1855]\n\nAdditionally, we will request for an appropriate amount of memory3, so our full resources directive will look like this:\nresources: cpus=8, mem_mb=32000\nThe script scripts/ASV_infer.R takes three arguments in total in the following order: 1. An input directory – i.e., the output dir of the previous rule. 2. An output filename – you can tell from rule all what that should be. 3. A number of cores/threads for the job – which you can pass using {threads}.\n\nWrite rule ASV_infer.\nRun the rule.\n\nHints\n\nBecause this is the final rule in the workflow, whose output is specified as the input of rule all, running Snakemake without arguments will do the job.\nNote that the previous two rules will also automatically be run for the two remaining samples.\n\nSolutions\n\nrule ASV_infer:\n    \"\"\"\n    A rule to run an R script that contains DADA2 functions to infer ASVs,\n    and produce a matrix with ASV counts for each sample.\n    \"\"\"\n    threads: 8\n    resources: cpus=8, mem_mb=32000\n    input:\n        expand(\n            \"{filter_dir}/{sample}_{read}.fastq\",\n            sample=SAMPLES,\n            read=READS,\n            filter_dir=FILTER_DIR,\n        ),\n    output:\n        join(ASV_DIR, \"ASV_table.tsv\")\n    log:\n        join(LOG_DIR, \"ASV_infer.log\")\n    shell:\n        \"\"\"\n        module load R/4.0.2-gnu9.1\n        scripts/ASV_infer.R {FILTER_DIR} {output} {threads} &> {log}\n        \"\"\"\n$ snakemake --profile slurm_profile\n$ ls -lh results/ASV_infer/\n#> -rw-r--r-- 1 jelmer PAS0471 426K Apr 21 12:57 ASV_table.tsv\n\nLet’s see an overview of all output files of the workflow:\n$ tree results\n#> results\n#> ├── ASV_infer\n#> │   └── ASV_table.tsv\n#> ├── cutadapt\n#> │   ├── 201_R1.fastq.gz\n#> │   ├── 201_R2.fastq.gz\n#> │   ├── 202_R1.fastq.gz\n#> │   ├── 202_R2.fastq.gz\n#> │   ├── 203_R1.fastq.gz\n#> │   └── 203_R2.fastq.gz\n#> ├── fastqc\n#> │   ├── 201_R1_fastqc.html\n#> │   ├── 201_R1_fastqc.zip\n#> │   ├── 201_R2_fastqc.html\n#> │   ├── 201_R2_fastqc.zip\n#> │   ├── 202_R1_fastqc.html\n#> │   ├── 202_R1_fastqc.zip\n#> │   ├── 202_R2_fastqc.html\n#> │   ├── 202_R2_fastqc.zip\n#> │   ├── 203_R1_fastqc.html\n#> │   ├── 203_R1_fastqc.zip\n#> │   ├── 203_R2_fastqc.html\n#> │   └── 203_R2_fastqc.zip\n#> ├── fastq_filter\n#> │   ├── 201_R1.fastq\n#> │   ├── 201_R2.fastq\n#> │   ├── 202_R1.fastq\n#> │   ├── 202_R2.fastq\n#> │   ├── 203_R1.fastq\n#> │   └── 203_R2.fastq\n#> └── multiqc\n#>     ├── multiqc_data\n#>     │   ├── multiqc_data.json\n#>     │   ├── multiqc_fastqc.txt\n#>     │   ├── multiqc_general_stats.txt\n#>     │   ├── multiqc.log\n#>     │   └── multiqc_sources.txt\n#>     └── multiqc_report.html\nOur final Snakefile\n(You can also find the final workflow files at /fs/ess/PAS1855/exercises/week13/bonus/final_workflow.)\n\"\"\"\nA workflow to perform QC and basic analyses on a set of FASTQ files from\nmetagenomic amplicon sequencing.\nThe following steps are performed:\n1. FastQC on the raw FASTQ files\n2. MultiQC on the output of FastQC\n3. Cutadapt to remove primers on the raw FASTQ files\n4. DADA2 FASTQ filtering\n5. DADA2 ASV inference, producing a single ASV output table\n\"\"\"\n\n# We'll use the `join` function to create paths:\nfrom os.path import join\n\n# Workflow configuration file:\nconfigfile: \"workflow/config.yaml\"\n\n# Get settings from config file:\nIN_DIR=config[\"in_dir\"]\nOUT_DIR=config[\"out_dir\"]\nLOG_DIR=config[\"log_dir\"]\n\nPRIMER_F=config[\"primer_f\"]\nPRIMER_R=config[\"primer_r\"]\n\n# Other constants, mostly based on config file:\n# Samples and reads:\nSAMPLES = glob_wildcards(join(IN_DIR, \"{sample}_R1.fastq.gz\")).sample\nREADS = [\"R1\", \"R2\"]\n# All output dirs:\nFASTQC_DIR=join(OUT_DIR, \"fastqc\")\nMULTIQC_DIR=join(OUT_DIR, \"multiqc\")\nCUTADAPT_DIR=join(OUT_DIR, \"cutadapt\")\nFILTER_DIR=join(OUT_DIR, \"fastq_filter\")\nASV_DIR=join(OUT_DIR, \"ASV_infer\")\n\n# Define local rules:\nlocalrules: all, clean  \n\n# Rules:\nrule all:\n    \"\"\"\n    A pseudo-rule that triggers execution of the entire workflow.\n    It's input files include all final output files.\n    \"\"\"\n    input:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n        join(ASV_DIR, \"ASV_table.tsv\")\n\nrule clean:\n    \"\"\"\n    A rule to remove all results from the workflow.\n    \"\"\"\n    shell:\n        \"\"\"\n        rm -rf results\n        rm -rf log/*\n        \"\"\"\n\nrule fastqc:\n    \"\"\"\n    A rule to run FastQC to QC the FASTQ files.\n    \"\"\"\n    input:\n        join(IN_DIR, \"{sample}_{read}.fastq.gz\"),\n    output:\n        join(FASTQC_DIR, \"{sample}_{read}_fastqc.html\"),\n    log:\n        join(LOG_DIR, \"fastqc_{sample}_{read}.log\"),\n    shell:\n        \"\"\"\n        module load fastqc/0.11.8\n        fastqc {input} -o {FASTQC_DIR} &> {log}\n        \"\"\"\n\nrule multiqc:\n    \"\"\"\n    A rule to run MultiQC to collate the FastQC results.\n    \"\"\"\n    input:\n        expand(\n            \"{fastqc_dir}/{sample}_{read}_fastqc.html\",\n            fastqc_dir=FASTQC_DIR,\n            sample=SAMPLES,\n            read=READS,\n        ),\n    output:\n        join(MULTIQC_DIR, \"multiqc_report.html\"),\n    log:\n        join(LOG_DIR, \"multiqc.log\"),\n    shell:\n        \"scripts/multiqc.sh {FASTQC_DIR} {MULTIQC_DIR} &> {log}\"\n\nrule cutadapt:\n    \"\"\"\n    A rule to run Cutadapt to remove primers from the FASTQ files.\n    \"\"\"\n    conda:\n        \"envs/cutadapt.yaml\"\n    input:\n        fwd=join(IN_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(IN_DIR, \"{sample}_R2.fastq.gz\"),\n    output:\n        fwd=join(CUTADAPT_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(CUTADAPT_DIR, \"{sample}_R2.fastq.gz\"),\n    log:\n        join(LOG_DIR, \"cutadapt_{sample}.log\")\n    shell:\n        \"scripts/cutadapt.sh {input.fwd} {CUTADAPT_DIR} {PRIMER_F} {PRIMER_R} &> {log}\"\n\nrule fastq_filter:\n    \"\"\"\n    A rule to run an R script that contains DADA2 functions to filter and trim FASTQ files.\n    \"\"\"\n    input:\n        fwd=join(CUTADAPT_DIR, \"{sample}_R1.fastq.gz\"),\n        rev=join(CUTADAPT_DIR, \"{sample}_R2.fastq.gz\"),\n    output:\n        fwd=join(FILTER_DIR, \"{sample}_R1.fastq\"),\n        rev=join(FILTER_DIR, \"{sample}_R2.fastq\"),\n    log:\n        join(LOG_DIR, \"fastq_filter_{sample}.log\")\n    shell:\n        \"\"\"\n        module load R/4.0.2-gnu9.1\n        scripts/fastq_filter.R {input.fwd} {FILTER_DIR} &> {log}\n        \"\"\"\n\nrule ASV_infer:\n    \"\"\"\n    A rule to run an R script that contains DADA2 functions to infer ASVs,\n    and produce a matrix with ASV counts for each sample.\n    \"\"\"\n    threads: 8\n    resources: cpus=8, mem_mb=32000\n    input:\n        expand(\n            \"{filter_dir}/{sample}_{read}.fastq\",\n            sample=SAMPLES,\n            read=READS,\n            filter_dir=FILTER_DIR,\n        ),\n    output:\n        join(ASV_DIR, \"ASV_table.tsv\")\n    log:\n        join(LOG_DIR, \"ASV_infer.log\")\n    shell:\n        \"\"\"\n        module load R/4.0.2-gnu9.1\n        scripts/ASV_infer.R {FILTER_DIR} {output} {threads} &> {log}\n        \"\"\"\n\n\nEach core has about 4 GB of memory so 8 * 4 = 32 GB makes sense here. Because our default resource request is for 4000 MB (4 GB), we would otherwise be requesting 4 GB total, even with 8 cores.↩︎\nEach core has about 4 GB of memory so 8 * 4 = 32 GB makes sense here. Because our default resource request is for 4000 MB (4 GB), we would otherwise be requesting 4 GB total, even with 8 cores.↩︎\nEach core has about 4 GB of memory so 8 * 4 = 32 GB makes sense here. Because our default resource request is for 4000 MB (4 GB), we would otherwise be requesting 4 GB total, even with 8 cores.↩︎\n",
      "last_modified": "2021-04-25T17:52:17-04:00"
    },
    {
      "path": "w13_exercises.html",
      "title": "Exercises: Week 13 -- Snakemake",
      "author": [],
      "contents": "\n\nContents\n1: Expanding and improving the workflow\n1. Set-up\n2. First DAG and Snakemake run\n3. Add an input file to the map rule\n4. Add a FastQC rule\n5. Run the FastQC rule\n6. When we get errors\n7. The log directive\n\n2: Running the workflow with SLURM jobs\n1. Introduction\n2. Submit Snakemake as a single SLURM job\n3. Let Snakemake submit jobs for us\n4. Use a “profile”\n4. Add options to the profile config file\n5. Local rules\n7. Rule-specific resources: threads\n8. Rule-specific resources: other\n\n\n\nIn these exercises, you’ll take up where we left off in class, using a dummy RNA-seq workflow.\n\n1: Expanding and improving the workflow\n1. Set-up\nLoad the Conda environment in which you have Snakemake installed:\nmodule load python/3.6-conda5.2\nsource activate ipy-env\n\n# Check if Snakemake can be found and is inside your ipy-env:\nwhich snakemake\n#> ~/.conda/envs/ipy-env/bin/snakemake\nWe will recreate a very similar set of files and the final Snakefile from class – copy the files from /fs/ess/PAS1855/exercises/week13/ to do so:\ncd /fs/ess/PAS1855/users/$USER/week13\ncp -r /fs/ess/PAS1855/exercises/week13/exercise1 .\n\ncd exercise1\n\ntree\n.\n├── data\n│   ├── smpA.fastq\n│   ├── smpC.fastq\n│   └── smpG.fastq\n├── res\n├── scripts\n│   ├── count.sh\n│   ├── map.sh\n│   └── trim.sh\n└── workflow\n    ├── DAGs\n    └── Snakefile\n\nOr use this code to create the dirs and files from scratch\nCreate the dir structure and the dummy FASTQ files:\nmkdir -p data scripts res workflow/DAGs\n\necho \"AAAAAAA\" > data/smpA.fastq\necho \"CCCCCCC\" > data/smpC.fastq\necho \"GGGGGGG\" > data/smpG.fastq\nCreate a script scripts/trim.sh containing:\n#!/bin/bash\nset -e -u -o pipefail\n\necho \"FASTQ file: $1 after trimming\"\ncat \"$1\"\nCreate a script scripts/map.sh containing:\n#!/bin/bash\nset -e -u -o pipefail\n\necho \"BAM from FASTQ file: $1\"\ncat \"$1\"\nCreate a script scripts/count.sh containing:\n#!/bin/bash\nset -e -u -o pipefail\n\necho \"Counts for $# BAM files:\"\ncat $@\nMake the scripts executable:\nchmod +x scripts/*\nCreate a file workflow/Snakefile and paste into it:\n\"\"\"\nA dummy RNAseq workflow with FASTQ files as input and a count table as output.  \n\"\"\"\n\nSAMPLES=glob_wildcards(\"data/{smp}.fastq\").smp\n\nrule all:\n    \"\"\"\n    This is a \"pseudo-rule\" meant to trigger the execution of the full workflow. \n    \"\"\"\n    input:\n        \"res/count_table.txt\",\n\nrule trim:\n    \"\"\"\n    Trim the FASTQ files.\n    \"\"\"\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}_trim.fastq\",\n    shell:\n        \"scripts/trim.sh {input} > {output}\"\n\nrule map:\n    \"\"\"\n    Map the trimmed FASTQ files to a reference genome.\n    \"\"\"\n    input:\n        \"res/{smp}_trim.fastq\",\n    output:\n        \"res/{smp}.bam\",\n    shell:\n        \"scripts/map.sh {input} > {output}\"\n\nrule count:\n    \"\"\"\n    From the BAM files, create a gene count table.\n    \"\"\"\n    input:\n        expand(\"res/{smp}.bam\", smp=SAMPLES),\n    output:\n        \"res/count_table.txt\",\n    shell:\n        \"scripts/count.sh {input} > {output}\"\n\nCurrently, our Snakefile workflow/Snakefile looks like this:\n\nShow Snakefile\n\"\"\"\nA dummy RNAseq workflow with FASTQ files as input and a count table as output.  \n\"\"\"\n\nSAMPLES = glob_wildcards(\"data/{smp}.fastq\").smp\n\nrule all:\n    \"\"\"\n    This is a \"pseudo-rule\" meant to trigger the execution of the full workflow. \n    \"\"\"\n    input:\n        \"res/count_table.txt\",\n\nrule trim:\n    \"\"\"\n    Trim the FASTQ files.\n    \"\"\"\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}_trim.fastq\",\n    shell:\n        \"scripts/trim.sh {input} > {output}\"\n\nrule map:\n    \"\"\"\n    Map the trimmed FASTQ files to a reference genome.\n    \"\"\"\n    input:\n        \"res/{smp}_trim.fastq\",\n    output:\n        \"res/{smp}.bam\",\n    shell:\n        \"scripts/map.sh {input} > {output}\"\n\nrule count:\n    \"\"\"\n    From the BAM files, create a gene count table.\n    \"\"\"\n    input:\n        expand(\"res/{smp}.bam\", smp=SAMPLES),\n    output:\n        \"res/count_table.txt\",\n    shell:\n        \"scripts/count.sh {input} > {output}\"\n\nNote that some formatting details in the Snakefile above are slightly different from what was shown in class. I used the Snakefmt VS Code extension, which lets a little program of the same name format Snakefiles for you.1\nrule count:\n    \"\"\"\n    From the BAM files, create a gene count table.\n    \"\"\"\n    input:\n        expand(\"res/{smp}.bam\", smp=SAMPLES),\n    output:\n        \"res/count_table.txt\",\n    shell:\n        \"scripts/count.sh {input} > {output}\"\nSnakefmt will detect syntax errors and ensure consistent and clear formatting, as shown for rule count above:\nThe value for every rule is on a new, indented line.\nEvery item in input and output is trailed by a comma.2\n\n\n\nFurthermore, I have added some Python multi-line (triple-quote) comments explaining what the Snakefile and each individual rule does.\n\n2. First DAG and Snakemake run\nCreate and view a DAG (workflow diagram in Directed Acyclic Graph form) for our current Snakefile to remind yourself of the steps in the workflow.\n\nSolution\nsnakemake --dag | dot -T svg > workflow/DAGs/v1.svg \n\n\n\nRun the entire workflow.\n\nSolution\n Below, Snakemake is being run with the -p (print shell commands) and -q (quiet) options for concise output. To better understand what Snakemake is doing, you may want to see more output yourself, e.g. with the -pr combination of options (-r: reason for execution of each job).\nAlso, recall that you can always do a “dry run” with the -n option to see what Snakemake wants to execute.\nsnakemake -j1 -pq\n#> Job counts:\n#>         count   jobs\n#>         1       all\n#>         1       count\n#>         3       map\n#>         3       trim\n#>         8\n#> scripts/trim.sh data/smpG.fastq > res/smpG_trim.fastq\n#> scripts/map.sh res/smpG_trim.fastq > res/smpG.bam\n#> scripts/trim.sh data/smpC.fastq > res/smpC_trim.fastq\n#> scripts/trim.sh data/smpA.fastq > res/smpA_trim.fastq\n#> scripts/map.sh res/smpA_trim.fastq > res/smpA.bam\n#> scripts/map.sh res/smpC_trim.fastq > res/smpC.bam\n#> scripts/count.sh res/smpG.bam res/smpC.bam res/smpA.bam > res/count_table.txt\n\n3. Add an input file to the map rule\nCurrently, our map and count rules don’t include any genomic reference files: mapping should happen against a reference genome (FASTA file), and counting needs an annotation for the reference genome (GTF/GFF file).\nFor now, we will fix this oversight just for the mapping rule. Let’s create a dummy genome FASTA file:\nmkdir metadata\necho \"AAAAAAACCCCCCCGGGGGGG\" > metadata/ref.fa\nWe’ll add a line to the map.sh script, such that the second argument passed to it is now the FASTA file with the reference genome:\necho 'echo \"Using ref genome: $2\" && cat $2' >> scripts/map.sh\nAdapt the Snakefile to accommodate this change.\n\nHints\n\nTo do so, you’ll need to do the following in the map rule:\nPass the reference FASTA file to the map.sh script in the shell directive.\nWhile not strictly necessary to avoid errors, you should include the reference FASTA file in the input directive.\nBecause you will now have multiple items for input, it will be good to name them – the syntax for that is: input: myname=myfile, and you can then recall this using {input.myname} in the shell directive.\nMoreover, it is good practice to define hard-coded input files like this one clearly at the top of your script/Snakefile (or use a configuration file.\n\nSolution\n\nAssign the filename somewhere near the top:\nREF_FA = \"metadata/ref.fa\"\nAdjust rule map:\nrule map:\n    input:\n        fastq=\"res/{smp}_trim.fastq\",\n        ref=REF_FA,\n    output:\n        \"res/{smp}.bam\",\n    shell:\n        \"scripts/map.sh {input.fastq} {input.ref} > {output}\"\n\nDo you think Snakemake would rerun parts of the workflow now? How could you test this without actually rerunning anything?\n\nSolution\n\nSnakemake will indeed rerun the rule map and rule count.\nIt will rerun rule map because one of the input files, the reference FASTA, is newer than the output files (we just created it!).\nThen, it will rerun rule count because that depends on the output files of rule map, which have just changed.\nIf we hadn’t just created the reference FASTA file, Snakemake would not have rerun: it will not rerun based on changes in the Snakefile code.\n(However, it will keep track of code changes and you can force a rerun based on files affected by such changes using:snakemake -j1 -R $(snakemake --list-code-changes))\nTo test this without actually rerunning anything, we can use the -n flag to do a dry-run. Moreover, if we also use the -r flag, Snakemake will tell us the reason for the execution of every job.\nsnakemake -j1 -nr\n\n# [...]\n[Mon Apr  5 21:14:35 2021]\nrule map:\n   input: res/smpC_trim.fastq, metadata/ref.fa\n   output: res/smpC.bam\n   jobid: 2\n   reason: Updated input files: metadata/ref.fa  # \"REASON\" GIVEN HERE \n   wildcards: smp=smpC\n# [...]  \n\nRerun Snakemake.\n\nSolution\n\nsnakemake -j1 -pq\n\n#> Job counts:\n#>         count   jobs\n#>         1       all\n#>         1       count\n#>         3       map\n#>         5\n#> scripts/map.sh res/smpA_trim.fastq metadata/ref.fa > res/smpA.bam\n#> scripts/map.sh res/smpG_trim.fastq metadata/ref.fa > res/smpG.bam\n#> scripts/map.sh res/smpC_trim.fastq metadata/ref.fa > res/smpC.bam\n#> scripts/count.sh res/smpC.bam res/smpA.bam res/smpG.bam > res/count_table.txt\n\n4. Add a FastQC rule\nAs you may recall, FastQC performs quality control of FASTQ files (see these slides from week 6). FastQC should be run on the raw and/or trimmed FASTQ files. If you want, you can implement both, but the solution below will do so just for the raw FASTQ files.\nSave this dummy script as scripts/fastqc.sh:\n#!/bin/bash\nset -e -u -o pipefail\n\ninfile=$1\noutdir=$2\n\noutfile=\"$outdir\"/$(basename \"$infile\" .fastq).fastqc.html\n\necho \"Running FastQC for file: $infile , to output dir: $outdir\"\n\necho \"FastQC results for file $infile\" > \"$outfile\"\ncat \"$infile\" > \"$outfile\"\n\nMake scripts/fastqc.sh executable.\n\nSolution\n\nchmod u+x scripts/fastqc.sh\n\nFigure out what the script does and create your rule fastqc accordingly.\nAs the output dir, you can just use the res (results) dir we have been using, or optionally a subdir of that.\n\nHints\n\nThe script will write to an output file <output-dir>/<sample>.fastqc.html. That is, you don’t specify the output file to the script, only the output directory. This is like the behavior of the actual FastQC program.\nThe script will also print a line to standard out (i.e., not to the output file).\nIn the Snakefile rule, you do want an output directive, but you don’t reference {output} in the shell directive. Instead, you just provide the output dir as the second argument to the script.\n\nSolution\n\nrule fastqc:\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}.fastqc.html\",\n    shell:\n        \"scripts/fastqc.sh {input} res\"\n\n5. Run the FastQC rule\nWhat will happen now if you run Snakemake?\n(Without providing it a rule or file to produce.)\nThink about what the answer should be before you try.\n\nSolution\n\nIt will not run anything:\nsnakemake -j1 -pr\n#> Building DAG of jobs...\n#> Nothing to be done.\nThis is because the FastQC output files are not being used by any other rule, nor are they listed in rule all.\n\nWhat will happen if you ask Snakemake to specifically run the FastQC rule?\nThink about what the answer should be before you try.\n\nSolution\n\nYou will get a WorkflowError:\nsnakemake -j1 -pr fastqc\n#> WorkflowError:\n#> Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards.\nThe target rule referenced in the error is fastqc, which we targeted in the command-line call.\nThe problem is that rule fastqc contains wildcards and Snakemake can’t resolve what their values should be. This may be surprising but recall that Snakemake works backwards in the sense that it looks at output files first (rather than first inferring input files from existing files), and here, it does not know which output files to produce.\n\nHow, then, can you get Snakemake to run the FastQC rule? First, think just in terms of changing the Snakemake command-line call.\n\nSolution\n\nYou could specify a specific output file for one of the samples, e.g. res/smpA.fastqc.html:\nsnakemake -j1 -pq res/smpA.fastqc.html\n#> Job counts:\n#>         count   jobs\n#>         1       fastqc\n#>         1\n#> scripts/fastqc.sh data/smpA.fastq res\n#> Running FastQC for file: data/smpA.fastq , to output dir: res\nNote above that the line the script printed to standard output (Running FastQC for file: $infile , to output dir: $outdir) is indeed simply printed to standard out among the other output of Snakemake.\n\nHow can you change the Snakefile to ensure that FastQC will be run as a standard part of the workflow? Run Snakemake after you changed the Snakefile.\n(I.e., without having to invoke each output file separately, like above.)\n\nHints\n You should include the output files of the FastQC script in the input of rule all, using the expand() function.\n\nSolution\n\nAdd a line to rule all so that it becomes:\nrule all:\n    input:\n        \"res/count_table.txt\",\n         expand(\"res/{smp}.fastqc.html\", smp=SAMPLES)\nRerun Snakemake:\nsnakemake -pq\n\n#> Job counts:\n#>       count   jobs\n#>       1       all\n#>       2       fastqc\n#>       3\n#> scripts/fastqc.sh data/smpC.fastq res\n#> Running FastQC for file: data/smpC.fastq , to output dir: res\n#> scripts/fastqc.sh data/smpG.fastq res\n#> Running FastQC for file: data/smpG.fastq , to output dir: res\n\n6. When we get errors\nTrigger an error in one of the shell scripts:\nRemove all the output files of the workflow.\nChange rule map by removing {input.ref} from the shell directive.\nTry to rerun the workflow.\nMake sure to undo the error-inducing change to rule map you just made.\n\n\nSolution\n\nRemove the output files:\nrm res/*\nRule map should look as follows:\nrule map:\n    input:\n        fastq=\"res/{smp}_trim.fastq\",\n        ref=REF_FA,\n    output:\n        \"res/{smp}.bam\",\n    shell:\n        \"scripts/map.sh {input.fastq} > {output}\"\nRun the workflow:\nsnakemake -j1 -pr\n\n#> Error in rule map:\n#>     jobid: 2\n#>     output: res/smpG.bam\n#>     shell:\n#>         scripts/map.sh res/smpG_trim.fastq > res/smpG.bam\n#>         (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)\n#> \n#> Removing output files of failed job map since they might be corrupted:\n#> res/smpG.bam\n#> scripts/map.sh: line 6: $2: unbound variable\n#> [Fri Apr  2 18:33:26 2021]\n#> Error in rule map:\n#>     jobid: 4\n#>     output: res/smpC.bam\n#>     shell:\n#>         scripts/map.sh res/smpC_trim.fastq > res/smpC.bam\n#>         (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)\n#> \n#> Removing output files of failed job map since they might be corrupted:\n#> res/smpC.bam\n#> Shutting down, this might take some time.\n#> Exiting because a job execution failed. Look above for error message\nUndo the change:\nrule map:\n    input:\n        fastq=\"res/{smp}_trim.fastq\",\n        ref=REF_FA,\n    output:\n        \"res/{smp}.bam\",\n    shell:\n        \"scripts/map.sh {input.fastq} {input.ref} > {output}\"\n\nThe actual error produced by the bash script is what you would likely be interested in if troubleshooting:\nscripts/map.sh: line 6: $2: unbound variable.\nHowever, this was buried among all the other Snakemake output. Earlier, we saw that a line of standard output was similarly printed to screen among the Snakemake output. And the same would be true for logging information and errors produced by programs like the actual FastQC.\nThis is not optimal when errors do occur – in the next section, we will therefore use log files.\nCheck the output files. Are there output files at all for rule map?\n\nSolution\n\nNope! These were removed by Snakemake because of the errors. You may have noticed this in the error messages above, where it said, for example:\n#> Removing output files of failed job map since they might be corrupted:\n#> res/smpG.bam\n\n7. The log directive\nIt would be better to save logging and error output to “log” files for each rule – or each job within a rule, more precisely.\nFor example, we may be inclined to do the following for a script/program that:\nSaves output data to file, and prints logging/errors to screen (standard out and/or standard error – recall that &> redirects both):\nshell: \"myscript.py -i {input} -o {output} &> log/myscript.log\"\nPrints output data to standard out, and logging/errors to standard error (recall that 2> redirects standard error):\nshell: \"myscript.py -i {input} > {output} 2> log/myscript.log\"\nBut should we also tell Snakemake explicitly about such a file, like we do for input and output files? Yes, but it’s best to use a separate directive for this: log.\nThis is convenient because Snakemake treats log files differently than regular output files: if a job fails, Snakemake will not delete the log files. As we saw above, Snakemake does delete regular output files by jobs that produced errors, so if we were to designate logging output as regular output, we would not be able to examine our errors.\nExample usage of a log key – note that we can use any wildcards that are also used in {output}:\nrule myscript:\n    #[...]\n    output: res/{sample}.out\n    log: log/myscript_{sample}.log\n    shell: \"myscript.py -i {input} > {output} 2> {log}\"\n\nAdd a log directive to each rule and rerun Snakemake.\nYou’ll also have to redirect standard out and/or standard error (this will depend on the rule!) to the log file in each rule.\nUse a different dir for logging output than the res dir, e.g. log, perhaps best with subdirs for each rule.\nAfter you have run Snakemake, check that you now have a log file for each job – though most will be empty in the absence of errors; only the logs for the fastqc rule should contain something.\n\n\nSolution\n\nThe full Snakefile with log directives added to each rule (minus comments):\nSAMPLES = glob_wildcards(\"data/{smp}.fastq\").smp\n\nREF_FA = \"metadata/ref.fa\"\n\nrule all:\n    input:\n        \"res/count_table.txt\",\n         expand(\"res/{smp}.fastqc.html\", smp=SAMPLES)\n\n\nrule trim:\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}_trim.fastq\",\n    log:\n        \"log/trim/{smp}.log\",\n    shell:\n        \"scripts/trim.sh {input} >{output} 2>{log}\"\n\n\nrule map:\n    input:\n        fastq=\"res/{smp}_trim.fastq\",\n        ref=REF_FA,\n    output:\n        \"res/{smp}.bam\",\n    log:\n        \"log/map/{smp}.log\",\n    shell:\n        \"scripts/map.sh {input.fastq} {input.ref} >{output} 2>{log}\"\n\n\nrule count:\n    input:\n        expand(\"res/{smp}.bam\", smp=SAMPLES),\n    output:\n        \"res/count_table.txt\",\n    log:\n        \"log/count/count.log\",\n    shell:\n        \"scripts/count.sh {input} >{output} 2>{log}\"\n\n\nrule fastqc:\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}.fastqc.html\",\n    log:\n        \"log/fastqc/{smp}.log\",\n    shell:\n        \"scripts/fastqc.sh {input} res &>{log}\"\nRun Snakemake after removing old output:\nrm res/*\n\nsnakemake -j1 -pq\n\n#> Job counts:\n#>         count   jobs\n#>         1       all\n#>         1       count\n#>         3       fastqc\n#>         3       map\n#>         3       trim\n#>         11\n#> scripts/fastqc.sh data/smpG.fastq res &>log/fastqc/smpG.log\n#> scripts/fastqc.sh data/smpA.fastq res &>log/fastqc/smpA.log\n#> scripts/trim.sh data/smpG.fastq >res/smpG_trim.fastq 2>log/trim/smpG.log\n#> scripts/trim.sh data/smpC.fastq >res/smpC_trim.fastq 2>log/trim/smpC.log\n#> scripts/trim.sh data/smpA.fastq >res/smpA_trim.fastq 2>log/trim/smpA.log\n#> scripts/fastqc.sh data/smpC.fastq res &>log/fastqc/smpC.log\n#> scripts/map.sh res/smpA_trim.fastq metadata/ref.fa >res/smpA.bam 2>log/map/smpA.log\n#> scripts/map.sh res/smpG_trim.fastq metadata/ref.fa >res/smpG.bam 2>log/map/smpG.log\n#> scripts/map.sh res/smpC_trim.fastq metadata/ref.fa >res/smpC.bam 2>log/map/smpC.log\n#> scripts/count.sh res/smpG.bam res/smpC.bam res/smpA.bam >res/count_table.txt #> 2>log/count/count.log\nCheck that the log files are there:\ntree log\n\n#> log\n#> ├── count\n#> │   └── count.log\n#> ├── fastqc\n#> │   ├── smpA.log\n#> │   ├── smpC.log\n#> │   └── smpG.log\n#> ├── map\n#> │   ├── smpA.log\n#> │   ├── smpC.log\n#> │   └── smpG.log\n#> └── trim\n#>     ├── smpA.log\n#>     ├── smpC.log\n#>     └── smpG.log\nOnly the fastqc log files should have nonzero file sizes, which you can quickly check by running ls -lhR log.\n\n2: Running the workflow with SLURM jobs\n1. Introduction\nSince we are running dummy scripts, it has been fine to run Snakemake on a login node at OSC. For any real workflow, this would not work.\nYou have two main options:\nFor small workflows that can be run on one node or less than that, you can run Snakemake as a single SLURM job, either on an interactive node (especially if the workflow is particularly small) or by submitting a script with sbatch.\nSnakemake can orchestrate SLURM job submissions for you, in which case every Snakemake job (every instance of every rule) will be submitted as a separate job.\nJobs that depend on others will only be allowed to run when those other jobs have successfully finished, so you can submit a very complex and potentially long-running workflow with a single Snakemake call!\nBecause the main Snakemake process will keep running as long as one of its constituent jobs is, you will still need to run Snakemake itself as a SLURM job too. Even though this process may not take substantial computing resources, it may have to run for long time – and recall that at OSC, any process that runs for more than 20 minutes on a login node is killed.\n2. Submit Snakemake as a single SLURM job\nWrite a Bash script with SLURM instructions to run Snakemake.\nThe job should request 1 hour, 1 task, 1 core, should be billed to the classroom project PAS1855, and it should run our current Snakefile.\nSave the script as workflow/snakemake_submit.sh.\n\nSolution\n\n#!/bin/bash\n\n#SBATCH --account=PAS1855\n#SBATCH --time=60\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n\nset -e -u -o pipefail\n\nsnakemake -j1 -p\n\nSo far, we have been using -j1 (i.e. --jobs 1), such that Snakemake will only run one process at a time. Several of our rules run the same script separately for each sample, so those jobs could be run at the same time (in parallel).\nModify the bash script to parallellize the workflow.\nHow many simultaneous jobs (processes) would it make sense to run at the same time?\nWhich SLURM directive corresponds to Snakemake’s -j flag: --ntasks or --cpus-per-task?\nHow could you make sure there is no mismatch between the resources requested from SLURM and the number of jobs provided to Snakemake?\n\n\nHints\n\nFor the last question: you should use a SLURM environment variables. The environment variable in question is $SLURM_NTASKS.\n\nSolution\n\nIt would make sense to run 6 jobs at the same time:\nOur workflow currently runs 3 samples, with several rules therefore creating 3 separate jobs that could be run in parallel.\nBecause rule fastqc and rule map both operate on the raw FASTQ independently, they can also be run in parallel.\nNote that you could easily forego this assessment at OSC and just set a maximum that you think is fair, like a 100 jobs – but it is a useful exercise to think about this all the same.\nThe --ntasks directive effectively corresponds to Snakemake’s jobs (-j):\neach SLURM “task” is a separate process.\nIf we would instead increase --cpus-per-task, we would just allocate more CPUs/cores/threads to each individual process, but we want to run multiple processes in parallel.\n#!/bin/bash\n\n#SBATCH --account=PAS1855\n#SBATCH --time=1:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=6\n#SBATCH --cpus-per-task=1\n\nset -e -u -o pipefail\n\nsnakemake -j\"$SLURM_NTASKS\" -p  \n\nDelete the workflow’s output files and submit the SLURM job. Check the SLURM log file (slurm-<job-id>.out) to see if it worked.\n\nSolution\n\nrm -r res log/*\n\nsbatch workflow/snakemake_submit.sh\n\n3. Let Snakemake submit jobs for us\nAt its most basic, all you need to do is add --cluster sbatch to the Snakemake command, and Snakemake will automatically submit a SLURM job for each of its jobs. But recall that at OSC, we also always need to specify an account.\nSo, we could use the following to have Snakemake submit at most 3 jobs at a time:\nsnakemake -j6 --cluster \"sbatch --account=PAS1855\"\nDelete the workflow’s output files and run the command above.\n\n4. Use a “profile”\nFor simple and quick workflows, providing SLURM options on the command line like we did above is reasonable, but if we have several non-default options, this will start to become a painfully long command.\nFor instance, you may note that the SLURM log files have currently all ended up in your project’s root dir, where you probably don’t want them.\nWe can also specify these options in a configuration file using a “profile”. This will additionally make it easier to customize SLURM options on a per-rule basis.\nTo use a profile, we should first create a directory. The name of this directory is arbitrary, but since we are using it in the context of providing settings for SLURM jobs, something like slurm_profile would make sense:\nmkdir slurm_profile\nInside this directory, Snakemake will expect a file called config.yaml:\ntouch slurm_profile/config.yaml\nIn config.yaml, we can provide a string for the cluster key, much like we did previously with the --cluster argument on the command-line:\ncluster: \"sbatch --account={resources.account}\n                 --mem={resources.mem_mb}\n                 --output=log/slurm-{rule}_{wildcards}.out\"\n\ndefault-resources: [mem_mb=4000, account=PAS1855]\nAbove, we use {resources.<resource-name>} instead of actual values in the cluster key. Then, the values for each <resource-name> are specified for the default-resources key. This setup is convenient because it allows us to also refer to the same resources in the Snakefile to set rule-specific values, as we’ll see a little later.\nCreate the profile directory and config file as shown above, and additionally specify requests for time (in minutes) and number of CPUs.\nNote that these resource requests are now a on a per-job basis, not for the whole workflow combined.\n\nSolution\n\nContents of slurm_profile/config.yaml:\ncluster: \"sbatch --account={resources.account}\n                 --time={resources.time_min}\n                 --mem={resources.mem_mb}\n                 --cpus-per-task={resources.cpus}\n                 --output=log/slurm-{rule}_{wildcards}.out\"\ndefault-resources: [cpus=1, mem_mb=1000, time_min=5, account=PAS1855]\n\nTo run Snakemake with the profile, we need to use the option --profile, and specify our profile dir: --profile <profile_dir>.\nRemove the old output and run Snakemake with the profile.\n\nSolution\n\nrm -r res log/*\n\nsnakemake -j6 --profile slurm_profile\n\n\nAbove, we also specified an output dir and names for our SLURM log files. If this directory does not already exist, make sure you create it manually, because Snakemake will not create it, and mysterious failures will occur!\nSo, Snakemake will create directories specified in the Snakefile as output, but won’t create directories that are only mentioned in this config file.\n\n\n4. Add options to the profile config file\nconfig.yaml can contain not just cluster settings, but anything that can be set with command-line options.\nWe can take that opportunity to also:\nSpecify the number of jobs (--jobs option).\nMake Snakemake wait longer for output files to appear using the latency-wait option: units are in seconds and we want 30 second.\n(I’ve had errors with shorter latency times.)\nAdd key-value pairs for the number of jobs and the latency to the config file.\n\nHints\n\nNote that in YAML, key-value pairs are specified as key:value, not --key value.\n\nSolution\n\njobs: 100\nlatency-wait: 30\nThe full slurm_profile/config.yaml should now contain the following:\ncluster: \"sbatch --account={resources.account}\n                 --time={resources.time_min}\n                 --mem={resources.mem_mb}\n                 --cpus-per-task={resources.cpus}\n                 --output=log/slurm-{rule}_{wildcards}.out\"\ndefault-resources: [cpus=1, mem_mb=1000, time_min=5, account=PAS1855]\njobs: 100\nlatency-wait: 30\n\nRun Snakemake with the updated profile.\nRecall that now, you don’t have to specify -j / --jobs at the command line.\n\nPay attention to the job submission pattern of Snakemake. You can also open up another terminal and monitor your jobs:\nsqueue -u $USER -l\n\n\nSolution\n\nrm -r res log/*\n\nsnakemake --profile slurm_profile\n\n5. Local rules\nIn practice, you may not want to submit a cluster job for every rule.\nFor instance, rule all is a Snakemake job, but it doesn’t actually run anything the way we have set it up. Additionally, you may have some very lightweight cleaning/logging rules.\nTo tell Snakemake that certain rules should not be submitted to the cluster, include a comma-separated list of rules near the top of your Snakefile with the localrules key:\nlocalrules: all, clean\nAdd a localrules directive with rule all, and run Snakemake again.\n\nSolution\n\nYour full Snakefile should now contain the following:\nSAMPLES = glob_wildcards(\"data/{smp}.fastq\").smp\n\nREF_FA = \"metadata/ref.fa\"\n\nlocalrules: all\n\nrule all:\n    input:\n        \"res/count_table.txt\",\n         expand(\"res/{smp}.fastqc.html\", smp=SAMPLES)\n\n\nrule trim:\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}_trim.fastq\",\n    log:\n        \"log/trim/{smp}.log\",\n    shell:\n        \"scripts/trim.sh {input} >{output} 2>{log}\"\n\n\nrule map:\n    input:\n        fastq=\"res/{smp}_trim.fastq\",\n        ref=REF_FA,\n    output:\n        \"res/{smp}.bam\",\n    log:\n        \"log/map/{smp}.log\",\n    shell:\n        \"scripts/map.sh {input.fastq} {input.ref} >{output} 2>{log}\"\n\n\nrule count:\n    input:\n        expand(\"res/{smp}.bam\", smp=SAMPLES),\n    output:\n        \"res/count_table.txt\",\n    log:\n        \"log/count/count.log\",\n    shell:\n        \"scripts/count.sh {input} >{output} 2>{log}\"\n\n\nrule fastqc:\n    input:\n        \"data/{smp}.fastq\",\n    output:\n        \"res/{smp}.fastqc.html\",\n    log:\n        \"log/fastqc/{smp}.log\",\n    shell:\n        \"scripts/fastqc.sh {input} res &>{log}\"\nrm -r res log/*\n\nsnakemake --profile slurm_profile\n\n7. Rule-specific resources: threads\nBecause the number of threads is a parameter that commonly differs between rules, Snakemake has a threads directive that we can add to any rule. This will tell Snakemake how many threads it should use for any single job that the rule in question generates.\nWhich SLURM directive does this correspond to?\n\nHints\n It is either --cpus-per-task or --ntasks.\n\nSolution\n\nThe threads directive in a Snakefile corresponds to the --cpus-per-task SLURM directive. (Recall that SLURM uses the terms CPU, cores and threads roughly equivalently.)\nWhen specifying multiple --cpus-per-task (SLURM) / threads (Snakemake), you are allowing a single process (task) to perform multithreading.\n\nWhen using the threads directive, we will generally also tell the program itself about the number of threads – see the example below, where we are calling the STAR aligner directly in a Snakemake rule:\nrule STAR:\n    threads: 8\n    shell: \"STAR --runThreadN {threads} ...\"\nFor now, we’ll keep it simple and merely use the threads directive:\nSpecify 4 threads for rule map.\n\nSolution\n\nRule map should now contain the following:\nrule map:\n    input:\n        fastq=\"res/{smp}_trim.fastq\",\n        ref=REF_FA,\n    output:\n        \"res/{smp}.bam\",\n    threads: 4\n    log:\n        \"log/map/{smp}.log\",\n    shell:\n        \"scripts/map.sh {input.fastq} {input.ref} >{output} 2>{log}\"\n\n8. Rule-specific resources: other\nWe can use a resources key for any rule to specify (mostly) arbitrary key-value pairs with resources:\nrule heavy_stuff:\n    # [...]\n    resources: mem_mb=50000\n    # [...]\nThese are arbitrary in the sense that mem_mb will not directly set the actual maximum memory usage in the way that the threads directive did set the number of threads. Instead, they refer to the keys we use in our config.yaml.\nFor example, to actually request 50,000 MB of memory for rule heavy_stuff, mem_mb should correspond to the term used in the SLURM resource request in config.yaml:\ncluster: \"sbatch --mem={resources.mem_mb} ...\"\ndefault-resources: [mem_mb=4000, ...]\nSetting resources: mem_mb=50000 for rule heavy_stuff will then override the default value of 4000, and pass that on the SLURM job request!\nSet the time request for rule map to 2 hours.\n\nSolution\n\nAssuming time is referred to as time_min in slurm_profile/config.yaml (this has to match!), rule map should now contain the following:\nrule map:\n    input:\n        fastq=\"res/{smp}_trim.fastq\",\n        ref=REF_FA,\n    output:\n        \"res/{smp}.bam\",\n    threads: 8\n    resources: time_min=120\n    log:\n        \"log/map/{smp}.log\",\n    shell:\n        \"scripts/map.sh {input.fastq} {input.ref} >{output} 2>{log}\"\n\nHow could you check whether requests such as your rule-specific time limit are successfully passed on the the SLURM job?\n\nSolution\n\nThere are a few different options:\nYou can look up (go to “input environment variables”) and echo SLURM environment variables in the script. For the time limit specifically, this variable is $SBATCH_TIMELIMIT, so you could include the following in your scripts/map.sh:\necho \"Time limit: $SBATCH_TIMELIMIT\"\nYou can print an overview of resources and statistics for your job using scontrol show job <job-ID>. You can do that manually in the shell for a specific SLURM job, or include the following in your script, to have this information include in the SLURM log file:\nscontrol show job $SLURM_JOB_ID\nThe allotted time specifically is also shown when you issue squeue with the -l (long) option:\nsqueue -u $USER -l\n\n\nNote that it doesn’t seem to be possible to install this extension in VS Code Server at OSC, but you can install it locally.↩︎\nThis may look a little strange (but think 1-element tuple!), yet is somewhat useful: when every line has a trailing comma, you can always add and remove lines without paying attention to whether a new item follows.↩︎\n",
      "last_modified": "2021-04-25T17:52:17-04:00"
    },
    {
      "path": "w13_readings.html",
      "title": "Week 13 content overview and readings",
      "author": [],
      "date": "2021-04-12",
      "contents": "\nContent overview for this week\nSince Snakemake is a Python program, its syntax is very similar to regular Python and we can even include plain Python code! Our previous experience with Python therefore puts us in a really good position to learn Snakemake.\nWhile its learning curve is not trivial even with all of our pre-existing knowledge, there are many advantages to using Snakemake when you have analysis pipelines that include at least a few scripts. For example, it makes our analysis pipelines more reproducible, portable, scalable, and transparent than regular Bash or Python scripts that glue together a pipeline. In addition, Snakemake takes care of a lot of boilerplate code for you, such as SLURM directives in shell scripts; and you can even forego using shell scripts altogether and specify all shell commands in a “Snakefile” (workflow script) directly. Finally, Snakemake can be an incredible timesaver when errors occur in your pipeline or you need to repeat parts of it – which is common!\nSome of the things you will learn this week:\nWhat challenges you can (will) run into when trying to glue analysis pipelines together with regular Bash (or Python) scripts.\nWhat Workflow Management Systems are, and why they are useful.\nThe basics of using Snakemake to describe and run workflows.\nHow to get Snakemake to submit SLURM jobs for you.\nReadings\nSince neither of the books cover Snakemake, you’ll be reading two articles: a journalistic feature article in Nature as a light introduction to workflow systems, and a manuscript by some of the authors of Snakemake.\nIn the latter article, I recommend you read until you reach the heading “2.2.1 Modularization” on page 7, and beyond there, you can skim/read/skip as you see fit.\nYou may also want to (re)visit the section “Make and Makefiles: Another Option for Pipelines” in the Buffalo book, Chapter 12, p. 421-423. Snakemake was heavily inspired by Make (hence its name) but is much more user-friendly and has many more options.\nRequired readings\nPerkel 2019, Nature “Toolbox” feature: “Workflow systems turn raw data into scientific knowledge”\nMölder et al. 2020, Zenodo: “Sustainable data analysis with Snakemake”\nFurther resources\nBuffalo\nThe official Snakemake tutorial which you can run in your browser!\nA short introduction to (Make and then) Snakemake by Vince Buffalo, the author of the secondary book we have been reading.\nA Carpentries lesson on working with a compute cluster, with a large section on Snakemake starting on this page.\nAnother useful Snakemake tutorial.\n\n\n\n",
      "last_modified": "2021-04-25T17:52:18-04:00"
    },
    {
      "path": "w14_readings.html",
      "title": "Week 14 readings",
      "author": [],
      "contents": "\nThis week’s readings are all optional.\nThe choice of the first chapter for the Buffalo book may seem strange, but much of what is written there would have been a better fit for the end of the book, in my opinion – and should be a useful read.\nDespite the title suggesting a focus on workflow systems only, the Reiter et al. paper is broader and covers and ties together many of the topics discussed in this course.\nOptional readings\nCSB Chapter 11: “Wrapping up”\nBuffalo Chapter 1: “How to learn Bioinformatics”\nReiter et al. 2021, GigaScience: “Streamlining data-intensive biology with workflow systems”\n\n\n\n",
      "last_modified": "2021-04-25T17:52:18-04:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
