{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "author": [],
      "contents": "\nThis is the Github website for the course Practical Computing Skills for Biologists (a section of PP8300 – Current Topics in Plant Pathology), a 2-credit online-only course at Ohio State University during the Spring semester of 2021. The course is taught by Jelmer Poelstra from the MCIC.\nOnly slide decks, code-along materials, and exercises are hosted on this website. If you are a student in this course, your starting point should always be the CarmenCanvas site for this course. (Note that you can always find the link to the CarmenCanvas site in the top-right corner of this site by clicking on the graduation cap icon.)\n\n\n\n",
      "last_modified": "2021-01-26T08:03:56-05:00"
    },
    {
      "path": "index.html",
      "title": "",
      "author": [],
      "contents": "\n\n\nPractical Computing Skills for Biologists  Spring 2021  A section of Current Topics in Plant Pathology (PLNTPTH 8300)\n\n\n\n\n",
      "last_modified": "2021-01-26T08:03:56-05:00"
    },
    {
      "path": "w01_exercises.html",
      "title": "Week 1 Exercises",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nGetting set up\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\nSolutions\nIntermezzo 1.1\nIntermezzo 1.2\nIntermezzo 1.3\n1.10.1 Next-Generation Sequencing Data\n1.10.2 Hormone Levels in Baboons\n1.10.3 Plant–Pollinator Networks\n1.10.4 Data Explorer\n\n\nThe following exercises were copied from Chapter 1 of the CSB book, with hints and solutions modified from those provided in the book’s Github repo.\n(The exercises marked as “Advanced” are omitted since they require topics not actually covered in the chapter, which we will cover in later modules.)\n\nMain exercises\nGetting set up\nYou should already have the book’s Github repository with exercise files.\nIf not, go to /fs/ess/PAS1855/users/$USER, and run git clone https://github.com/CSB-book/CSB.git.\nNow, you should have the CSB directory referred to in the first step of the first exercise.\nIntermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\nShow hints\nRecall that .. is one level up in the dir tree, and that you can combine multiple .. in a single statement.\n\n(d) Use an absolute path to go to the sandbox directory within python.\n(e) Return to the data directory within the python directory.\nShow hints\nRecall that there is a shortcut to return to your most recent working dir.\n\nIntermezzo 1.2\nTo familiarize yourself with these basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\nShow hints\nThe touch command creates an empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt.\nIntermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? Which is the last?\nShow hints\nYou can either first select the 5th column using cut, and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter.\nTo view just the first or the last line, pipe to head or tail.\n\n(b) How many families are represented in the database?\nShow hints\nStart by selecting the relevant column.\nUse a tail trick shown in the chapter to exclude the first line.\nRemember to sort before using uniq.\n1.10.1 Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (>).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\nShow hints\nTo show the size of a file you can use the -l option of the command ls, and to display human-readable file sizes, the -h option.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\nShow hints\nRecall that with the cp command, it is also possible to give the copy a new name at once.\n\n4. How many contigs are classified as isogroup00036?\nShow hints\nIs there a grep option that counts the number of occurrences?\nAlternatively, you can pass the output of grep to wc -l.\n\n5. Replace the original “two-spaces” delimiter with a comma.\nShow hints\nIn the file, the information on each contig is separated by two spaces:\n>contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n>contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\nIn Linux, we can’t write output to a file that also serves as input (see here), so the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" > myfile.txt # Don't do this! \nIn this case, we will have to save the output in a temporary file and on a separate line, overwrite the original file using mv.\n\n6. How many unique isogroups are in the file?\nShow hints\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nShow hints\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically.\n\n\nBonus exercises\n1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nHow many times were the levels of individuals 3 and 27 recorded?\nShow hints\nYou can use cut to extract just the maleID from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”.\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nShow hints\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written.\nIn the script, you can use the first two so-called positional parameters, $1 and $2, to represent the file name and the maleID, respectively.\n$1 and $2 represent the first and the second argument passed to a script like so: bash myscript.sh arg1 arg2.\n1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\n$ bash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\nShow hints\nTo build the script, you need to combine several commands:\nTo find the number of rows, you can use wc.\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n, and count characters.\n1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\nFor example, running the script with:\n$ bash explore.sh ../data/Buzzard2015_data.csv 7\nshould return:\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\nShow hints\nYou can select a given column from a csv file using the command cut. Then:\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the positional parameters $1 and $2 for the file name and column number, respectively.\n\n\nSolutions\nIntermezzo 1.1\nSolution\n(a) Go to your home directory. Go to /fs/ess/PAS1855/users/$USER.\n$ cd /fs/ess/PAS1855/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS1855/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\ncd -\n\nIntermezzo 1.2\nSolution\n(a) Go to the data directory within CSB/unix.\n$ cd /fs/ess/PAS1855/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory         without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\nIntermezzo 1.3\nSolution\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv\nin alphabetical order, which is the first species? And which is the last?\ncd ∼/CSB/unix/data/\n\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\n\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\n\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t\";\" -k 5 Pacifici2013_data.csv | head -n 1\n\n\nFollowing the output that you wanted, you may have gotten errors like this:\nsort: write failed: 'standard output': Broken pipe\nsort: write error\nThis may seem disconcerting, but is nothing to worry about, and has to do with the way data streams through a pipe: after head/tail exits because it has done what was asked (print one line), sort or cut may still try to pass on data.\n\n\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n\n1.10.1 Next-Generation Sequencing Data\n1. Change directory to CSB/unix/sandbox.\ncd /fs/ess/PAS1855/users/$USER/CSB/unix/sandbox\n\n2. What is the size of the file Marra2014_data.fasta?\nls -lh ../data/Marra2014_data.fasta\n# Among other output, this should show a file size of 553K\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\ncp ../data/Marra2014_data.fasta my_file.fasta\nTo make sure the copy went well, list the files in the sandbox:\nls\n\n4. How many contigs are classified as isogroup00036?\nTo count the occurrences of a given string, use grep with the option -c\ngrep -c isogroup00036 my_file.fasta \n# 16\nYou can also use a pipe and wc -l to count:\ngrep isogroup00036 my_file.fasta | wc -l\n\n5. Replace the original “two-spaces” delimiter with a comma.\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,:\ncat my_file.fasta | tr -s ' ' ',' > my_file.tmp\nmv my_file.tmp my_file.fasta\n\n6. How many unique isogroups are in the file?\nFirst, searching for > with grep will extract all lines with contig information:\ngrep '>' my_file.fasta | head -n 2\n# >contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n# >contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, use cut to extract the 4th column\ngrep '>' my_file.fasta | cut -d ',' -f 4 | head -n 2\n#gene=isogroup00001\n#gene=isogroup00001\nFinally, use sort -> uniq -> wc -l to count the number of unique occurrences:\ngrep '>' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n# 43\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\nWe need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n# >contig00001,numreads=2\n# >contig00002,numreads=8\n# >contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t ‘=’ to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n# >contig00089,numreads=1\n# >contig00176,numreads=1\n# >contig00210,numreads=1\n# >contig00001,numreads=2\n# >contig00003,numreads=2\nUsing the flag -r we can sort in reverse order:\ngrep '>' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n# >contig00302,numreads=3330\nFinding that contig 00302 has the highest coverage, with 3330 reads.\n\n\n1.10.2 Hormone Levels in Baboons\n1. How many times were the levels of individuals 3 and 27 recorded?\nFirst, let’s see the structure of the file:\nhead -n 3 ../data/Gesquiere2011_data.csv \n# maleID        GC      T\n# 1     66.9    64.57\n# 1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 ../data/Gesquiere2011_data.csv | head -n 3\n# maleID\n# 1\n# 1\nThen we can pipe the results to grep -c to count the number of occurrences (note the flag -w as we want to match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 3\n# 61\n\n# For maleID 27\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 27\n# 5\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\nIn the script, we just need to incorporate the arguments given when calling the script, using $1 for the file name and $2 for the individual ID, into the commands that we used above:\ncut -f 1 $1 | grep -c -w $2\nA slightly more verbose and readable example:\n#!/bin/bash\n\nfilename=$1\nind_ID=$2\necho \"Counting the nr of occurrences of individual ${ind_ID} in file ${filename}:\" \ncut -f 1 ${filename} | grep -c -w ${ind_ID}\n\n\nVariables are assigned using name=value (no dollar sign!), and recalled using $name or ${name}. It is good practice to put curly braces around the variable name. We will talk more about bash variables in the next few weeks.\n\n\nTo run the script, assuming it is named count_baboons.sh:\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n1.10.3 Plant–Pollinator Networks\nSolution\nCounting rows:\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\nTo avoid printing the file name, we can either use cat or input redirection:\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l < ../data/Saavedra2013/n10.txt \nCounting rows:\nCounting the number of columns is more work. First, we need only the first line:\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\nNow we can remove all spaces and the line terminator using tr:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\nFinally, we can use wc -c to count the number of characters in the string:\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nFinal script:\n#!/bin/bash\n\nfilename=$1\n\necho \"Filename:\"\necho ${filename}\necho \"Number of rows:\"\ncat ${filename} | wc -l\necho \"Number of columns:\"\nhead -n 1 ${filename} | tr -d ' ' | tr -d '\\n' | wc -c\nTo run the script, assuming it is named counter.sh:\nbash counter.sh ../data/Saavedra2013/n10.txt\n# 5\n\n\nWe’ll learn about a quicker and more general way to count columns in a few weeks.\n\n\n\n1.10.4 Data Explorer\nSolution\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\nHere is an example of what the script could look like:\n#!/bin/bash\n\nfilename=$1\ncolumn_nr=$2\n\necho \"Column name\"\ncut -d ',' -f ${column_nr} ${filename} | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n",
      "last_modified": "2021-01-26T08:03:57-05:00"
    },
    {
      "path": "w02_exercises.html",
      "title": "Exercises: Week 2",
      "author": [],
      "contents": "\n\nContents\nMain exercises\nExercise 1: Course notes in Markdown\nExercise 2\n\nBonus exercises\nExercise 3\nBuffalo Chapter 3 code-along\n\nSolutions\nExercise 2\n\n\nMain exercises\nExercise 1: Course notes in Markdown\nCreate a Markdown document with course notes. I recommend writing this document in VS Code.\nMake notes of this week’s material in some detail. If you have notes from last week in another format, include those too. (And try to keep using this document throughout the course!)\nSome pointers:\nUse several header levels and use them consistently: e.g. a level 1 header (#) for the document’s title, level 2 headers (##) for each week, and so on.\nThough this should foremost be a functional document for notes, try to incorporate any appropriate formatting option: e.g. bold text, italic text, inline code, code blocks, ordered/unordered lists, hyperlinks, and perhaps a figure.\nMake sure you understand and try out how Markdown deals with whitespace, e.g. starting a new paragraph and how to force a newline.\nExercise 2\nWhile doing this exercise, save the commands you use in a text document – either write in a text document in VS Code and send the commands to the terminal, or copy them into a text document later.\nGetting set up\nCreate a directory for this exercise, and change your working dir to go there. You can do this either in your $HOME dir (e.g. ~/pracs-sp21/w02/ex2/) or your personal dir in the course’s project dir (/fs/ess/PAS1855/users/$USER/w02/ex2/).\nCreate a disorganized mock project\nUsing the touch command and brace expansions, create a mock project by creating 100s of empty files, either in a single directory or a disorganized directory structure.\nIf you want, you can create file types according to what you typically have in your project – otherwise, a suggestion is to create files with:\nRaw data (e.g. .fastq.gz)\nReference data (e.g. .fasta),\nMetadata (e.g. .txt or .csv)\nProcessed data and results (e.g. .bam, .out)\nScripts (e.g. .sh, .py or .R)\nFigures (e.g. .png or .eps)\nNotes (.txt and/or .md)\nPerhaps some other file type you usually have in your projects.\n\nOrganize the mock project\nOrganize the mock project according to some of the principles we discussed this week.\nEven while adhering to these principles, there is plenty of wiggle room and no single perfect dir structure: what is optimal will depend on what works for you and on the project size and structure. Therefore, think about what makes sense to you, and what makes sense given the files you find yourself with.\nTry to use as few commands as possible to move the files – use wildcards!\nChange file permissions\nMake sure no-one has write permissions for the raw data files. You can also change other permissions to what you think is reasonable or necessary precaution for your fictional project.\nHints\nUse the chmod command to change file permissions and recall that you can use wildcard expansion to operate on many files at once.\nSee the slides starting from here for an overview of file permissions and the chmod command.\nAlternatively, chmod also has an -R argument to act recursively: that is, to act on dirs and all of their contents (including other dirs and their contents).\n\nCreate mock alignment files\nCreate a directory alignment inside an appropriate dir in your project (e.g. analysis, results, or a dir for processed data), and create files for all combinations of 30 samples (01-30), 5 treatments (A-E), and 2 dates (08-14-2020 and 09-16-2020), like so: sample01_A_08-14-2020.sam - sample50_H_09-16-2020.sam.\nThese 300 files can be created with a single touch command. (If you already had an alignment dir, first delete its contents or rename it.)\nHints\nUse brace expansion three times: to expand sample IDs, treatments, and dates.\nNote that {01..20} will successfully zero-pad single-digit numbers.\n\nRename files in a batch\nWoops! We stored the alignment files that we created in the previous step as SAM files (.sam), but this was a mistake – the files are actually the binary counterparts of SAM files: BAM files (.bam).\nMove into the dir with your misnamed BAM files, and use a for loop to rename them: change the extension from .sam to .bam.\nHints\nLoop over the files using globbing (wildcard expansion) directly; there is no need to call ls.\nUse the basename command, or alternatively, cut, to strip the extension.\nStore the output of the basename (or cut) call using command substitution ($(command) syntax).\nThe new extension can simply be pasted behind the file name, like newname=\"$filename_no_extension\"bam or newname=$(basename ...)bam.\nCopy files with wildcards\nStill in the dir with your SAM files, create a new dir called subset. Then, using a single cp command, copy files that satisfy the following conditions into the subset dir:\nThe sample ID/number should be 01-19;\nThe treatment should be A, B, or C.\nCreate a README.md in the dir that explains what you did.\nHints\nJust like you used multiple consecutive brace expansions above, you can use two consecutive wildcard character sets ([]) here.\n\nBonus: a trickier wildcard selection\nStill in the dir with your SAM files, create a new dir subset2. Then, copy all files except the one for “sample28” into this dir.\nDo so using a single cp command, though you’ll need two separate wildcard expansion or brace expansion arguments (as in cp wildcard-selection1 wildcard-selection2 destination/).\nHints\nWhen using brace expansion ({}), simply use two numeric ranges: one for IDs smaller than and one for IDs larger than the focal ID.\nWhen trying to do this with wildcard character sets ([]), you’ll run into one of its limits: you can’t combine conditions with a logical or. Therefore, to exclude only sample 28, you have to separately select IDs that (1) do not start with a 2 and (2) start with a 2 but do not end with an 8.\nBonus: a trickier renaming loop\nYou now realize that your date format is suboptimal (MM-DD-YYYY; which gave 08-14-2020 and 09-16-2020) and that you should use the YYYY-MM-DD format. Use a for loop to rename the files.\nHints\nUse cut to extract the three elements of the date (day, month, and year) on three separate lines.\nStore the output of these lines in variables using commands substitution, like: day=$(commands).\nFinally, paste your new file name together like: newname=\"$part1\"_\"$year\" etc.\nWhen first writing your commands, it’s helpful to be able to experiment easily: start by echo-ing a single example file name, as in: echo sample23_C_09-16-2020.sam | cut ....\nCreate a README\nInclude a README.md that described what you did; again, try to get familiar with Markdown syntax by using formatting liberally.\nBonus exercises\nExercise 3\nIf you feel like it would be good to reorganize one of your own, real projects, you can do so using what you’ve learned this week. Make sure you create a backup copy of the entire project first!\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3.\n\nSolutions\nExercise 2\n1. Getting set up\nmkdir ~/pracs-sp21/w02/ex2/ # or similar, whatever dir you chose\ncd !$                       # !$ is a shortcut to recall the last argument from the last commands\n\n2. Create a disorganized mock project\nAn example:\ntouch sample{001..150}_{F,R}.fastq.gz\ntouch ref.fasta ref.fai\ntouch sample_info.csv sequence_barcodes.txt\ntouch sample{001..150}{.bam,.bam.bai,_fastqc.zip,_fastqc.html} gene-counts.tsv DE-results.txt GO-out.txt\ntouch fastqc.sh multiqc.sh align.sh sort_bam.sh count1.py count2.py DE.R GO.R KEGG.R\ntouch Fig{01..05}.png all_qc_plots.eps weird-sample.png\ntouch dontforget.txt README.md README_DE.md tmp5.txt\ntouch slurm-84789570.out slurm-84789571.out slurm-84789572.out\n\n3. Organize the mock project\nAn example:\nCreate directories:\nmkdir -p data/{raw,meta,ref}\nmkdir -p results/{alignment,counts,DE,enrichment,logfiles,qc/figures}\nmkdir -p scripts\nmkdir -p figures/{ms,sandbox}\nmkdir -p doc/misc\nMove files:\nmv *fastq.gz data/raw/\nmv ref.fa* data/ref/\nmv sample_info.csv sequence_barcodes.txt data/meta/\nmv *.bam *.bam.bai results/alignment/\nmv *fastqc* results/qc/\nmv gene-counts.tsv results/counts/\nmv DE-results.txt results/DE/\nmv GO-out.txt results/enrichment/\nmv *.sh *.R *.py scripts/\nmv README_DE.md results/DE/\nmv Fig[0-9][0-9]* figures/ms\nmv weird-sample.png figures/sandbox\nmv all_qc_plots.eps results/qc/figures/\nmv dontforget.txt tmp5.txt doc/misc/\nmv slurm* results/logfiles/\n\n4. Change file permissions\nTo ensure that no-one has write permission for the raw data, you could, for example, use:\nchmod a=r data/raw/*   # set permissions for \"a\" (all) to \"r\" (read)\n\nchmod a-w data/raw/*   # take away \"w\" (write) permissions for \"a\" (all)\n\n5. Create mock alignment files\n$ mkdir -p results/alignment\n$ # rm results/alignment/* # In the example above, we already had such a dir with files\n$ cd results/alignment \n\n# Create the files:\n$ touch sample{01..30}_{A..E}_{08-14-2020,09-16-2020}.sam\n\n# Check if we have 300 files:\n$ ls | wc -l\n# 300\n\n6. Rename files in a batch\n# cd results/alignment  # If you weren't already there\n\n# Use *globbing* to match the files to loop over (rather than `ls`):\nfor oldname in *.sam\ndo\n   # Remove the `sam` suffix using `basename $oldname sam`,\n   # use command substitution (`$()` syntax) to catch the output of the\n   # `basename` command, and paste `bam` at the end:\n   newname=$(basename $oldname sam)bam\n   \n   # Report what we have:\n   # (Using `-e` with echo we can print an extra newline with '\\n`,\n   # to separate files by an empty line)\n   echo \"Old name: $oldname\"\n   echo -e \"New name: $newname \\n\"\n   \n   # Execute the move:\n   mv \"$oldname\" \"$newname\"\ndone\nA couple of take-aways:\nNote that we don’t need a special construction to paste strings together. we simply type bam after what will be the extension-less file name from the basename command.\nWe print the old and new names to screen; this is not necessary, of course, but good practice. Moreover, this way we can test whether our loop works before adding the mv command.\nWe use informative variable names (oldname and newname), not cryptic ones like i and o.\n7. Copy files with wildcards\nCreate the new dir:\nmkdir subset\nCopy the files using four consecutive wildcard selections:\nThe first digit should be a 0 or a 1 [0-1],\nThe second can be any number [0-9] (? would work, too),\nThe third, after an underscore, should be A, B, or C [A-C],\nWe don’t care about what comes after that, but do need to account for the additional characters, so will use a * to match any character:\n\ncp sample[0-1][0-9]_[A-C]* subset/\nReport what we did, including a command substitution to insert the current date:\necho \"On $(date), created a dir \"subset\" and copied only files for samples 1-29 \\\nand treatments A-D into this dir\" > subset/README.md\n\n# See if it worked:\ncat subset/README.md\n\n8. Bonus: a trickier wildcard selection\nCreate the new dir:\nmkdir subset2\nThe most straightforward way in this case is using two brace expansion selections, one for sample numbers smaller than 28, and one for sample numbers larger than 28:\ncp sample{01..27}* sample{29..30}* subset2/\nHowever, we may not always be able to use ranges like that, and being a little creative with wildcard expansion also works — first, we select all samples not starting with a 2, and then among samples that do start with a 2, we exclude 28:\ncp sample[^2]* sample2[^8]* subset2/\n\n9. Bonus: a trickier renaming loop\nfor oldname in *.bam\ndo\n   # Use `cut` to extract month, day, year, and a \"prefix\" that contains\n   # the sample number and the treatment, and save these using command substitution:\n   month=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 1)\n   day=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 2)\n   year=$(basename \"$oldname\" .bam | cut -d \"_\" -f 3 | cut -d \"-\" -f 3)\n   prefix=$(echo \"$oldname\" | cut -d \"_\" -f 1-2)\n   \n   # Paste together the new name:\n   # (This will fail without quotes around prefix, because the underscore\n   # is then interpreted as being part of the variable name.)\n   newname=\"$prefix\"_\"$year\"-\"$month\"-\"$day\".bam\n   \n   # Report what we have:\n   echo \"Old name: $oldname\"\n   echo -e \"New name: $newname \\n\"\n   \n   # Execute the move:\n   mv \"$oldname\" \"$newname\"\ndone\n\n\nThis renaming task can be done more succinctly using regular expressions and the sed command – we’ll learn about both of these topics later in the course.\n\n\n\n\n\n\n",
      "last_modified": "2021-01-26T08:03:57-05:00"
    },
    {
      "path": "w02_UA_github-signup.html",
      "author": [],
      "contents": "\nWhat?\nCreate a Github account and let me know you’ve done so.\nWhy?\nGithub is a website that hosts git repositories, i.e. version-controlled projects. In Module 3 of this course, we will be learning how to use git together with Github. Also, all graded assignments for this course will be submitted through Github. I will need to know your Github user names in advance to set up the infrastructure for this.\nHow?\nIf you already have a Github account, log in and start at step 6.\nGo to https://github.com. Click “Sign Up” in the top right. Fill out the form: When choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use Github to share your code, for instance when publishing a paper.) You can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.) You probably want to uncheck the box under Email Preferences. When you’re done, click “Create account”. You can answer the questions Github will now ask you, but you should also be able to just skip them. Check your email and click the link to verify your email address. In the far top-right of the page back on Github, click your randomly assigned avatar, and in the dropdown menu, click “Settings”. In the Emails tab (left-hand menu), deselect the box “Keep my email addresses private”. In the Profile tab, enter your Name. Still in the Profile tab, upload an avatar. This can be a picture of yourself but if you prefer, you can use something else. Here at this CarmenCanvas assignment, submit the link to your Github profile, which will be “https://github.com/”.\n\n\n\n",
      "last_modified": "2021-01-26T08:03:58-05:00"
    },
    {
      "path": "w03_GA_git.html",
      "title": "Graded Assignment I: Shell, Markdown, and Git",
      "author": [],
      "contents": "\n\nContents\nIntroduction\nPart I – Create a new Git repo\nPart II – Add some Markdown content\nPart III – Add some “data” files\nPart IV – Renaming files on a new branch\nPart V – Create and sync an online version of the repo\n\n\nIntroduction\nIn this assignment, you will primarily be practicing your Git skills. You will also show that you can write in Markdown and can write a fo loop to rename files.\nThe instructions below will take you through it step-by-step. All of the the steps involve things that we have gone through in class and/or that you have practiced with in last week’s exercises.\nGrading information:\nThe number of points (if any) that you can earn for each step are denoted in cursive between square brackets (e.g. [0.5]). In total, you can earn 10 points with this assignment, which is 10% of your grade for the class.\nIf you make a mistake like an unnecessary commit, or something that requires you to subsequently make an extra commit: this is no problem. Points will not be subtracted for extra commits as long as you indicate what each commit is for & I’m still able to figure out which commits are the ones that I requested.\nPoints are not subtracted for minor things like typos in commit messages, either.\nBasically, you should not feel like you have to restart the assignment unless things have become a total mess…\nSome general hints:\nDon’t forget to constantly check the status of your repository (repo) with git status: before and after nearly all other Git commands that you issue. This will help prevent mistakes, and will also help you understand what’s going on as you get up and running with Git.\nIt’s also a good idea to regularly check the log with git log; recall that git log --oneline will provide a quick overview with one line per commit.\n\nPart I – Create a new Git repo\nCreate a new directory at OSC.\nA good place for this directory is in /fs/ess/PAS1855/users/$USER/week03/, but you are free to create it elsewhere (I will only be checking the online version of your repo).\nI suggest the name pracs-sp21-GA1 for the dir, (“GA1” for “Graded Assignment 1”), but you are free to pick a name that makes sense to you.\nLoad the OSC git module. Don’t forget to do this, or you will be working with a much older version of Git.\nInitialize a local Git repository inside your new directory. [0.5]\nCreate a README file in Markdown format. [0.5]\nThe file should be named README.md, and for now, just contain a header saying that this is a repository for your assignment.\nStage and then commit the README file. [0.5]\nInclude an appropriate commit message.\nPart II – Add some Markdown content\nCreate a second Markdown file with some more contents. [1.5]\nThis file can have any name you want, and you can also choose what you want to write about.\nUse of a good variety of Markdown syntax, as discussed and practiced in week 2: headers, lists, hyperlinks, and so on. (You’ll have to include some inline code and a code block later on, so you may or may not choose to use those now.) Also, make sure to read the next step before you finish writing.\nAs long as you are not writing minimalistic dummy text without any meaning (like “Item 1, Item 2”), you will not be graded for what you are writing about, so feel free to pick something you like – and don’t worry about the details.\n(If you’re not feeling inspired, here are some suggestions: lecture and reading notes for this week; a table of Unix and/or Git commands that we’ve covered; things that you so far find challenging or interesting about Git; how computational skills may help you with your research.)\nHints In VS Code, open the Markdown preview on the side, so you can experiment and see whether your formatting is working the way you intend.\n\nCreate at least two commits while you work on the Markdown file. [0.5]\nTry to break your progress up into logical units that can be summarized with a descriptive commit message.\nHints\n\nBad commits/commit messages:\n“First part of the file” along with “Second part of the file”.\nGood commits and commit messages:\n“Summarized key concepts in the Git workflow” along with “Made a table of common Git commands”.\n\nUpdate the README.md file. [0.5]\nBriefly describe the current contents of your repo now that you actually have some contents.\nStage and commit the updated README file. [0.5]\nPart III – Add some “data” files\nCreate a directory with dummy data files. [0.5]\nCreate a directory called data and inside this directory, create 100 empty files with a single touch command and at least one brace expansion (e.g. for samples numbered 1-100, or 20 samples for 5 treatments). Give the files the extension .txt.\nStage and commit the data directory and its contents. [0.5]\nAs always, include a descriptive message with your commit.\nPart IV – Renaming files on a new branch\nAs it turns out, there was a miscommunication, and and all the “data” files will have to be renamed. You realize that this is risky business, and you don’t want to lose any data.\nIt occurs to you that one good way of going about this is creating a new Git branch and performing the renaming task on that branch. Then, if anything goes wrong, it will be easy to go back: you can just switch back to the master branch.\n(Note that there are many other ways of undoing and going back in Git, but this one is perhaps conceptually the easiest, and safe.)\nGo to a new branch. [0.5]\nCreate a new branch called rename-data-files or something similar, and move to the new branch. (Also, check whether this worked.)\nWrite a for loop to rename the files. [1.5]\nYou can keep it as simple as switching the file extension to .csv, or do something more elaborate if you want.\nBecause these files are being tracked by Git, recall that there is a Git-friendly modification to the command to rename files: use that in the loop.\nHints\nThese slides from last week have a similar renaming loop example.\nTo replace the last part of the filename, like the extension, recall that you can strip suffices using the basename command.\nBefore you go ahead and actually rename the files, use echo in your loop to print the (old and) new filenames: if it looks good, then add the renaming command to the loop.\n\nIf you just used mv and forgot to use git mv, don’t fret. Have a look at the status of the repo, then do git add --all and check the status again: Git figured out the rename.\nIf things go wrong, for instance you renamed incorrectly, or even deleted the files, you can start over in a few ways:\nYou can delete any files in data/, commit the deletion, and recreate the files. (The extra commits will not affect your grade, but use your commit messages to clarify what you’re doing.)\nA solution that would always work, including in case these files were actually valuable and you couldn’t just recreate them, is as follows:\nLike above, delete any files in data/ and commit the deletion.\nSwitch back to the master branch. Your data/ files should be back.\nCreate a new renaming branch and move there.\nOptionally, delete the old/failed renaming branch.\n\nCommit the changes made by the renaming operation.\nMerge into the master branch. [0.5]\nWith the files successfully renamed, go back to the master branch.\nThen, merge the rename-data-files branch into the master branch.\n(Optionally, remove the rename-data-files branch, since you will no longer need it.)\nUpdate the README file.\nIn the README file, describe what you did, including some inline code formatting, and put the code for the for loop in a code block.\n(No, the repo doesn’t really make sense as a cohesive unit anymore, but that’s okay while we’re practicing. :) )\nStage and commit the updated README file. [0.5]\nPart V – Create and sync an online version of the repo\nPhew, those were a lot of commits! Let’s share all of this work with the world.\nCreate a Github repository. [0.5]\nGo to https://github.com, sign in, and create a new repository.\nIt’s a good idea to give it the same name as your local repo, but these names don’t have to match.\nYou want to create an empty GitHub repository, because you will upload all the contents from your local repo: therefore, don’t check any of the boxes to create files like a README.\n\nPush your local repo online. [0.5]\nWith your repo created, follow the instructions that Github now gives you, under the heading “…or push an existing repository from the command line”.\nThese instructions will comprise three commands: git remote add to add the “remote” (online) connection, git branch to rename the default branch from master to main, and git push to actually “push” (upload) your local repo.\nWhen you’re done, click the Code button, and admire the nicely rendered README on the front page of your Github repo.\nHints\n\nAssuming that you’re using SSH authentication, which you should have set up in this week’s ungraded assignment, make sure you use the SSH link type: starting with git@github.com rather than HTTPS.\n\n\nCreate an issue to mark that you’re done! [0.5]\nFind the “Issues” tab for your repo on Github:\n\n\n\nIn the Issues tab, click the green button New Issue to open a new issue for you repo.\nGive it a title like “I finished my assignment”, and in the issue text, tag @jelmerp. You can say, for instance, “Hey @jelmerp, can you please take a look at my repo?”.\n\n",
      "last_modified": "2021-01-26T08:03:58-05:00"
    },
    {
      "path": "w03_UA_git-setup.html",
      "title": "Week 3 -- Ungraded Assignment : <br> Git setup and Github authentication",
      "author": [],
      "contents": "\nPart 1: Git setup\nThese instructions are for setting up Git at OSC, but from Step 3 onwards, you can also follow them to set up Git for your local computer.\nOpen up a terminal at OSC.\nYou can do this after logging in at https://ondemand.osc.edu in one of two ways:\nDirect shell access: Clusters (top blue bar) > Pitzer Shell Access.\nIn VS Code: Interactive Apps > Code Server > then open a terminal using Ctrl+backtick > break out of the Singularity shell by typing bash.\n\nLoad the OSC Git module.\n(Note: Git is available in any OSC shell without loading any modules, but that is a rather old version – so we load a newer one.)\nmodule load git/2.18.0\nUse git config to make your name known to Git.\nSomewhat confusingly, mote that this should be your actual name, and not your Github username:\ngit config --global user.name 'John Doe'\nUse git config to make your email address known to Git.Make sure that you use the same email address you used to sign up for Github.\ngit config --global user.email 'doe.391@osu.edu'\nUse git config to set a default text editor for Git.\nOccasionally, when you need to provide Git with a “commit message” to Git and you haven’t entered one on the command line, Git will open up a text editor for you. Even though we’ll be mostly be working in VS Code during this course, in this case, it is better to select a text editor that can be run within the terminal, like nano (or vim, if you are familiar with it). To specify Nano as your default text editor for Git:\ngit config --global core.editor \"nano -w\"\n\n# You could set VS Code as your editor on your local computer,\n# if you use it there:\n# git config --global core.editor \"code --wait\"\nCheck whether you successfully changed the settings:\ngit config --global --list\n\n# user.name=John Doe\n# user.email=doe.39@osu.edu\n# colour.ui=true\n# core.editor=nano -w\nSet colors if needed.\nMake sure you see colour.ui=true in the list (like above), so Git output in the terminal will use colors. If you don’t see this line, set it using:\ngit config --global color.ui true\n\nPart 2: Github authentication\nGithub authentication: Background\nTo be able to link local Git repositories to their online counterparts on Github, we need to set up Github authentication.\nRegular password access (over HTTP/HTTPS) is now “deprecated” by Github, and two better options are to set up SSH access with an SSH key, or HTTPS access with a Public Access Token.\nWe’ll use SSH, as it is easier – though still a bit of drag – and because learning this procedure will also be useful for when you’ll be setting up SSH access to the Ohio Supercomputer Center. (But note that Github now labels HTTPS access as the “preferred” method.)\nFor everything on Github, there are separate SSH and HTTPS URLs, and Github will always show you both types of URLs. When using SSH, we need to use URLs with the following format:\ngit@github.com:<USERNAME>/<REPOSITORY>.git\n(And when using HTTPS, you would use URLS like https://github.com/<USERNAME>/<REPOSITORY>.git)\nSetting up Github SSH authentication\nThese instructions are for setting up authentication at OSC, but you can repeat the same steps to set up authentication for your local computer.\nIn a terminal at OSC, use the ssh-keygen command to generate a public-private SSH key pair like so:\nssh-keygen -t rsa\nYou’ll be asked three questions, and for all three, you can accept the default by just pressing Enter:\n# Enter file in which to save the key (<default path>):\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again: \n\n\n\nNow, you have a file called id_rsa.pub in your ~/.ssh folder, which is your public key. To enable authentication, we will put this public key on Github – our public key interacts with our private key, which we do not share.\nPrint the public key to screen using cat:\ncat ~/.ssh/id_rsa.pub\nCopy the public key, i.e. the contents of the public key file, to your clipboard. Make sure you get all of it, including the “ssh-rsa” part (but beware that your new prompt may start on the same line as the end the key):\n\n\n\nIn your browser, go to https://github.com and log in.\nGo to your personal Settings. (Click on your avatar in the far top-right of the page, and select Settings in the drop-down menu.)\nClick on SSH and GPG keys in the sidebar.\nClick the green New SSH key button.\nGive the key an arbitrary, informative name, e.g. “OSC” to indicate that you are using this key at OSC.\nPaste the public key, which you copied to your clipboard earlier, into the box.\n\n\nClick the green Add SSH key button. Done!\n\n\n\n\n",
      "last_modified": "2021-01-26T08:03:59-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
